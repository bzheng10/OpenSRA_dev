{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create input JSON file for EQHazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROJ: proj_create_from_database: Cannot find proj.db\n"
     ]
    }
   ],
   "source": [
    "##### Main libraries/modules\n",
    "\n",
    "import json, os, subprocess, shapely\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function to calculate the epicentral and hypocentral distances using the haversine equation\n",
    "\n",
    "def get_haversine_dist(r,lat1,long1,lat2,long2,z=0):\n",
    "    r = 6371\n",
    "    # Haversine\n",
    "    d = 2*r*np.arcsin(np.sqrt(np.sin((lat2-lat1)/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin((long2-long1)/2)**2))\n",
    "    return d, np.sqrt(d**2 + z**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import liquefaction susceptibility map for Bay Area\n",
    "##### https://pubs.usgs.gov/of/2006/1037/\n",
    "# df_susc = gpd.GeoDataFrame.from_file('C:\\\\Users\\\\BarryZheng\\\\Downloads\\\\of06-1037_4b.shp\\\\'+'sfq2py.shp')\n",
    "df_liq_susc = gpd.GeoDataFrame.from_file(os.getcwd()+'\\\\of06-1037_4b.shp\\\\sfq2py.shp')\n",
    "liq_susc_dict = {'VL': 'very low',\n",
    "                 'L': 'low',\n",
    "                 'M': 'low',\n",
    "                 'H': 'low',\n",
    "                 'L': 'low',\n",
    "                 'W': 'water',\n",
    "                 'NM': 'not mapped'}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### Import landslide susceptibility map, cropped and converted to WGS84 coordinate system\n",
    "##### ftp://ftp.conservation.ca.gov/pub/dmg/pubs/ms/058/\n",
    "ras_ls_susc = rio.open(os.getcwd() + '\\\\ms58_ls_susc_clip_wgs84.tif')\n",
    "data_ls_susc = ras_ls_susc.read(1)\n",
    "data_ls_susc = data_ls_susc.astype(float)\n",
    "# coordBound = [ras_ls_susc.bounds.right - ras_ls_susc.bounds.left,\n",
    "#               ras_ls_susc.bounds.top - ras_ls_susc.bounds.bottom]\n",
    "# coordNW = [ras_ls_susc.bounds.left,ras_ls_susc.bounds.top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import depth to groundwater map, converted to WGS84 coordinate system and downsampled by a factor of 4\n",
    "##### https://datadryad.org/stash/dataset/doi:10.6078/D1W01Q\n",
    "ras_z2gw = rio.open(os.getcwd() + '\\\\MinDTWmodel_meters_wgs84_ds4.tif')\n",
    "data_z2gw = ras_z2gw.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\n",
    "show(ras_ls_susc, ax=ax1, title='liq_susc')\n",
    "show(ras_z2gw.read(1), ax=ax2, transform=ras_z2gw.transform, cmap='Blues', title='depth to gw')\n",
    "ax1.set_xlim([-122.7,-121.6])\n",
    "ax1.set_ylim([37.3,38.3])\n",
    "ax2.set_xlim([-122.7,-121.6])\n",
    "ax2.set_ylim([37.3,38.3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Input file name\n",
    "\n",
    "fn_fp_list = ['fn', 'fp']\n",
    "rup_ind_list = [0, 0, 0, 0]\n",
    "# source_ind_list = [83, 108, 606, 626]\n",
    "fn_fp_list = [None]\n",
    "rup_ind_list = [0]\n",
    "source_ind_list = [606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/BarryZheng/OneDrive - SlateGeotech/Fragility/output//sim_grid_ucerf3_0_606.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[make_EQHazard_input(ii, rup_ind_list[jj], source_ind_list[jj]) for ii in fn_fp_list for jj in range(len(source_ind_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EQHazard_input(fn_fp, rup_ind, source_ind):\n",
    "    \n",
    "    ##### Other inputs\n",
    "    flag_h_corr = 0\n",
    "    flag_p_corr = 0\n",
    "    flag_ucerf = 3\n",
    "    flag_psha = 0\n",
    "    prob_annual_exc = 0.05\n",
    "    nSites = 5\n",
    "    if fn_fp == 'fn':\n",
    "    #     z2gw = np.hstack([np.random.rand(1)*(10-5)+5,np.random.rand(2)*(20-5)+5,np.random.rand(2)*(30-10)+10])\n",
    "        z2gw = ([5,5,5,10,10])\n",
    "    elif fn_fp == 'fp':\n",
    "    #     z2gw = np.hstack([np.random.rand(3)*(7-3)+3,np.random.rand(2)*(10-5)+5])\n",
    "        z2gw = ([3,3,3,5,5])\n",
    "\n",
    "    if flag_psha == 1:\n",
    "        str_rup = '_psha'\n",
    "    else:\n",
    "        str_rup =  '_' + str(rup_ind) + '_' + str(source_ind)\n",
    "\n",
    "    # z2gw = np.linspace(10,5,nSites)\n",
    "\n",
    "    \n",
    "    ### Pick site type/definition\n",
    "    # site_type = 'SingleLocation'\n",
    "    site_type = 'Grid'\n",
    "#     site_type = 'SiteList'\n",
    "    \n",
    "    if site_type == 'SiteList':\n",
    "        # inName = os.getcwd() + '\\\\sim_s' + str(nSites) + '_' + flag_fn_fp + '_ucerf' + \\\n",
    "        #          str(flag_ucerf) + '_h' + str(flag_h_corr) + '_p' + str(flag_p_corr) + '.json'\n",
    "        inName = os.getcwd() + '\\\\output\\\\' + '\\\\sim_s' + str(nSites) + '_' + fn_fp + '_ucerf' + \\\n",
    "                 str(flag_ucerf) + str_rup + '.json'\n",
    "        inName = inName.replace('\\\\','/')\n",
    "    elif site_type == 'Grid':\n",
    "        inName = os.getcwd() + '\\\\output\\\\' + '\\\\sim_grid_ucerf' + \\\n",
    "                 str(flag_ucerf) + str_rup + '.json'\n",
    "        inName = inName.replace('\\\\','/')\n",
    "    \n",
    "    \n",
    "    ##### Basic\n",
    "    out_dict = {}\n",
    "    \n",
    "    out_dict = {'Site': {},\n",
    "                'EqRupture': {},\n",
    "                'GMPE': {},\n",
    "                'IntensityMeasure': {}}\n",
    "    \n",
    "    ##### Site\n",
    "    out_dict['Site'].clear()\n",
    "\n",
    "    ### Update if defining a single point\n",
    "    # siteList = [37.8719, -122.2585, None] # site coordinate, Vs30 of point, if \"None\", then inferred from Wald and Allen (2008)\n",
    "\n",
    "    ### Update if defining a grid\n",
    "    lat_range = [37, 39, round((39-37)/0.01)] # min, max, nDiv\n",
    "    long_range = [-123, -121, round((-121--123)/0.01)] # min, max, nDiv\n",
    "    vs30 = None # Vs30 of grid, if \"None\", then inferred from Wald and Allen (2008)\n",
    "    \n",
    "    ### Update if defining a list of sites\n",
    "    # pt1_line = [37.8745, -122.2542]\n",
    "    # pt2_line = [37.8390, -122.2793]\n",
    "    # pt1_line = [37.892, -122.277]\n",
    "    # pt2_line = [37.808, -122.271]\n",
    "    ###### pt1_fp = [37.822140, -122.236366]\n",
    "    ###### pt2_fp = [37.901043, -122.297522]\n",
    "    ###### pt1_fn = [37.871003, -122.253224]\n",
    "    ###### pt2_fn = [37.851118, -122.289518]\n",
    "    pt1_fp = [37.924836, -122.316215]\n",
    "    pt2_fp = [37.802175, -122.218137]\n",
    "    pt1_fn = [37.83284025, -122.2426565]\n",
    "    pt2_fn = [37.93091825, -122.1199955]\n",
    "    if fn_fp == 'fp':\n",
    "        pt1_line = pt1_fp\n",
    "        pt2_line = pt2_fp\n",
    "    elif fn_fp == 'fn':\n",
    "        pt1_line = pt1_fn\n",
    "        pt2_line = pt2_fn\n",
    "    nDiv_line = nSites\n",
    "\n",
    "    \n",
    "    ### Update if defining a list\n",
    "    if site_type == 'SiteList':\n",
    "        siteListMethod = 3 # 1 = provide list below, 2 = define in txt file and import, 3 = define from two points and divisions\n",
    "        if siteListMethod == 1:\n",
    "            siteList = [[37.8719, -122.2585, 240.0],\n",
    "                        [37.86,   -122.2585, 750.0],\n",
    "                        [37.85,   -122.2585, None]]\n",
    "        elif siteListMethod == 2:\n",
    "            siteListPath = 'siteList.txt'\n",
    "            siteList = []\n",
    "            with open(siteListPath, 'r') as f:\n",
    "                for lines in f:\n",
    "                    line = []\n",
    "                    for i in lines.split(','):\n",
    "                        if i.split():\n",
    "                            if 'None' in i:\n",
    "                                line.append(None)\n",
    "                            else:\n",
    "                                line.append(float(i))\n",
    "                    siteList.append(line)\n",
    "        elif siteListMethod == 3:\n",
    "            siteList = []\n",
    "            for i in range(nDiv_line):\n",
    "                xc = pt1_line[0]+(i+0.5)*(pt2_line[0] - pt1_line[0])/nDiv_line\n",
    "                yc = pt1_line[1]+(i+0.5)*(pt2_line[1] - pt1_line[1])/nDiv_line\n",
    "\n",
    "                x1 = pt1_line[0]+(i+0.0)*(pt2_line[0] - pt1_line[0])/nDiv_line\n",
    "                y1 = pt1_line[1]+(i+0.0)*(pt2_line[1] - pt1_line[1])/nDiv_line\n",
    "                x2 = pt1_line[0]+(i+1.0)*(pt2_line[0] - pt1_line[0])/nDiv_line\n",
    "                y2 = pt1_line[1]+(i+1.0)*(pt2_line[1] - pt1_line[1])/nDiv_line\n",
    "\n",
    "                d = get_haversine_dist(0,np.radians(x1),np.radians(y1),\n",
    "                                       np.radians(x2),np.radians(y2),z=0)\n",
    "\n",
    "                siteList.append([xc,yc,vs30,d[0]])\n",
    "\n",
    "            ## Liquefaction susceptibility\n",
    "            liq_susc = []\n",
    "            for i in range(nDiv_line):\n",
    "                # Setting the coordinates for the point\n",
    "                coord2search = shapely.geometry.Point((siteList[i][1],siteList[i][0])) # Longitude & Latitude\n",
    "                # Searching for the geometry that intersects the point. Returning the index for the appropriate polygon.\n",
    "                liq_susc.append(df_liq_susc[df_liq_susc.geometry.intersects(coord2search)].LIQ.values[0])\n",
    "            liq_susc = [liq_susc_dict.get(i,None) for i in liq_susc]\n",
    "\n",
    "            ## Landslide susceptibility\n",
    "            ls_susc = [data_ls_susc[ras_ls_susc.index(i[1], i[0])] for i in siteList]\n",
    "            ls_susc = [int(i) if i >= 0 else 0 for i in ls_susc]\n",
    "\n",
    "    elif site_type == 'Grid':\n",
    "        siteList = []\n",
    "        for i in range(lat_range[2]+1):\n",
    "            xc = lat_range[0]+i*(lat_range[1] - lat_range[0])/lat_range[2]\n",
    "            for j in range(long_range[2]+1):\n",
    "                yc = long_range[0]+j*(long_range[1] - long_range[0])/long_range[2]\n",
    "                siteList.append([xc,yc,vs30])\n",
    "\n",
    "    ### update dictionary\n",
    "    if site_type == 'SingleLocation':\n",
    "        site_loc = {'Latitude': coord[0],\n",
    "                    'Longitude': coord[1],\n",
    "                    'Vs30': vs30}\n",
    "        out_dict['Site'].update({'Type': site_type,\n",
    "                                 'Location': site_loc})\n",
    "    elif site_type == 'Grid':\n",
    "        site_loc = {'Latitude': {'Min': lat_range[0], 'Max': lat_range[1], 'Divisions': lat_range[2]},\n",
    "                    'Longitude': {'Min': long_range[0], 'Max': long_range[1], 'Divisions': long_range[2]},\n",
    "                    'Vs30': vs30}\n",
    "        out_dict['Site'].update({'Type': site_type,\n",
    "                                 site_type: site_loc})\n",
    "    elif site_type == 'SiteList':\n",
    "        site_loc_full = []\n",
    "        for i in range(len(siteList)):\n",
    "            site_loc = {'Location': {'Latitude': siteList[i][0],\n",
    "                                     'Longitude': siteList[i][1]},\n",
    "                        'Vs30': siteList[i][2],\n",
    "                        'LSegment': siteList[i][3],\n",
    "                        'LiqSusc': liq_susc[i],\n",
    "                        'LsSusc': ls_susc[i],\n",
    "                        'Z2gw': z2gw[i]}\n",
    "            site_loc_full.append(site_loc)\n",
    "        out_dict['Site'].update({'Type': site_type,\n",
    "                                 site_type: site_loc_full})\n",
    "      \n",
    "    \n",
    "    ##### EqRupture\n",
    "    out_dict['EqRupture'].clear()\n",
    "\n",
    "    ### define rupture\n",
    "    # eq_rup_type = 'PointSource'\n",
    "    eq_rup_type = 'ERF'\n",
    "\n",
    "    ### actions for point source\n",
    "    if eq_rup_type == 'PointSource':\n",
    "        eq_mag = 7.05\n",
    "        eq_rup_loc = {'Latitude': 37.9,\n",
    "                      'Longitude': -122.3,\n",
    "                      'Depth': 0.0}\n",
    "        eq_avg_rake = 0.0\n",
    "        eq_avg_dip = 90.0\n",
    "        out_dict['EqRupture'].update({'Type': eq_rup_type,\n",
    "                                      'Magnitude': eq_mag,\n",
    "                                      'Location': eq_rup_loc,\n",
    "                                      'AverageRake': eq_avg_rake,\n",
    "                                      'AverageDip': eq_avg_dip})\n",
    "\n",
    "    ### update dictionary\n",
    "    elif eq_rup_type == 'ERF':\n",
    "    #     eq_rup_forecast = 'WGCEP (2007) UCERF2 - Single Branch'\n",
    "    #     eq_source_index = 28\n",
    "    #     eq_rup_index = 7\n",
    "        eq_rup_forecast = 'Mean UCERF3'\n",
    "    #     eq_source_index = 606\n",
    "    #     eq_rup_index = 0\n",
    "    #     eq_prob = 2.0749991502810872E-8\n",
    "        out_dict['EqRupture'].update({'Type': eq_rup_type,\n",
    "                                      'RuptureForecast': eq_rup_forecast,\n",
    "                                      'SourceIndex': source_ind,\n",
    "                                      'RuptureIndex': rup_ind})\n",
    "        \n",
    "    ##### GMPE\n",
    "    out_dict['GMPE'].clear()\n",
    "\n",
    "    ### pick GMPE\n",
    "    # gmpe_type = 'Campbell & Bozorgnia (2014)'\n",
    "    # gmpe_type = 'Abrahamson, Silva & Kamai (2014)'\n",
    "    gmpe_type = 'Boore, Stewart, Seyhan & Atkinson (2014)'\n",
    "\n",
    "    ### define GMPE params\n",
    "    gmpe_params = {}\n",
    "\n",
    "    ### update dictionary\n",
    "    out_dict['GMPE'].update({'Type': gmpe_type,\n",
    "                             'Parameters': gmpe_params})\n",
    "    \n",
    "    ##### Intensity Measure\n",
    "\n",
    "    out_dict['IntensityMeasure'].clear()\n",
    "\n",
    "    ### choose type of IM to save: individual or all\n",
    "    # im_type = 'SA'\n",
    "    # im_type = 'PGA'\n",
    "    im_type = 'PGV'\n",
    "#     im_type = 'All'\n",
    "\n",
    "    ### choose type of output to get: json, csv, geojson\n",
    "    im_json_out = True\n",
    "    im_csv_out = False\n",
    "    im_geo_json_out = True\n",
    "\n",
    "    ### define period for SA\n",
    "    ### if unspecified, default period = [0.01, 0.02, 0.03, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 7.5, 10.0]\n",
    "    im_period = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "    ### update dictionary\n",
    "    if flag_psha == 1:\n",
    "        im_type = 'UHS'\n",
    "        out_dict['IntensityMeasure'].update({'Type': im_type,\n",
    "                                             'ExceedenceProbability': prob_annual_exc,\n",
    "                                             'EnableJsonOutput': im_json_out,\n",
    "                                             'EnableCsvOutput': im_csv_out,\n",
    "                                             'EnableGeoJsonOutput': im_geo_json_out})\n",
    "    else:\n",
    "        out_dict['IntensityMeasure'].update({'Type': im_type,\n",
    "                                             'Periods': im_period,\n",
    "                                             'EnableJsonOutput': im_json_out,\n",
    "                                             'EnableCsvOutput': im_csv_out,\n",
    "                                             'EnableGeoJsonOutput': im_geo_json_out})\n",
    "        \n",
    "    ##### Other Parameters\n",
    "    # if flag_h_corr == 1:\n",
    "    #     corr_h = True\n",
    "    # else:\n",
    "    #     corr_h = False\n",
    "\n",
    "    # if flag_p_corr == 1:\n",
    "    #     corr_p = True\n",
    "    # else:\n",
    "    #     corr_p = False\n",
    "\n",
    "    # ### update dictionary\n",
    "    # out_dict['Other'].update({'Corr_Dist': corr_h,\n",
    "    #                           'Corr_Period': corr_p})\n",
    "    \n",
    "    \n",
    "    ##### See output before saving\n",
    "#     print(out_dict)\n",
    "    \n",
    "    \n",
    "    #####\n",
    "    print(inName)\n",
    "    \n",
    "    \n",
    "    ##### save file\n",
    "    with open(inName, 'w') as f:\n",
    "        json.dump(out_dict, f, indent=4)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import folium\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "siteList1 = np.asarray(siteList)\n",
    "siteList1 = siteList1.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eq_rup_loc1 = np.array([[-122.4342,38.0875],\n",
    "                     [-122.21280000000002,37.8273],\n",
    "                     [-122.1284,37.7299],\n",
    "                     [-122.0824,37.676],\n",
    "                     [-121.84900000000002,37.454]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# eq_rup_loc1 = np.loadtxt('rup_0_606.txt')\n",
    "eq_rup_loc1 = np.loadtxt('rup_0_108.txt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eq_rup_loc1[:,[0, 1]] = eq_rup_loc1[:,[1, 0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if eq_rup_type == 'ERF':\n",
    "    indjson = inName.find('.json')\n",
    "    outName = inName[0:indjson] + '_OUT.json'\n",
    "    with open(outName, 'r') as f:\n",
    "        jd = json.load(f)\n",
    "    eq_rup_erf_loc = jd['EqRupture']['Surface']\n",
    "    eq_rup_erf_lat = [i['Latitude'] for i in jd['EqRupture']['Surface']]\n",
    "    eq_rup_erf_long = [i['Longitude'] for i in jd['EqRupture']['Surface']]\n",
    "    eq_rup_erf_z = [i['Depth'] for i in jd['EqRupture']['Surface']]\n",
    "    eq_rup_loc = dict()\n",
    "    eq_rup_loc.update({'Latitude': np.mean(eq_rup_erf_lat),\n",
    "                       'Longitude': np.mean(eq_rup_erf_long)})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cutoff = data_z2gw < 0\n",
    "data_z2gw[cutoff] = None # screen out depths less than 0\n",
    "data_z2gw = data_z2gw*3.28 # convert to feet\n",
    "cutoff = data_ls_susc > 10\n",
    "data_ls_susc[cutoff] = None # screen out landslide susceptibility greater than 10 (max)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_liq_susc.plot(cmap='Pastel1',ec=[.7,.7,.7])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_liq_susc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(25,12))\n",
    "# df_liq_susc.plot(cmap='Pastel1',ec=[.7,.7,.7])\n",
    "show(data_ls_susc, ax=ax, transform=ras_ls_susc.transform, alpha=1, cmap='YlGn',\n",
    "     title='liquefaction susceptibility (ylw,grn) and depth to groundwater (blue)')\n",
    "show(data_z2gw, ax=ax, transform=ras_z2gw.transform, alpha=1, cmap='Blues_r')\n",
    "df_liq_susc.plot(ax=ax, cmap='Pastel1',ec=[.7,.7,.7], alpha=0.4)\n",
    "# plt.plot([pt1_fp[1],pt2_fp[1]],[pt1_fp[0],pt2_fp[0]],'-r',linewidth=4)\n",
    "plt.plot([pt1_fn[1],pt2_fn[1]],[pt1_fn[0],pt2_fn[0]],'-r',linewidth=4)\n",
    "for i in range(nSites):\n",
    "    plt.plot(siteList[i][1],siteList[i][0],'o',mec='r',mfc='w')\n",
    "plt.plot(list(eq_rup_loc1[:,[1]]),list(eq_rup_loc1[:,[0]]),'-k',linewidth=4)\n",
    "ax.set_xlim([-122.5,-121.9])\n",
    "ax.set_ylim([37.4,38.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(25,12))\n",
    "# df_liq_susc.plot(cmap='Pastel1',ec=[.7,.7,.7])\n",
    "# show(data_ls_susc, ax=ax, transform=ras_ls_susc.transform, alpha=1, cmap='YlGn',\n",
    "#      title='liquefaction susceptibility (ylw,grn) and depth to groundwater (blue)')\n",
    "# show(data_z2gw, ax=ax, transform=ras_z2gw.transform, alpha=1, cmap='Blues_r')\n",
    "df_liq_susc.plot(ax=ax, cmap='Pastel1',ec=[.7,.7,.7])\n",
    "# plt.plot([pt1_fp[1],pt2_fp[1]],[pt1_fp[0],pt2_fp[0]],'-r',linewidth=4)\n",
    "plt.plot([pt1_fn[1],pt2_fn[1]],[pt1_fn[0],pt2_fn[0]],'-r',linewidth=4)\n",
    "for i in range(nSites):\n",
    "    plt.plot(siteList[i][1],siteList[i][0],'o',mec='r',mfc='w')\n",
    "plt.plot(list(eq_rup_loc1[:,[1]]),list(eq_rup_loc1[:,[0]]),'-k',linewidth=4)\n",
    "ax.set_xlim([-122.5,-121.9])\n",
    "ax.set_ylim([37.4,38.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Available tiles = OpenStreetMap (default), Stamen Terrain, Stamen Toner, Mapbox Bright, Mapbox Control Room,\n",
    "mapEQ = folium.Map(location = [eq_rup_loc['Latitude'],eq_rup_loc['Longitude']],\n",
    "                   tiles = 'Stamen Terrain',\n",
    "                   zoom_start = 11)\n",
    "# if eq_rup_type == 'ERF':\n",
    "#     folium.Rectangle(bounds=[[min(eq_rup_erf_lat),min(eq_rup_erf_long)],\n",
    "#                              [max(eq_rup_erf_lat),max(eq_rup_erf_long)]],\n",
    "#                      color='black',fill_opacity=0.2,\n",
    "#                      opacity=1,fill=True,fill_color='black').add_to(mapEQ)\n",
    "folium.PolyLine(locations=eq_rup_loc1,color='black',popup='<i>Earthquake Rupture</i>').add_to(mapEQ)\n",
    "# tooltip = 'Click me!'\n",
    "# folium.Marker([eq_rup_loc['Latitude'],eq_rup_loc['Longitude']],\n",
    "#               popup='<i>Earthquake Rupture</i>',\n",
    "#               icon = folium.Icon(color='black', icon='info-sign'),\n",
    "#               tooltip=tooltip).add_to(mapEQ)\n",
    "\n",
    "for i in range(len(siteList)):\n",
    "#     folium.Marker([siteList[i][0],siteList[i][1]],\n",
    "#               popup='<i>Site </i>' + str(i+1)).add_to(mapEQ)\n",
    "    folium.CircleMarker(location=[siteList[i][0],siteList[i][1]],radius=3,fill=True,color='red').add_to(mapEQ)\n",
    "if site_type == 'Grid':\n",
    "    folium.Rectangle(bounds=[[lat_range[0],long_range[0]],\n",
    "                             [lat_range[1],long_range[1]]],color='red',fill_opacity=0.2,\n",
    "                     opacity=1,fill=True,fill_color='red').add_to(mapEQ)\n",
    "elif site_type == 'SiteList':\n",
    "    folium.PolyLine(locations=[pt1_fp,pt2_fp],color='red',fill_opacity=0.2,\n",
    "                    opacity=1,fill=True,fill_color='red').add_to(mapEQ)\n",
    "    folium.PolyLine(locations=[pt1_fn,pt2_fn],color='red',fill_opacity=0.2,\n",
    "                    opacity=1,fill=True,fill_color='red').add_to(mapEQ)\n",
    "mapEQ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g1 = nx.Graph()\n",
    "g1.add_edge('a', 'b', weight=1)\n",
    "g1.add_edge('b', 'c', weight=1)\n",
    "g1.add_edge('a', 'c', weight=1)\n",
    "g1.add_edge('c', 'd', weight=1)\n",
    "g1.add_edge('a', 'e', weight=1)\n",
    "g1.add_edge('b', 'e', weight=1)\n",
    "g1.add_edge('a', 'd', weight=1)\n",
    "g1 = nx.Graph(g1)\n",
    "nx.draw(g1, with_labels=True, font_weight='bold')\n",
    "nx.shortest_path(g1, 'e', 'd')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(folium.Icon)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "outName = os.getcwd() + '\\\\output1.json'\n",
    "outName = outName.replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EQH_path = 'C:/Users/BarryZheng/Desktop/SimCenter/test/EQHazard/build/EQHazard.jar'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subprocess.call(['java', '-jar', EQH_path, inName, outName],shell=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.system('java -jar ' + EQH_path + ' ' + inName + ' ' + outName)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
