{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Copyright (c) 2018 Leland Stanford Junior University\n",
    "Copyright (c) 2018 The Regents of the University of California\n",
    "\n",
    "This file is part of pelicun.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice,\n",
    "this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "this list of conditions and the following disclaimer in the documentation\n",
    "and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors\n",
    "may be used to endorse or promote products derived from this software without\n",
    "specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n",
    "LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
    "CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
    "SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
    "INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
    "CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
    "ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
    "POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "You should have received a copy of the BSD 3-Clause License along with\n",
    "pelicun. If not, see <http://www.opensource.org/licenses/>.\n",
    "\n",
    "Contributors:\n",
    "Adam ZsarnÃ³czay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module has classes and methods that control the loss assessment.\n",
    "\n",
    ".. rubric:: Contents\n",
    "\n",
    ".. autosummary::\n",
    "\n",
    "    Assessment\n",
    "    FEMA_P58_Assessment\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .base import *\n",
    "# from .uq import *\n",
    "# from .model import *\n",
    "# from .file_io import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assessment(object):\n",
    "    \"\"\"\n",
    "    A high-level class that collects features common to all supported loss\n",
    "    assessment methods. This class will only rarely be called directly when\n",
    "    using pelicun.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # initialize the basic data containers\n",
    "        # inputs\n",
    "        self._AIM_in = None\n",
    "        self._EDP_in = None\n",
    "        self._POP_in = None\n",
    "        self._FG_in = None\n",
    "\n",
    "        # random variables and loss model\n",
    "        self._RV_dict = None # dictionary to store random variables\n",
    "        self._EDP_dict = None\n",
    "        self._FG_dict = None\n",
    "\n",
    "        # results\n",
    "        self._TIME = None\n",
    "        self._POP = None\n",
    "        self._COL = None\n",
    "        self._ID_dict = None\n",
    "        self._DMG = None\n",
    "        self._DV_dict = None\n",
    "        self._SUMMARY = None\n",
    "\n",
    "        self._assessment_type = 'generic'\n",
    "\n",
    "    @property\n",
    "    def beta_tot(self):\n",
    "        \"\"\"\n",
    "        Calculate the total additional uncertainty for post processing.\n",
    "\n",
    "        The total additional uncertainty is the squared root of sum of squared\n",
    "        uncertainties corresponding to ground motion and modeling.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        beta_total: float\n",
    "            The total uncertainty (logarithmic EDP standard deviation) to add\n",
    "            to the EDP distribution. Returns None if no additional uncertainty\n",
    "            is assigned.\n",
    "        \"\"\"\n",
    "\n",
    "        AU = self._AIM_in['general']['added_uncertainty']\n",
    "\n",
    "        beta_total = 0.\n",
    "        if AU['beta_m'] is not None:\n",
    "            beta_total += AU['beta_m'] ** 2.\n",
    "        if AU['beta_gm'] is not None:\n",
    "            beta_total += AU['beta_gm'] ** 2.\n",
    "\n",
    "        # if no uncertainty is assigned, we return None\n",
    "        if beta_total == 0:\n",
    "            beta_total = None\n",
    "        else:\n",
    "            beta_total = np.sqrt(beta_total)\n",
    "\n",
    "        return beta_total\n",
    "\n",
    "    def read_inputs(self, path_DL_input, path_EDP_input, verbose=False):\n",
    "        \"\"\"\n",
    "        Read and process the input files to describe the loss assessment task.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_DL_input: string\n",
    "            Location of the Damage and Loss input file. The file is expected to\n",
    "            be a JSON with data stored in a standard format described in detail\n",
    "            in the Input section of the documentation.\n",
    "        path_EDP_input: string\n",
    "            Location of the EDP input file. The file is expected to follow the\n",
    "            output formatting of Dakota. The Input section of the documentation\n",
    "            provides more information about the expected formatting.\n",
    "        verbose: boolean, default: False\n",
    "            If True, the method echoes the information read from the files.\n",
    "            This can be useful to ensure that the information in the file is\n",
    "            properly read by the method.\n",
    "        \"\"\"\n",
    "\n",
    "        # read SimCenter inputs -----------------------------------------------\n",
    "        # BIM file\n",
    "        self._AIM_in = read_SimCenter_DL_input(\n",
    "            path_DL_input, assessment_type=self._assessment_type,\n",
    "            verbose=verbose)\n",
    "\n",
    "        # EDP file\n",
    "        if self._hazard == 'EQ':\n",
    "            self._EDP_in = read_SimCenter_EDP_input(\n",
    "                path_EDP_input,\n",
    "                units=dict(PID=1.,\n",
    "                           PFA=self._AIM_in['units']['acceleration']),\n",
    "                verbose=verbose)\n",
    "        elif self._hazard == 'HU':\n",
    "            self._EDP_in = read_SimCenter_EDP_input(\n",
    "                path_EDP_input, EDP_kinds=('PWS',),\n",
    "                units=dict(PWS=self._AIM_in['units']['speed']),\n",
    "                verbose=verbose)\n",
    "\n",
    "    def define_random_variables(self):\n",
    "        \"\"\"\n",
    "        Define the random variables used for loss assessment.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def define_loss_model(self):\n",
    "        \"\"\"\n",
    "        Create the stochastic loss model based on the inputs provided earlier.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calculate_damage(self):\n",
    "        \"\"\"\n",
    "        Characterize the damage experienced in each random event realization.\n",
    "\n",
    "        \"\"\"\n",
    "        self._ID_dict = {}\n",
    "\n",
    "    def calculate_losses(self):\n",
    "        \"\"\"\n",
    "        Characterize the consequences of damage in each random event realization.\n",
    "\n",
    "        \"\"\"\n",
    "        self._DV_dict = {}\n",
    "\n",
    "    def write_outputs(self):\n",
    "        \"\"\"\n",
    "        Export the results.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _create_RV_demands(self):\n",
    "\n",
    "        # Unlike other random variables, the demand RV is based on raw data.\n",
    "\n",
    "        # First, collect the raw values from the EDP dict...\n",
    "        demand_data = []\n",
    "        d_tags = []\n",
    "        detection_limits = []\n",
    "        collapse_limits = []\n",
    "        GI = self._AIM_in['general']\n",
    "        s_edp_keys = sorted(self._EDP_in.keys())\n",
    "        for d_id in s_edp_keys:\n",
    "            d_list = self._EDP_in[d_id]\n",
    "            for i in range(len(d_list)):\n",
    "                demand_data.append(d_list[i]['raw_data'])\n",
    "                d_tags.append(str(d_id) +\n",
    "                              '-LOC-' + str(d_list[i]['location']) +\n",
    "                              '-DIR-' + str(d_list[i]['direction']))\n",
    "                det_lim = GI['detection_limits'][d_id]\n",
    "                if det_lim is None:\n",
    "                    det_lim = np.inf\n",
    "                if GI['response']['EDP_dist_basis'] == 'non-collapse results':\n",
    "                    coll_lim = GI['collapse_limits'][d_id]\n",
    "                    if coll_lim is None:\n",
    "                        coll_lim = np.inf\n",
    "                elif GI['response']['EDP_dist_basis'] == 'all results':\n",
    "                    coll_lim = np.inf\n",
    "\n",
    "                detection_limits.append([0., det_lim])\n",
    "                collapse_limits.append([0., coll_lim])\n",
    "\n",
    "        detection_limits = np.transpose(np.asarray(detection_limits))\n",
    "        collapse_limits = np.transpose(np.asarray(collapse_limits))\n",
    "        demand_data = np.transpose(np.asarray(demand_data))\n",
    "\n",
    "        # If more than one sample is available...\n",
    "        if demand_data.shape[0] > 1:\n",
    "\n",
    "            # Second, we discard the collapsed EDPs if the fitted distribution shall\n",
    "            # represent non-collapse EDPs.\n",
    "            EDP_filter = np.all([np.all(demand_data > collapse_limits[0], axis=1),\n",
    "                                 np.all(demand_data < collapse_limits[1], axis=1)],\n",
    "                                axis=0)\n",
    "            demand_data = demand_data[EDP_filter]\n",
    "\n",
    "            # Third, we censor the EDPs that are beyond the detection limit.\n",
    "            EDP_filter = np.all([np.all(demand_data > detection_limits[0], axis=1),\n",
    "                                 np.all(demand_data < detection_limits[1], axis=1)],\n",
    "                                axis=0)\n",
    "            censored_count = len(EDP_filter) - sum(EDP_filter)\n",
    "            demand_data = demand_data[EDP_filter]\n",
    "            demand_data = np.transpose(demand_data)\n",
    "\n",
    "            # Fourth, we create the random variable\n",
    "            demand_RV = RandomVariable(ID=200, dimension_tags=d_tags,\n",
    "                                       raw_data=demand_data,\n",
    "                                       detection_limits=detection_limits,\n",
    "                                       censored_count=censored_count\n",
    "                                       )\n",
    "\n",
    "            # And finally, if requested, we fit a multivariate lognormal or a\n",
    "            # truncated multivariate lognormal distribution to the censored raw\n",
    "            # data.\n",
    "            target_dist = GI['response']['EDP_distribution']\n",
    "\n",
    "            if target_dist == 'lognormal':\n",
    "                demand_RV.fit_distribution('lognormal')\n",
    "            elif target_dist == 'truncated lognormal':\n",
    "                demand_RV.fit_distribution('lognormal', collapse_limits)\n",
    "\n",
    "        # This is a special case when only a one sample is provided.\n",
    "        else:\n",
    "            # TODO: what to do when the sample is larger than the collapse or detection limit and when truncated distribution is prescribed\n",
    "\n",
    "            # Since we only have one data point, the best we can do is assume\n",
    "            # it is the median of the multivariate distribution. The dispersion\n",
    "            # is assumed to be negligible.\n",
    "            dim = len(demand_data[0])\n",
    "            if dim > 1:\n",
    "                sig = np.abs(demand_data[0])*1e-6\n",
    "                rho = np.zeros((dim,dim))\n",
    "                np.fill_diagonal(rho, 1.0)\n",
    "                COV = np.outer(sig,sig) * rho\n",
    "            else:\n",
    "                COV = np.abs(demand_data[0][0])*(1e-6)**2.0\n",
    "\n",
    "            demand_RV = RandomVariable(ID=200, dimension_tags=d_tags,\n",
    "                                       distribution_kind='lognormal',\n",
    "                                       theta=demand_data[0],\n",
    "                                       COV=COV)\n",
    "\n",
    "        # To consider additional uncertainty in EDPs, we need to redefine the\n",
    "        # random variable. If the EDP distribution is set to 'empirical' then\n",
    "        # adding uncertainty by increasing its variance is not possible.\n",
    "        if ((self.beta_tot is not None) and\n",
    "            (GI['response']['EDP_distribution'] != 'empirical')):\n",
    "            # determine the covariance matrix with added uncertainty\n",
    "            if demand_RV.COV.shape != ():\n",
    "                sig_mod = np.sqrt(demand_RV.sig ** 2. + self.beta_tot ** 2.)\n",
    "                COV_mod = np.outer(sig_mod, sig_mod) * demand_RV.corr\n",
    "            else:\n",
    "                COV_mod = np.sqrt(demand_RV.COV**2. + self.beta_tot**2.)\n",
    "\n",
    "            # redefine the random variable\n",
    "            demand_RV = RandomVariable(\n",
    "                ID=200,\n",
    "                dimension_tags=demand_RV.dimension_tags,\n",
    "                distribution_kind=demand_RV.distribution_kind,\n",
    "                theta=demand_RV.theta,\n",
    "                COV=COV_mod)\n",
    "\n",
    "        return demand_RV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAZUS_Assessment_BLZ(Assessment):\n",
    "    \"\"\"\n",
    "    An Assessment class that implements the damage and loss assessment method\n",
    "    following the HAZUS Technical Manual and the HAZUS software.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hazard:  {'EQ', 'HU'}\n",
    "        Identifies the type of hazard. EQ corresponds to earthquake, HU\n",
    "        corresponds to hurricane.\n",
    "        default: 'EQ'.\n",
    "    inj_lvls: int\n",
    "        Defines the discretization used to describe the severity of injuries.\n",
    "        The HAZUS earthquake methodology uses 4 levels.\n",
    "        default: 4\n",
    "    \"\"\"\n",
    "    def __init__(self, hazard='EQ', inj_lvls = 4):\n",
    "        super(HAZUS_Assessment, self).__init__()\n",
    "\n",
    "        self._inj_lvls = inj_lvls\n",
    "        self._hazard = hazard\n",
    "        self._assessment_type = 'HAZUS_{}'.format(hazard)\n",
    "\n",
    "    def read_inputs(self, path_DL_input, path_EDP_input, verbose=False):\n",
    "        \"\"\"\n",
    "        Read and process the input files to describe the loss assessment task.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_DL_input: string\n",
    "            Location of the Damage and Loss input file. The file is expected to\n",
    "            be a JSON with data stored in a standard format described in detail\n",
    "            in the Input section of the documentation.\n",
    "        path_EDP_input: string\n",
    "            Location of the EDP input file. The file is expected to follow the\n",
    "            output formatting of Dakota. The Input section of the documentation\n",
    "            provides more information about the expected formatting.\n",
    "        verbose: boolean, default: False\n",
    "            If True, the method echoes the information read from the files.\n",
    "            This can be useful to ensure that the information in the file is\n",
    "            properly read by the method.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(HAZUS_Assessment, self).read_inputs(path_DL_input,\n",
    "                                                  path_EDP_input, verbose)\n",
    "\n",
    "        # assume that the asset is a building\n",
    "        # TODO: If we want to apply HAZUS to non-building assets, several parts of this methodology need to be extended.\n",
    "        BIM = self._AIM_in\n",
    "\n",
    "        # read component and population data ----------------------------------\n",
    "        # components\n",
    "        self._FG_in = read_component_DL_data(\n",
    "            self._AIM_in['data_sources']['path_CMP_data'], BIM['components'],\n",
    "            assessment_type=self._assessment_type, verbose=verbose)\n",
    "\n",
    "        # population (if needed)\n",
    "        if self._AIM_in['decision_variables']['injuries']:\n",
    "            POP = read_population_distribution(\n",
    "                self._AIM_in['data_sources']['path_POP_data'],\n",
    "                BIM['general']['occupancy_type'],\n",
    "                assessment_type=self._assessment_type,\n",
    "                verbose=verbose)\n",
    "\n",
    "            POP['peak'] = BIM['general']['population']\n",
    "            self._POP_in = POP\n",
    "\n",
    "    def define_random_variables(self):\n",
    "        \"\"\"\n",
    "        Define the random variables used for loss assessment.\n",
    "\n",
    "        Following the HAZUS methodology, only the groups of parameters below\n",
    "        are considered random. Correlations within groups are not considered\n",
    "        because each Fragility Group has only one Performance Group with a\n",
    "        in this implementation.\n",
    "\n",
    "        1. Demand (EDP) distribution\n",
    "\n",
    "        Describe the uncertainty in the demands. Unlike other random variables,\n",
    "        the EDPs are characterized by the EDP input data provided earlier. All\n",
    "        EDPs are handled in one multivariate lognormal distribution. If more\n",
    "        than one sample is provided, the distribution is fit to the EDP data.\n",
    "        Otherwise, the provided data point is assumed to be the median value\n",
    "        and the additional uncertainty prescribed describes the dispersion. See\n",
    "        _create_RV_demands() for more details.\n",
    "\n",
    "        2. Fragility EDP limits\n",
    "\n",
    "        Describe the uncertainty in the EDP limit that corresponds to\n",
    "        exceedance of each Damage State. EDP limits are grouped by Fragility\n",
    "        Groups. See _create_RV_fragilities() for details.\n",
    "\n",
    "        \"\"\"\n",
    "        super(HAZUS_Assessment, self).define_random_variables()\n",
    "\n",
    "        # create the random variables -----------------------------------------\n",
    "        self._RV_dict = {}\n",
    "\n",
    "        # fragilities 300\n",
    "        s_fg_keys = sorted(self._FG_in.keys())\n",
    "        for c_id, c_name in enumerate(s_fg_keys):\n",
    "            comp = self._FG_in[c_name]\n",
    "\n",
    "            self._RV_dict.update({\n",
    "                'FR-' + c_name:\n",
    "                    self._create_RV_fragilities(c_id, comp,'PG')})\n",
    "\n",
    "        # demands 200\n",
    "        self._RV_dict.update({'EDP': self._create_RV_demands()})\n",
    "\n",
    "        # sample the random variables -----------------------------------------\n",
    "        realization_count = self._AIM_in['general']['realizations']\n",
    "        is_coupled = self._AIM_in['general']\n",
    "\n",
    "        s_rv_keys = sorted(self._RV_dict.keys())\n",
    "        for r_i in s_rv_keys:\n",
    "            rv = self._RV_dict[r_i]\n",
    "            if rv is not None:\n",
    "                rv.sample_distribution(\n",
    "                    sample_size=realization_count, preserve_order=is_coupled)\n",
    "\n",
    "    def define_loss_model(self):\n",
    "        \"\"\"\n",
    "        Create the stochastic loss model based on the inputs provided earlier.\n",
    "\n",
    "        Following the HAZUS methodology, the component assemblies specified in\n",
    "        the Damage and Loss input file are used to create Fragility Groups.\n",
    "        Each Fragility Group corresponds to one assembly that represents every\n",
    "        component of the given type in the structure. See\n",
    "        _create_fragility_groups() for more details about the creation of\n",
    "        Fragility Groups.\n",
    "\n",
    "        \"\"\"\n",
    "        super(HAZUS_Assessment, self).define_loss_model()\n",
    "\n",
    "        # fragility groups\n",
    "        self._FG_dict = self._create_fragility_groups()\n",
    "\n",
    "        # demands\n",
    "        self._EDP_dict = dict(\n",
    "            [(tag, RandomVariableSubset(self._RV_dict['EDP'], tags=tag))\n",
    "             for tag in self._RV_dict['EDP']._dimension_tags])\n",
    "\n",
    "    def calculate_damage(self):\n",
    "        \"\"\"\n",
    "        Characterize the damage experienced in each random event realization.\n",
    "\n",
    "        First, the time of the event (month, weekday/weekend, hour) is randomly\n",
    "        generated for each realization. Given the event time, the number of\n",
    "        people present at each floor of the building is calculated.\n",
    "\n",
    "        Next, the quantities of components in each damage state are estimated.\n",
    "        See _calc_damage() for more details on damage estimation.\n",
    "\n",
    "        \"\"\"\n",
    "        super(HAZUS_Assessment, self).calculate_damage()\n",
    "\n",
    "        # event time - month, weekday, and hour realizations\n",
    "        self._TIME = self._sample_event_time()\n",
    "\n",
    "        # if we are interested in injuries...\n",
    "        if self._AIM_in['decision_variables']['injuries']:\n",
    "            # get the population conditioned on event time\n",
    "            self._POP = self._get_population()\n",
    "\n",
    "        # collapses are handled as the ultimate DS in HAZUS\n",
    "        self._ID_dict.update({'collapse': []})\n",
    "\n",
    "        # select the non-collapse cases for further analyses\n",
    "        non_collapsed_IDs = self._TIME.index.values.astype(int)\n",
    "        self._ID_dict.update({'non-collapse': non_collapsed_IDs})\n",
    "\n",
    "        # damage in non-collapses\n",
    "        self._DMG = self._calc_damage()\n",
    "\n",
    "    def calculate_losses(self):\n",
    "        \"\"\"\n",
    "        Characterize the consequences of damage in each random event realization.\n",
    "\n",
    "        For the sake of efficiency, only the decision variables requested in\n",
    "        the input file are estimated. The following consequences are handled by\n",
    "        this method for a HAZUS assessment:\n",
    "\n",
    "        Reconstruction time and cost\n",
    "        Get a cost and time estimate for each Damage State in each Performance\n",
    "        Group. For more information about estimating reconstruction cost and\n",
    "        time see _calc_repair_cost_and_time() methods.\n",
    "\n",
    "        Injuries\n",
    "        The number of injuries are based on the probability of injuries of\n",
    "        various severity specified in the component data file. For more\n",
    "        information about estimating injuries _calc_non_collapse_injuries.\n",
    "\n",
    "        \"\"\"\n",
    "        super(HAZUS_Assessment, self).calculate_losses()\n",
    "        DVs = self._AIM_in['decision_variables']\n",
    "\n",
    "        # reconstruction cost and time\n",
    "        if DVs['rec_cost'] or DVs['rec_time']:\n",
    "            # all damages are considered repairable in HAZUS\n",
    "            repairable_IDs = self._ID_dict['non-collapse']\n",
    "            self._ID_dict.update({'repairable': repairable_IDs})\n",
    "            self._ID_dict.update({'irrepairable': []})\n",
    "\n",
    "            # reconstruction cost and time for repairable cases\n",
    "            DV_COST, DV_TIME = self._calc_repair_cost_and_time()\n",
    "\n",
    "            if DVs['rec_cost']:\n",
    "                self._DV_dict.update({'rec_cost': DV_COST})\n",
    "\n",
    "            if DVs['rec_time']:\n",
    "                self._DV_dict.update({'rec_time': DV_TIME})\n",
    "\n",
    "        # injuries due to collapse\n",
    "        if DVs['injuries']:\n",
    "            # there are no separate collapse cases in HAZUS\n",
    "\n",
    "            # injuries in non-collapsed cases\n",
    "            DV_INJ_dict = self._calc_non_collapse_injuries()\n",
    "\n",
    "            # store result\n",
    "            self._DV_dict.update({'injuries': DV_INJ_dict})\n",
    "\n",
    "    def aggregate_results(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        DVs = self._AIM_in['decision_variables']\n",
    "\n",
    "        MI_raw = [\n",
    "            ('event time', 'month'),\n",
    "            ('event time', 'weekday?'),\n",
    "            ('event time', 'hour'),\n",
    "            ('reconstruction', 'cost'),\n",
    "        ]\n",
    "\n",
    "        if DVs['rec_time']:\n",
    "            MI_raw += [\n",
    "                ('reconstruction', 'time'),\n",
    "            ]\n",
    "\n",
    "        if DVs['injuries']:\n",
    "            MI_raw += [\n",
    "                ('inhabitants', ''),\n",
    "                ('injuries', 'sev. 1'),\n",
    "                ('injuries', 'sev. 2'),\n",
    "                ('injuries', 'sev. 3'),\n",
    "                ('injuries', 'sev. 4'),\n",
    "            ]\n",
    "\n",
    "        ncID = self._ID_dict['non-collapse']\n",
    "        colID = self._ID_dict['collapse']\n",
    "        if DVs['rec_cost'] or DVs['rec_time']:\n",
    "            repID = self._ID_dict['repairable']\n",
    "            irID = self._ID_dict['irrepairable']\n",
    "\n",
    "        MI = pd.MultiIndex.from_tuples(MI_raw)\n",
    "\n",
    "        SUMMARY = pd.DataFrame(np.empty((\n",
    "            self._AIM_in['general']['realizations'],\n",
    "            len(MI))), columns=MI)\n",
    "        SUMMARY[:] = np.NaN\n",
    "\n",
    "        # event time\n",
    "        for prop in ['month', 'weekday?', 'hour']:\n",
    "            offset = 0\n",
    "            if prop == 'month':\n",
    "                offset = 1\n",
    "            SUMMARY.loc[:, ('event time', prop)] = \\\n",
    "                self._TIME.loc[:, prop] + offset\n",
    "\n",
    "        # inhabitants\n",
    "        if DVs['injuries']:\n",
    "            SUMMARY.loc[:, ('inhabitants', '')] = self._POP.sum(axis=1)\n",
    "\n",
    "        # reconstruction cost\n",
    "        if DVs['rec_cost']:\n",
    "            repl_cost = self._AIM_in['general']['replacement_cost']\n",
    "\n",
    "            SUMMARY.loc[ncID, ('reconstruction', 'cost')] = \\\n",
    "                self._DV_dict['rec_cost'].sum(axis=1)\n",
    "            #SUMMARY.loc[:, ('reconstruction', 'cost')] *= repl_cost\n",
    "\n",
    "        # reconstruction time\n",
    "        if DVs['rec_time']:\n",
    "            SUMMARY.loc[ncID, ('reconstruction', 'time')] = \\\n",
    "                self._DV_dict['rec_time'].sum(axis=1)\n",
    "\n",
    "        # injuries\n",
    "        if DVs['injuries']:\n",
    "            for sev_id in range(4):\n",
    "                sev_tag = 'sev. {}'.format(sev_id+1)\n",
    "                SUMMARY.loc[ncID, ('injuries', sev_tag)] = \\\n",
    "                    self._DV_dict['injuries'][sev_id].sum(axis=1)\n",
    "\n",
    "        self._SUMMARY = SUMMARY.dropna(axis=1, how='all')\n",
    "\n",
    "    def _create_RV_fragilities(self, c_id, comp, rho_fr):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        c_id\n",
    "        comp\n",
    "        rho_fr\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare the basic multivariate distribution data for one component subgroup considering all damage states\n",
    "        d_theta, d_sig, d_tag, d_distr_kind = [np.array([]) for i in range(4)]\n",
    "\n",
    "        s_dsg_keys = sorted(comp['DSG_set'].keys())\n",
    "        for d_id in s_dsg_keys:\n",
    "            DSG = comp['DSG_set'][d_id]\n",
    "            d_theta = np.append(d_theta, DSG['theta'])\n",
    "            d_sig = np.append(d_sig, DSG['sig'])\n",
    "            d_tag = np.append(d_tag, comp['ID'] + '-' + str(d_id))\n",
    "            d_distr_kind = np.append(d_distr_kind, DSG['distribution_kind'])\n",
    "        dims = len(d_theta)\n",
    "\n",
    "        # get the total number of random variables for this fragility group\n",
    "        # TODO: add the possibility of multiple locations and directions\n",
    "        #rv_count = len(comp['locations']) * len(comp['directions']) * dims\n",
    "        rv_count = sum([len(csg_w) for csg_w in comp['csg_weights']]) * dims\n",
    "\n",
    "        # create the (empty) input arrays for the RV\n",
    "        c_theta = np.zeros(rv_count)\n",
    "        c_tag = np.empty(rv_count, dtype=object)\n",
    "        c_sig = np.zeros(rv_count)\n",
    "        c_distr_kind = np.empty(rv_count, dtype=object)\n",
    "\n",
    "        pos_id = 0\n",
    "        #for l_id in comp['locations']:\n",
    "        #    # for each location-direction pair)\n",
    "        #    for d_id, __ in enumerate(comp['directions']):\n",
    "        #        # for each component-subgroup\n",
    "        #        c_theta[pos_id:pos_id + dims] = d_theta\n",
    "        #        c_sig[pos_id:pos_id + dims] = d_sig\n",
    "        #        c_tag[pos_id:pos_id + dims] = [\n",
    "        #            t + '-LOC-{}-CSG-{}'.format(l_id, d_id) for t in d_tag]\n",
    "        #        c_distr_kind[pos_id:pos_id + dims] = d_distr_kind\n",
    "        #        pos_id += dims\n",
    "\n",
    "        for l_id, d_id, csg_list in zip(comp['locations'], comp['directions'],\n",
    "                                        comp['csg_weights']):\n",
    "            # for each location-direction pair)\n",
    "            for csg_id, __ in enumerate(csg_list):\n",
    "                # for each component-subgroup\n",
    "                c_theta[pos_id:pos_id + dims] = d_theta\n",
    "                c_sig[pos_id:pos_id + dims] = d_sig\n",
    "                c_tag[pos_id:pos_id + dims] = [\n",
    "                    t + '-LOC-{}-DIR-{}-CSG-{}'.format(l_id, d_id, csg_id)\n",
    "                    for t in d_tag]\n",
    "                c_distr_kind[pos_id:pos_id + dims] = d_distr_kind\n",
    "                pos_id += dims\n",
    "\n",
    "        # create the covariance matrix\n",
    "        #c_rho = self._create_correlation_matrix(rho_fr, c_target=c_id,\n",
    "        #                                        include_DSG=True,\n",
    "        #                                        include_CSG=True)\n",
    "        c_rho = np.ones((rv_count, rv_count))\n",
    "        c_COV = np.outer(c_sig, c_sig) * c_rho\n",
    "\n",
    "        if c_tag.size > 0:\n",
    "            fragility_RV = RandomVariable(ID=300 + c_id,\n",
    "                                          dimension_tags=c_tag,\n",
    "                                          distribution_kind=c_distr_kind,\n",
    "                                          theta=c_theta,\n",
    "                                          COV=c_COV)\n",
    "        else:\n",
    "            fragility_RV = None\n",
    "\n",
    "        return fragility_RV\n",
    "\n",
    "    def _create_fragility_groups(self):\n",
    "\n",
    "        RVd = self._RV_dict\n",
    "        DVs = self._AIM_in['decision_variables']\n",
    "\n",
    "        # use the building replacement cost to calculate the absolute\n",
    "        # reconstruction cost for component groups\n",
    "        repl_cost = self._AIM_in['general']['replacement_cost']\n",
    "\n",
    "        # create a list for the fragility groups\n",
    "        FG_dict = dict()\n",
    "\n",
    "        s_fg_keys = sorted(self._FG_in.keys())\n",
    "        for c_id in s_fg_keys:\n",
    "            comp = self._FG_in[c_id]\n",
    "\n",
    "            FG_ID = len(FG_dict.keys()) + 1\n",
    "\n",
    "            # create a list for the performance groups\n",
    "            performance_groups = []\n",
    "\n",
    "            # one group for each of the stories prescribed by the user\n",
    "            PG_locations = comp['locations']\n",
    "            PG_directions = comp['directions']\n",
    "            PG_csg_lists = comp['csg_weights']\n",
    "            for loc, dir_, csg_list in zip(PG_locations, PG_directions,\n",
    "                                           PG_csg_lists):\n",
    "                PG_ID = 1000 * FG_ID + 10 * loc + dir_\n",
    "\n",
    "                # get the quantity\n",
    "                QNT = None\n",
    "                #QNT = RandomVariableSubset(\n",
    "                #    RVd['QNT'],\n",
    "                #    tags=[c_id + '-QNT-' + str(loc) + '-' + str(dir_), ])\n",
    "\n",
    "                # create the damage objects\n",
    "                # consequences are calculated on a performance group level\n",
    "\n",
    "                # create a list for the damage state groups and their tags\n",
    "                DSG_list = []\n",
    "                d_tags = []\n",
    "                s_dsg_keys = sorted(comp['DSG_set'].keys())\n",
    "                for dsg_i, DSG_ID in enumerate(s_dsg_keys):\n",
    "                    DSG = comp['DSG_set'][DSG_ID]\n",
    "                    d_tags.append(c_id + '-' + DSG_ID)\n",
    "\n",
    "                    # create a list for the damage states\n",
    "                    DS_set = []\n",
    "\n",
    "                    s_ds_keys = sorted(DSG['DS_set'].keys())\n",
    "                    for ds_i, DS_ID in enumerate(s_ds_keys):\n",
    "                        DS = DSG['DS_set'][DS_ID]\n",
    "\n",
    "                        # create the consequence functions\n",
    "                        # note: consequences in HAZUS are conditioned on\n",
    "                        # damage with no added uncertainty\n",
    "\n",
    "                        if DVs['rec_cost']:\n",
    "                            data = DS['repair_cost']\n",
    "                            f_median = prep_constant_median_DV(data*repl_cost)\n",
    "                            CF_cost = ConsequenceFunction(\n",
    "                                DV_median=f_median,\n",
    "                                DV_distribution=None)\n",
    "                        else:\n",
    "                            CF_cost = None\n",
    "\n",
    "                        if DVs['rec_time'] and ('repair_time' in DS.keys()):\n",
    "                            data = DS['repair_time']\n",
    "                            f_median = prep_constant_median_DV(data)\n",
    "                            CF_time = ConsequenceFunction(\n",
    "                                DV_median=f_median,\n",
    "                                DV_distribution=None)\n",
    "                        else:\n",
    "                            CF_time = None\n",
    "\n",
    "                        # note: no red tag in HAZUS assessments\n",
    "\n",
    "                        if (DVs['injuries']) and ('injuries' in DS.keys()):\n",
    "                            CF_inj_set = []\n",
    "                            for inj_i, theta in enumerate(\n",
    "                                DS['injuries']):\n",
    "                                if theta > 0.:\n",
    "                                    f_median = prep_constant_median_DV(\n",
    "                                        theta)\n",
    "                                    CF_inj_set.append(ConsequenceFunction(\n",
    "                                        DV_median=f_median,\n",
    "                                        DV_distribution=None))\n",
    "                                else:\n",
    "                                    CF_inj_set.append(None)\n",
    "                        else:\n",
    "                            CF_inj_set = [None, ]\n",
    "\n",
    "                        DS_set.append(DamageState(ID=ds_i + 1,\n",
    "                                                  description=DS[\n",
    "                                                      'description'],\n",
    "                                                  weight=DS['weight'],\n",
    "                                                  repair_cost_CF=CF_cost,\n",
    "                                                  reconstruction_time_CF=CF_time,\n",
    "                                                  injuries_CF_set=CF_inj_set\n",
    "                                                  ))\n",
    "\n",
    "                    # add the DSG to the list\n",
    "                    DSG_list.append(DamageStateGroup(ID=dsg_i + 1,\n",
    "                                                     DS_set=DS_set,\n",
    "                                                     DS_set_kind=DSG[\n",
    "                                                         'DS_set_kind']\n",
    "                                                     ))\n",
    "\n",
    "                # create the fragility functions\n",
    "                FF_set = []\n",
    "                #CSG_this = np.where(comp['directions'] == dir_)[0]\n",
    "                #PG_weights = np.asarray(comp['csg_weights'])[CSG_this]\n",
    "                # normalize the weights\n",
    "                #PG_weights /= sum(PG_weights)\n",
    "                for csg_id, __ in enumerate(csg_list):\n",
    "                    # assign the appropriate random variable to the fragility\n",
    "                    # function\n",
    "                    ff_tags = [t + '-LOC-{}-DIR-{}-CSG-{}'.format(loc, dir_,\n",
    "                                                                  csg_id)\n",
    "                               for t in d_tags]\n",
    "                    EDP_limit = RandomVariableSubset(RVd['FR-' + c_id],\n",
    "                                                     tags=ff_tags)\n",
    "                    FF_set.append(FragilityFunction(EDP_limit))\n",
    "\n",
    "                # create the performance group\n",
    "                PG = PerformanceGroup(ID=PG_ID,\n",
    "                                      location=loc,\n",
    "                                      quantity=QNT,\n",
    "                                      fragility_functions=FF_set,\n",
    "                                      DSG_set=DSG_list,\n",
    "                                      csg_weights=csg_list,\n",
    "                                      direction=dir_\n",
    "                                      )\n",
    "                performance_groups.append(PG)\n",
    "\n",
    "            # create the fragility group\n",
    "            FG = FragilityGroup(ID=FG_ID,\n",
    "                                #kind=comp['kind'],\n",
    "                                demand_type=comp['demand_type'],\n",
    "                                performance_groups=performance_groups,\n",
    "                                directional=comp['directional'],\n",
    "                                correlation=comp['correlation'],\n",
    "                                demand_location_offset=comp['offset'],\n",
    "                                incomplete=comp['incomplete'],\n",
    "                                name=str(FG_ID) + ' - ' + comp['ID'],\n",
    "                                description=comp['description']\n",
    "                                )\n",
    "\n",
    "            FG_dict.update({comp['ID']: FG})\n",
    "\n",
    "        return FG_dict\n",
    "\n",
    "    def _sample_event_time(self):\n",
    "\n",
    "        sample_count = self._AIM_in['general']['realizations']\n",
    "\n",
    "        # month - uniform distribution over [0,11]\n",
    "        month = np.random.randint(0, 12, size=sample_count)\n",
    "\n",
    "        # weekday - binomial with p=5/7\n",
    "        weekday = np.random.binomial(1, 5. / 7., size=sample_count)\n",
    "\n",
    "        # hour - uniform distribution over [0,23]\n",
    "        hour = np.random.randint(0, 24, size=sample_count)\n",
    "\n",
    "        data = pd.DataFrame(data={'month'   : month,\n",
    "                                  'weekday?': weekday,\n",
    "                                  'hour'    : hour},\n",
    "                            dtype=int)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _get_population(self):\n",
    "        \"\"\"\n",
    "        Use the population characteristics to generate random population samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        POPin = self._POP_in\n",
    "        TIME = self._TIME\n",
    "\n",
    "        POP = pd.DataFrame(\n",
    "            np.ones((len(TIME.index), len(POPin['peak']))) * POPin['peak'],\n",
    "            columns=['LOC' + str(loc + 1)\n",
    "                     for loc in range(len(POPin['peak']))])\n",
    "\n",
    "        weekdays = TIME[TIME['weekday?'] == 1].index\n",
    "        weekends = TIME[~TIME.index.isin(weekdays)].index\n",
    "\n",
    "        for col in POP.columns.values:\n",
    "            POP.loc[weekdays, col] = (\n",
    "                POP.loc[weekdays, col] *\n",
    "                np.array(POPin['weekday']['daily'])[\n",
    "                    TIME.loc[weekdays, 'hour'].values.astype(int)] *\n",
    "                np.array(POPin['weekday']['monthly'])[\n",
    "                    TIME.loc[weekdays, 'month'].values.astype(int)])\n",
    "\n",
    "            POP.loc[weekends, col] = (\n",
    "                POP.loc[weekends, col] *\n",
    "                np.array(POPin['weekend']['daily'])[\n",
    "                    TIME.loc[weekends, 'hour'].values.astype(int)] *\n",
    "                np.array(POPin['weekend']['monthly'])[\n",
    "                    TIME.loc[weekends, 'month'].values.astype(int)])\n",
    "\n",
    "        return POP\n",
    "\n",
    "    def _calc_damage(self):\n",
    "\n",
    "        ncID = self._ID_dict['non-collapse']\n",
    "        NC_samples = len(ncID)\n",
    "        DMG = pd.DataFrame()\n",
    "\n",
    "        s_fg_keys = sorted(self._FG_dict.keys())\n",
    "        for fg_id in s_fg_keys:\n",
    "            FG = self._FG_dict[fg_id]\n",
    "\n",
    "            PG_set = FG._performance_groups\n",
    "\n",
    "            DS_list = []\n",
    "            for DSG in PG_set[0]._DSG_set:\n",
    "                for DS in DSG._DS_set:\n",
    "                    DS_list.append(str(DSG._ID) + '-' + str(DS._ID))\n",
    "            d_count = len(DS_list)\n",
    "\n",
    "            MI = pd.MultiIndex.from_product([[FG._ID, ],\n",
    "                                             [pg._ID for pg in PG_set],\n",
    "                                             DS_list],\n",
    "                                            names=['FG', 'PG', 'DS'])\n",
    "\n",
    "            FG_damages = pd.DataFrame(np.zeros((NC_samples, len(MI))),\n",
    "                                      columns=MI,\n",
    "                                      index=ncID)\n",
    "\n",
    "            for pg_i, PG in enumerate(PG_set):\n",
    "\n",
    "                PG_ID = PG._ID\n",
    "                if PG._quantity is not None:\n",
    "                    PG_qnt = PG._quantity.samples.loc[ncID]\n",
    "                else:\n",
    "                    PG_qnt = pd.DataFrame(np.ones(NC_samples),index=ncID)\n",
    "\n",
    "                # get the corresponding demands\n",
    "                demand_ID = (FG._demand_type +\n",
    "                             '-LOC-' + str(PG._location + FG._demand_location_offset) +\n",
    "                             '-DIR-' + str(PG._direction))\n",
    "                if demand_ID in self._EDP_dict.keys():\n",
    "                    EDP_samples = self._EDP_dict[demand_ID].samples.loc[ncID]\n",
    "                else:\n",
    "                    # If the required demand is not available, then we are most\n",
    "                    # likely analyzing a 3D structure using results from a 2D\n",
    "                    # simulation. The best thing we can do in that particular\n",
    "                    # case is to use the EDP from the 1 direction for all other\n",
    "                    # directions.\n",
    "                    demand_ID = (FG._demand_type +\n",
    "                                 '-LOC-' + str(PG._location + FG._demand_location_offset) + '-DIR-1')\n",
    "                    EDP_samples = self._EDP_dict[demand_ID].samples.loc[ncID]\n",
    "\n",
    "                csg_w_list = PG._csg_weights\n",
    "\n",
    "                for csg_i, csg_w in enumerate(csg_w_list):\n",
    "                    DSG_df = PG._FF_set[csg_i].DSG_given_EDP(EDP_samples)\n",
    "\n",
    "                    for DSG in PG._DSG_set:\n",
    "                        in_this_DSG = DSG_df[DSG_df.values == DSG._ID].index\n",
    "                        if DSG._DS_set_kind == 'single':\n",
    "                            DS = DSG._DS_set[0]\n",
    "                            DS_tag = str(DSG._ID) + '-' + str(DS._ID)\n",
    "                            FG_damages.loc[in_this_DSG,\n",
    "                                           (FG._ID, PG_ID, DS_tag)] += csg_w\n",
    "                        elif DSG._DS_set_kind == 'mutually exclusive':\n",
    "                            DS_weights = [DS._weight for DS in DSG._DS_set]\n",
    "                            DS_RV = RandomVariable(\n",
    "                                ID=-1, dimension_tags=['me_DS', ],\n",
    "                                distribution_kind='multinomial',\n",
    "                                p_set=DS_weights)\n",
    "                            DS_df = DS_RV.sample_distribution(\n",
    "                                len(in_this_DSG)) + 1\n",
    "                            for DS in DSG._DS_set:\n",
    "                                DS_tag = str(DSG._ID) + '-' + str(DS._ID)\n",
    "                                in_this_DS = DS_df[DS_df.values == DS._ID].index\n",
    "                                FG_damages.loc[in_this_DSG[in_this_DS],\n",
    "                                               (FG._ID, PG_ID, DS_tag)] += csg_w\n",
    "                        elif DSG._DS_set_kind == 'simultaneous':\n",
    "                            DS_weights = [DS._weight for DS in DSG._DS_set]\n",
    "                            DS_df = np.random.uniform(\n",
    "                                size=(len(in_this_DSG), len(DS_weights)))\n",
    "                            which_DS = DS_df < DS_weights\n",
    "                            any_DS = np.any(which_DS, axis=1)\n",
    "                            no_DS_ids = np.where(any_DS == False)[0]\n",
    "\n",
    "                            while len(no_DS_ids) > 0:\n",
    "                                DS_df_add = np.random.uniform(\n",
    "                                    size=(len(no_DS_ids), len(DS_weights)))\n",
    "                                which_DS_add = DS_df_add < DS_weights\n",
    "                                which_DS[no_DS_ids] = which_DS_add\n",
    "\n",
    "                                any_DS = np.any(which_DS_add, axis=1)\n",
    "                                no_DS_ids = no_DS_ids[\n",
    "                                    np.where(any_DS == False)[0]]\n",
    "\n",
    "                            for ds_i, DS in enumerate(DSG._DS_set):\n",
    "                                DS_tag = str(DSG._ID) + '-' + str(DS._ID)\n",
    "                                in_this_DS = which_DS[:, ds_i]\n",
    "                                FG_damages.loc[in_this_DSG[in_this_DS], (\n",
    "                                    FG._ID, PG_ID, DS_tag)] += csg_w\n",
    "\n",
    "                        else:\n",
    "                            raise ValueError(\n",
    "                                \"Unknown damage state type: {}\".format(\n",
    "                                    DSG._DS_set_kind)\n",
    "                            )\n",
    "\n",
    "                FG_damages.iloc[:, pg_i * d_count:(pg_i + 1) * d_count] = \\\n",
    "                    FG_damages.mul(PG_qnt.iloc[:, 0], axis=0)\n",
    "\n",
    "            DMG = pd.concat((DMG, FG_damages), axis=1)\n",
    "\n",
    "        DMG.index = ncID\n",
    "\n",
    "        # sort the columns to enable index slicing later\n",
    "        DMG = DMG.sort_index(axis=1, ascending=True)\n",
    "\n",
    "        return DMG\n",
    "\n",
    "    def _calc_repair_cost_and_time(self):\n",
    "\n",
    "        idx = pd.IndexSlice\n",
    "        DVs = self._AIM_in['decision_variables']\n",
    "\n",
    "        DMG_by_FG_and_DS = self._DMG.groupby(level=[0, 2], axis=1).sum()\n",
    "\n",
    "        repID = self._ID_dict['repairable']\n",
    "        REP_samples = len(repID)\n",
    "        DV_COST = pd.DataFrame(np.zeros((REP_samples, len(self._DMG.columns))),\n",
    "                               columns=self._DMG.columns, index=repID)\n",
    "        DV_TIME = deepcopy(DV_COST)\n",
    "\n",
    "        s_fg_keys = sorted(self._FG_dict.keys())\n",
    "        for fg_id in s_fg_keys:\n",
    "            FG = self._FG_dict[fg_id]\n",
    "\n",
    "            PG_set = FG._performance_groups\n",
    "\n",
    "            DS_list = self._DMG.loc[:, idx[FG._ID, PG_set[0]._ID, :]].columns\n",
    "            DS_list = DS_list.levels[2][DS_list.codes[2]].values\n",
    "\n",
    "            for pg_i, PG in enumerate(PG_set):\n",
    "\n",
    "                PG_ID = PG._ID\n",
    "\n",
    "                for d_i, d_tag in enumerate(DS_list):\n",
    "                    dsg_i = int(d_tag[0]) - 1\n",
    "                    ds_i = int(d_tag[-1]) - 1\n",
    "\n",
    "                    DS = PG._DSG_set[dsg_i]._DS_set[ds_i]\n",
    "\n",
    "                    TOT_qnt = DMG_by_FG_and_DS.loc[repID, (FG._ID, d_tag)]\n",
    "                    PG_qnt = self._DMG.loc[repID,\n",
    "                                           (FG._ID, PG_ID, d_tag)]\n",
    "\n",
    "                    # repair cost\n",
    "                    if DVs['rec_cost']:\n",
    "                        COST_samples = DS.unit_repair_cost(quantity=TOT_qnt)\n",
    "                        if COST_samples is not None:\n",
    "                            DV_COST.loc[:,\n",
    "                            (FG._ID, PG_ID, d_tag)] = COST_samples * PG_qnt\n",
    "\n",
    "                    if DVs['rec_time']:\n",
    "                        # repair time\n",
    "                        TIME_samples = DS.unit_reconstruction_time(\n",
    "                            quantity=TOT_qnt)\n",
    "                        if TIME_samples is not None:\n",
    "                            DV_TIME.loc[:,\n",
    "                            (FG._ID, PG_ID, d_tag)] = TIME_samples * PG_qnt\n",
    "\n",
    "        # sort the columns to enable index slicing later\n",
    "        if DVs['rec_cost']:\n",
    "            DV_COST = DV_COST.sort_index(axis=1, ascending=True)\n",
    "        else:\n",
    "            DV_COST = None\n",
    "        if DVs['rec_time']:\n",
    "            DV_TIME = DV_TIME.sort_index(axis=1, ascending=True)\n",
    "        else:\n",
    "            DV_TIME = None\n",
    "\n",
    "        return DV_COST, DV_TIME\n",
    "\n",
    "    def _calc_non_collapse_injuries(self):\n",
    "\n",
    "        idx = pd.IndexSlice\n",
    "\n",
    "        ncID = self._ID_dict['non-collapse']\n",
    "        NC_samples = len(ncID)\n",
    "        DV_INJ_dict = dict([(i, pd.DataFrame(np.zeros((NC_samples,\n",
    "                                                       len(self._DMG.columns))),\n",
    "                                             columns=self._DMG.columns,\n",
    "                                             index=ncID))\n",
    "                            for i in range(self._inj_lvls)])\n",
    "        s_fg_keys = sorted(self._FG_dict.keys())\n",
    "        for fg_id in s_fg_keys:\n",
    "            FG = self._FG_dict[fg_id]\n",
    "\n",
    "            PG_set = FG._performance_groups\n",
    "\n",
    "            DS_list = self._DMG.loc[:, idx[FG._ID, PG_set[0]._ID, :]].columns\n",
    "            DS_list = DS_list.levels[2][DS_list.codes[2]].values\n",
    "\n",
    "            for pg_i, PG in enumerate(PG_set):\n",
    "\n",
    "                PG_ID = PG._ID\n",
    "\n",
    "                for d_i, d_tag in enumerate(DS_list):\n",
    "                    dsg_i = int(d_tag[0]) - 1\n",
    "                    ds_i = int(d_tag[-1]) - 1\n",
    "\n",
    "                    DS = PG._DSG_set[dsg_i]._DS_set[ds_i]\n",
    "\n",
    "                    P_affected = self._POP.loc[ncID]\n",
    "\n",
    "                    QNT = self._DMG.loc[:, (FG._ID, PG_ID, d_tag)]\n",
    "\n",
    "                    # estimate injuries\n",
    "                    for i in range(self._inj_lvls):\n",
    "                        INJ_samples = DS.unit_injuries(severity_level=i,\n",
    "                                                       sample_size=NC_samples)\n",
    "                        if INJ_samples is not None:\n",
    "                            P_aff_i = P_affected.loc[:,\n",
    "                                      'LOC{}'.format(PG._location)]\n",
    "                            INJ_i = INJ_samples * P_aff_i * QNT\n",
    "                            DV_INJ_dict[i].loc[:,\n",
    "                            (FG._ID, PG_ID, d_tag)] = INJ_i\n",
    "\n",
    "        # remove the useless columns from DV_INJ\n",
    "        for i in range(self._inj_lvls):\n",
    "            DV_INJ = DV_INJ_dict[i]\n",
    "            DV_INJ_dict[i] = DV_INJ.loc[:, (DV_INJ != 0.0).any(axis=0)]\n",
    "\n",
    "        # sort the columns to enable index slicing later\n",
    "        for i in range(self._inj_lvls):\n",
    "            DV_INJ_dict[i] = DV_INJ_dict[i].sort_index(axis=1, ascending=True)\n",
    "\n",
    "        return DV_INJ_dict"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
