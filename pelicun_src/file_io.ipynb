{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Copyright (c) 2018 Leland Stanford Junior University\n",
    "Copyright (c) 2018 The Regents of the University of California\n",
    "\n",
    "This file is part of pelicun.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice,\n",
    "this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "this list of conditions and the following disclaimer in the documentation\n",
    "and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors\n",
    "may be used to endorse or promote products derived from this software without\n",
    "specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n",
    "LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
    "CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
    "SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
    "INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
    "CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
    "ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
    "POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "You should have received a copy of the BSD 3-Clause License along with\n",
    "pelicun. If not, see <http://www.opensource.org/licenses/>.\n",
    "\n",
    "Contributors:\n",
    "Adam Zsarn√≥czay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module has classes and methods that handle file input and output.\n",
    "\n",
    ".. rubric:: Contents\n",
    "\n",
    ".. autosummary::\n",
    "\n",
    "    read_SimCenter_DL_input\n",
    "    read_SimCenter_EDP_input\n",
    "    read_population_distribution\n",
    "    read_component_DL_data\n",
    "    convert_P58_data_to_json\n",
    "    create_HAZUS_EQ_json_files\n",
    "    create_HAZUS_HU_json_files\n",
    "    write_SimCenter_DL_output\n",
    "    write_SimCenter_DM_output\n",
    "    write_SimCenter_DV_output\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from distutils.util import strtobool\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "convert_dv_name = {\n",
    "    'DV_rec_cost': 'Reconstruction Cost',\n",
    "    'DV_rec_time': 'Reconstruction Time',\n",
    "    'DV_injuries_0': 'Injuries lvl. 1',\n",
    "    'DV_injuries_1': 'Injuries lvl. 2',\n",
    "    'DV_injuries_2': 'Injuries lvl. 3',\n",
    "    'DV_injuries_3': 'Injuries lvl. 4',\n",
    "    'DV_red_tag': 'Red Tag ',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# this is a convenience function for converting strings to float or None\n",
    "def float_or_None(string):\n",
    "    try:\n",
    "        res = float(string)\n",
    "        return res\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def int_or_None(string):\n",
    "    try:\n",
    "        res = int(string)\n",
    "        return res\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_loc(string, stories):\n",
    "    try:\n",
    "        res = int(string)\n",
    "        return [res, ]\n",
    "    except:\n",
    "        if \"-\" in string:\n",
    "            s_low, s_high = string.split('-')\n",
    "            s_low = process_loc(s_low, stories)\n",
    "            s_high = process_loc(s_high, stories)\n",
    "            return list(range(s_low[0], s_high[0]+1))\n",
    "        elif string == \"all\":\n",
    "            return list(range(1, stories+1))\n",
    "        elif string == \"top\":\n",
    "            return [stories,]\n",
    "        elif string == \"roof\":\n",
    "            return [stories,]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_SimCenter_DL_input(input_path, assessment_type='P58', verbose=False):\n",
    "    \"\"\"\n",
    "    Read the damage and loss input information from a json file.\n",
    "\n",
    "    The SimCenter in the function name refers to having specific fields\n",
    "    available in the file. Such a file is automatically prepared by the\n",
    "    SimCenter PBE Application, but it can also be easily manipulated or created\n",
    "    manually. The accepted input fields are explained in detail in the Input\n",
    "    section of the documentation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path: string\n",
    "        Location of the DL input json file.\n",
    "    assessment_type: {'P58', 'HAZUS_EQ', 'HAZUS_HU'}\n",
    "        Tailors the warnings and verifications towards the type of assessment.\n",
    "        default: 'P58'.\n",
    "    verbose: boolean\n",
    "        If True, the function echoes the information read from the file. This\n",
    "        can be useful to ensure that the information in the file is properly\n",
    "        read by the method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: dict\n",
    "        A dictionary with all the damage and loss data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    AT = assessment_type\n",
    "\n",
    "    with open(input_path, 'r') as f:\n",
    "        jd = json.load(f)\n",
    "\n",
    "    # get the data required for DL\n",
    "    data = dict([(label, dict()) for label in [\n",
    "        'general', 'units', 'components', 'collapse_modes',\n",
    "        'decision_variables', 'dependencies', 'data_sources',\n",
    "    ]])\n",
    "\n",
    "    # create a few internal variables for convenience\n",
    "    DL_input = jd['DamageAndLoss']\n",
    "\n",
    "    response = DL_input.get('ResponseModel',None)\n",
    "    if response is not None:\n",
    "        res_description = response.get('ResponseDescription', None)\n",
    "        det_lims = response.get('DetectionLimits', None)\n",
    "        uncertainty = response.get('AdditionalUncertainty', None)\n",
    "\n",
    "    else:\n",
    "        res_description = None\n",
    "        det_lims = None\n",
    "        uncertainty = None\n",
    "\n",
    "    damage = DL_input.get('DamageModel',None)\n",
    "    if damage is not None:\n",
    "        irrep_res_drift = damage.get('IrrepairableResidualDrift', None)\n",
    "        coll_prob = damage.get('CollapseProbability', None)\n",
    "        coll_lims = damage.get('CollapseLimits', None)\n",
    "        design_lvl = damage.get('DesignLevel', None)\n",
    "        struct_type = damage.get('StructureType', None)\n",
    "\n",
    "    else:\n",
    "        irrep_res_drift = None\n",
    "        coll_prob = None\n",
    "        coll_lims = None\n",
    "        design_lvl = None\n",
    "        struct_type = None\n",
    "\n",
    "    loss = DL_input.get('LossModel', None)\n",
    "    if loss is not None:\n",
    "        repl_cost = loss.get('ReplacementCost', None)\n",
    "        repl_time = loss.get('ReplacementTime', None)\n",
    "        dec_vars = loss.get('DecisionVariables', None)\n",
    "        inhabitants = loss.get('Inhabitants', None)\n",
    "\n",
    "    else:\n",
    "        repl_cost = None\n",
    "        repl_time = None\n",
    "        dec_vars = None\n",
    "        inhabitants = None\n",
    "\n",
    "    depends = DL_input.get('Dependencies', None)\n",
    "    components = DL_input.get('Components', None)\n",
    "    coll_modes = DL_input.get('CollapseModes', None)\n",
    "\n",
    "    # decision variables of interest\n",
    "    if dec_vars is not None:\n",
    "        for target_att, source_att in [ ['injuries', 'Injuries'],\n",
    "                                        ['rec_cost', 'ReconstructionCost'],\n",
    "                                        ['rec_time', 'ReconstructionTime'],\n",
    "                                        ['red_tag', 'RedTag'], ]:\n",
    "            val = bool(dec_vars.get(source_att, False))\n",
    "            data['decision_variables'].update({target_att: val})\n",
    "    else:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"No decision variables specified in the input file. Assuming that \"\n",
    "            \"only reconstruction cost and time needs to be calculated.\"))\n",
    "        data['decision_variables'].update({ 'injuries': False,\n",
    "                                            'rec_cost': True,\n",
    "                                            'rec_time': True,})\n",
    "        # red tag is only used by P58 now\n",
    "        if AT == 'P58':\n",
    "            data['decision_variables'].update({'red_tag': False})\n",
    "\n",
    "    dec_vars = data['decision_variables']\n",
    "\n",
    "    # data sources\n",
    "    # check if the user specified custom data sources\n",
    "    path_CMP_data = DL_input.get(\"ComponentDataFolder\", \"\")\n",
    "\n",
    "    if inhabitants is not None:\n",
    "        path_POP_data = inhabitants.get(\"PopulationDataFile\", \"\")\n",
    "    else:\n",
    "        path_POP_data = \"\"\n",
    "\n",
    "\n",
    "    # if not, use the default location\n",
    "    default_data_name = {\n",
    "        'P58'     : 'FEMA P58 first edition',\n",
    "        'HAZUS_EQ': 'HAZUS MH 2.1 earthquake',\n",
    "        'HAZUS_HU': 'HAZUS MH 2.1 hurricane'\n",
    "    }\n",
    "\n",
    "    if path_CMP_data == \"\":\n",
    "        warnings.warn(UserWarning(\n",
    "            \"The component database is not specified; using the default \"\n",
    "            \"{} data.\".format(default_data_name[AT])\n",
    "        ))\n",
    "        path_CMP_data = pelicun_path\n",
    "        if AT == 'P58':\n",
    "            path_CMP_data += '/resources/FEMA P58 first edition/DL json/'\n",
    "        elif AT == 'HAZUS_EQ':\n",
    "            path_CMP_data += '/resources/HAZUS MH 2.1 earthquake/DL json/'\n",
    "        elif AT == 'HAZUS_HU':\n",
    "            path_CMP_data += '/resources/HAZUS MH 2.1 hurricane/DL json/'\n",
    "    data['data_sources'].update({'path_CMP_data': path_CMP_data})\n",
    "\n",
    "    # The population data is only needed if we are interested in injuries\n",
    "    if data['decision_variables']['injuries']:\n",
    "        if path_POP_data == \"\":\n",
    "            warnings.warn(UserWarning(\n",
    "                \"The population distribution is not specified; using the default \"\n",
    "                \"{} data.\".format(default_data_name[AT])\n",
    "            ))\n",
    "            path_POP_data = pelicun_path\n",
    "            if AT == 'P58':\n",
    "                path_POP_data += '/resources/FEMA P58 first edition/population.json'\n",
    "            elif AT == 'HAZUS_EQ':\n",
    "                path_POP_data += '/resources/HAZUS MH 2.1 earthquake/population.json'\n",
    "        data['data_sources'].update({'path_POP_data': path_POP_data})\n",
    "\n",
    "    # general information\n",
    "    GI = jd.get(\"GeneralInformation\", None)\n",
    "\n",
    "    # units\n",
    "    if (GI is not None) and ('units' in GI.keys()):\n",
    "        for key, value in GI['units'].items():\n",
    "            if value == 'in':\n",
    "                value = 'inch'\n",
    "            if value in globals().keys():\n",
    "                data['units'].update({key: globals()[value]})\n",
    "            else:\n",
    "                warnings.warn(UserWarning(\n",
    "                    \"Unknown {} unit: {}\".format(key, value)\n",
    "                ))\n",
    "        if 'length' in data['units'].keys():\n",
    "            data['units'].update({\n",
    "                'area': data['units']['length']**2.,\n",
    "                'volume': data['units']['length']**3.\n",
    "            })\n",
    "            if 'speed' not in data['units'].keys():\n",
    "                data['units'].update({\n",
    "                    'speed': data['units']['length']})\n",
    "            if 'acceleration' not in data['units'].keys():\n",
    "                data['units'].update({\n",
    "                    #'acceleration': 1.0 })\n",
    "                    'acceleration': data['units']['length']})\n",
    "    else:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"No units were specified in the input file. Standard units are \"\n",
    "            \"assumed.\"))\n",
    "        data['units'].update({\n",
    "            'force': globals()['N'],\n",
    "            'length': globals()['m'],\n",
    "            'area': globals()['m2'],\n",
    "            'volume': globals()['m3'],\n",
    "            'speed': globals()['mps'],\n",
    "            'acceleration': globals()['mps2'],\n",
    "        })\n",
    "\n",
    "    # other attributes that can be used by a P58 assessment\n",
    "    if AT == 'P58':\n",
    "        for target_att, source_att, f_conv, unit_kind, dv_req in [\n",
    "            ['plan_area', 'planArea', float, 'area', 'injuries'],\n",
    "            ['stories', 'stories', int, '', 'all'],\n",
    "            # The following lines are commented out for now, because we do not\n",
    "            # use these pieces of data anyway.\n",
    "            #['building_type', 'type', str, ''],\n",
    "            #['height', 'height', float, 'length'],\n",
    "            #['year_built', 'year', int, ''],\n",
    "        ]:\n",
    "            if (GI is not None) and (source_att in GI.keys()):\n",
    "                if unit_kind != '':\n",
    "                    f_unit = data['units'][unit_kind]\n",
    "                else:\n",
    "                    f_unit = 1\n",
    "                att_value = f_conv(GI[source_att]) * f_unit\n",
    "                data['general'].update({target_att: att_value})\n",
    "            else:\n",
    "                if (dv_req != '') and ((dv_req == 'all') or dec_vars[dv_req]):\n",
    "                    raise ValueError(\n",
    "                        \"{} has to be specified in the DL input file to \"\n",
    "                        \"estimate {} decision variable(s).\".format(source_att,\n",
    "                                                                   dv_req))\n",
    "\n",
    "    # is this a coupled assessment?\n",
    "    if res_description is not None:\n",
    "        data['general'].update({'coupled_assessment':\n",
    "                            res_description.get('CoupledAssessment', False)})\n",
    "    else:\n",
    "        data['general'].update({'coupled_assessment': False})\n",
    "\n",
    "    # components\n",
    "    # Having components defined is not necessary, but if a component is defined\n",
    "    # then all of its attributes need to be specified. Note that the required\n",
    "    # set of attributes depends on the type of assessment.\n",
    "    if components is not None:\n",
    "        for fg_id, frag_group in components.items():\n",
    "            if AT == 'P58':\n",
    "                # TODO: allow location and direction inputs with '-' in them\n",
    "                comp_data = {\n",
    "                    'locations'   : [],\n",
    "                    'directions'  : [],\n",
    "                    'quantities'  : [],\n",
    "                    'unit'        : [],\n",
    "                    'distribution': [],\n",
    "                    'cov'         : [],\n",
    "                    'csg_weights':  [],\n",
    "                }\n",
    "\n",
    "                for comp in frag_group:\n",
    "                    locs = []\n",
    "                    for loc_ in comp['location'].split(','):\n",
    "                        for l in process_loc(loc_, data['general']['stories']):\n",
    "                            locs.append(l)\n",
    "                    locs.sort()\n",
    "\n",
    "                    dirs = sorted([int_or_None(dir_)\n",
    "                                   for dir_ in comp['direction'].split(',')])\n",
    "                    qnts = [float(qnt)\n",
    "                            for qnt in comp['median_quantity'].split(',')]\n",
    "                    csg_weights = (qnts / np.sum(qnts)).tolist()\n",
    "                    qnts = np.sum(qnts)\n",
    "\n",
    "                    pg_count = len(locs) * len(dirs)\n",
    "\n",
    "                    comp_data['locations'] = (comp_data['locations'] +\n",
    "                                               [l for l in locs for d in dirs])\n",
    "                    comp_data['directions'] = (comp_data['directions'] +\n",
    "                                              dirs * len(locs))\n",
    "\n",
    "                    unit = comp['unit']\n",
    "                    if unit not in globals().keys():\n",
    "                        raise ValueError(\n",
    "                            \"Unknown unit for component {}: {}\".format(fg_id,\n",
    "                                                                       unit))\n",
    "                    for i in range(pg_count):\n",
    "                        comp_data['quantities'].append(qnts)\n",
    "                        comp_data['csg_weights'].append(csg_weights)\n",
    "                        comp_data['unit'].append(unit)\n",
    "                        comp_data['distribution'].append(comp['distribution'])\n",
    "                        comp_data['cov'].append(comp.get('cov', None))\n",
    "\n",
    "                sorted_ids = np.argsort(comp_data['locations'])\n",
    "                for key in ['locations', 'directions', 'quantities',\n",
    "                            'csg_weights', 'distribution', 'cov']:\n",
    "                    comp_data[key] = [comp_data[key][s_id] for s_id in sorted_ids]\n",
    "\n",
    "                if len(set(comp_data['unit'])) != 1:\n",
    "                    raise ValueError(\n",
    "                        \"Multiple types of units specified for fragility group \"\n",
    "                        \"{}. Make sure that every component group in a \"\n",
    "                        \"fragility group is defined using the same \"\n",
    "                        \"unit.\".format(fg_id))\n",
    "                comp_data['unit'] = comp_data['unit'][0]\n",
    "\n",
    "            elif AT.startswith('HAZUS'):\n",
    "                comp_data = {\n",
    "                    'locations'    : [1, ],\n",
    "                    'directions'   : [1, ],\n",
    "                    'quantities'  : [1, ],\n",
    "                    'unit'        : 'ea',\n",
    "                    'distribution': ['N/A',],\n",
    "                    'cov'         : [None,],\n",
    "                    'csg_weights' : [[1.0,],]\n",
    "                }\n",
    "\n",
    "                # some basic pre-processing\n",
    "                # sort the dirs and their weights to have better structured\n",
    "                # matrices later\n",
    "                #dir_order = np.argsort(comp_data['directions'])\n",
    "                #comp_data['directions'] = [comp_data['directions'][d_i] for d_i\n",
    "                #                     in dir_order]\n",
    "\n",
    "                # get the location(s) of components based on non-zero quantities\n",
    "                #comp_data.update({\n",
    "                #    'locations': (np.where(comp_data['quantities'] > 0.)[\n",
    "                #                      0] + 1).tolist()\n",
    "                #})\n",
    "                # remove the zeros from the quantities\n",
    "                #nonzero = comp_data['quantities'] > 0.\n",
    "                #comp_data['quantities'] = comp_data['quantities'][\n",
    "                #    nonzero].tolist()\n",
    "\n",
    "                # scale the quantities according to the specified unit\n",
    "\n",
    "                # store the component data\n",
    "            data['components'].update({fg_id: comp_data})\n",
    "    else:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"No components were defined in the input file.\"))\n",
    "\n",
    "\n",
    "    # collapse modes\n",
    "    if AT == 'P58':\n",
    "        # Having collapse modes defined is not necessary, but if a collapse mode\n",
    "        # is defined, then all of its attributes need to be specified.\n",
    "        if coll_modes is not None:\n",
    "            for coll_mode in coll_modes:\n",
    "                cm_data = {\n",
    "                    'w'            : float(coll_mode['weight']),\n",
    "                    'injuries'     : [float(inj) for inj in\n",
    "                                      coll_mode['injuries'].split(',')],\n",
    "                    'affected_area': [float(cfar) for cfar in\n",
    "                                      coll_mode['affected_area'].split(',')],\n",
    "                }\n",
    "                if len(cm_data['affected_area']) == 1:\n",
    "                    cm_data['affected_area'] = (np.ones(data['general']['stories'])*cm_data['affected_area']).tolist()\n",
    "                if len(cm_data['injuries']) == 1:\n",
    "                    cm_data['injuries'] = (np.ones(data['general']['stories'])*cm_data['injuries']).tolist()\n",
    "                data['collapse_modes'].update({coll_mode['name']: cm_data})\n",
    "        else:\n",
    "            warnings.warn(UserWarning(\n",
    "                \"No collapse modes were defined in the input file.\"))\n",
    "\n",
    "    # the number of realizations has to be specified in the file\n",
    "    if res_description is not None:\n",
    "        realizations = res_description.get(\"Realizations\", None)\n",
    "        if realizations is not None:\n",
    "            data['general'].update({'realizations': int(realizations)})\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Number of realizations is not specified in the input file.\")\n",
    "\n",
    "    EDP_units = dict(\n",
    "        # PID is not here because it is unitless\n",
    "        PFA = 'acceleration',\n",
    "        PWS = 'speed'\n",
    "    )\n",
    "    if AT in ['P58', 'HAZUS_EQ']:\n",
    "        EDP_keys = ['PID', 'PFA']\n",
    "    elif AT in ['HAZUS_HU']:\n",
    "        EDP_keys = ['PWS', ]\n",
    "\n",
    "    # response model info ------------------------------------------------------\n",
    "    if response is None:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"Response model characteristics were not defined in the input \"\n",
    "            \"file\"))\n",
    "\n",
    "    # detection limits\n",
    "    if ((response is not None) and (det_lims is not None)):\n",
    "        data['general'].update({\n",
    "            'detection_limits':\n",
    "                dict([(key, float_or_None(value)) for key, value in\n",
    "                      det_lims.items()])})\n",
    "        DGDL = data['general']['detection_limits']\n",
    "        # scale the limits by the units\n",
    "        for EDP_kind, value in DGDL.items():\n",
    "            if (EDP_kind in EDP_units.keys()) and (value is not None):\n",
    "                f_EDP = data['units'][EDP_units[EDP_kind]]\n",
    "                DGDL[EDP_kind] = DGDL[EDP_kind] * f_EDP\n",
    "    else:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"EDP detection limits were not defined in the input file. \"\n",
    "            \"Assuming no detection limits.\"))\n",
    "\n",
    "        data['general'].update({'detection_limits':{}})\n",
    "    # make sure that PID and PFA detection limits are initialized\n",
    "    for key in EDP_keys:\n",
    "        if key not in data['general']['detection_limits'].keys():\n",
    "            data['general']['detection_limits'].update({key: None})\n",
    "\n",
    "    # response description\n",
    "    if ((response is not None) and (res_description is not None)):\n",
    "        #TODO: move the collapse-related data to another field\n",
    "        data['general'].update({'response': {\n",
    "            'EDP_distribution': res_description.get('EDP_Distribution',\n",
    "                                                    'lognormal'),\n",
    "            'EDP_dist_basis':   res_description.get('BasisOfEDP_Distribution',\n",
    "                                                    'all results')}})\n",
    "    else:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"EDP estimation method was not defined in the input file. All EDP \"\n",
    "            \"samples are used to define a multivariate lognormal EDP \"\n",
    "            \"distribution.\"))\n",
    "\n",
    "        data['general'].update({'response': {\n",
    "            'EDP_distribution': 'lognormal',\n",
    "            'EDP_dist_basis'  : 'all results'}})\n",
    "\n",
    "    # additional uncertainty\n",
    "    if ((response is not None) and (uncertainty is not None)):\n",
    "        data['general'].update({\n",
    "            'added_uncertainty': {\n",
    "                'beta_gm': float_or_None(uncertainty['GroundMotion']),\n",
    "                'beta_m' : float_or_None(uncertainty['Modeling'])}})\n",
    "    else:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"No additional uncertainties were defined in the input file. \"\n",
    "            \"Assuming that EDPs already include all ground motion and model \"\n",
    "            \"uncertainty.\"))\n",
    "        data['general'].update({\n",
    "            'added_uncertainty': {\n",
    "                'beta_gm': 0.0001,\n",
    "                'beta_m': 0.0001\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # damage model info --------------------------------------------------------\n",
    "    if damage is None:\n",
    "        if AT == 'P58':\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Damage model characteristics were not defined in the \"\n",
    "                \"input file\"))\n",
    "        elif AT.startswith('HAZUS'):\n",
    "            pass\n",
    "\n",
    "    # P58-specific things\n",
    "    if AT == 'P58':\n",
    "        # EDP limits for collapse\n",
    "        if ((damage is not None) and (coll_lims is not None)):\n",
    "            # load the limits\n",
    "            data['general'].update({\n",
    "                'collapse_limits':\n",
    "                    dict([(key, float_or_None(value)) for key, value\n",
    "                          in coll_lims.items()])})\n",
    "\n",
    "            # scale the limits according to their units\n",
    "            DGCL = data['general']['collapse_limits']\n",
    "            for EDP_kind, value in DGCL.items():\n",
    "                if (EDP_kind in EDP_units.keys()) and (value is not None):\n",
    "                    f_EDP = data['units'][EDP_units[EDP_kind]]\n",
    "                    DGCL[EDP_kind] = DGCL[EDP_kind] * f_EDP\n",
    "        else:\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Collapse EDP limits were not defined in the input file. \"\n",
    "                \"No EDP limits are assumed.\"))\n",
    "\n",
    "            data['general'].update({'collapse_limits': {}})\n",
    "\n",
    "        # make sure that PID and PFA collapse limits are initialized\n",
    "        for key in EDP_keys:\n",
    "            if key not in data['general']['collapse_limits'].keys():\n",
    "                data['general']['collapse_limits'].update({key: None})\n",
    "\n",
    "        # irrepairable drift\n",
    "        if ((damage is not None) and (irrep_res_drift is not None)):\n",
    "            data['general'].update({\n",
    "                'irrepairable_res_drift':\n",
    "                    dict([(key, float_or_None(value)) for key, value in\n",
    "                          irrep_res_drift.items()])})\n",
    "            # TODO: move this in the irrepairable part of general\n",
    "            yield_drift = irrep_res_drift.get(\"YieldDriftRatio\", None)\n",
    "            if yield_drift is not None:\n",
    "                data['general'].update({\n",
    "                    'yield_drift': float_or_None(yield_drift)})\n",
    "            elif ((data['decision_variables']['rec_cost']) or\n",
    "                  (data['decision_variables']['rec_time'])):\n",
    "                warnings.warn(UserWarning(\n",
    "                    \"Yield drift ratio was not defined in the input file. \"\n",
    "                    \"Assuming a yield drift ratio of 0.01 radian.\"))\n",
    "                data['general'].update({'yield_drift': 0.01})\n",
    "\n",
    "        elif ((data['decision_variables']['rec_cost']) or\n",
    "              (data['decision_variables']['rec_time'])):\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Residual drift limits corresponding to irrepairable \"\n",
    "                \"damage were not defined in the input file. We assume that \"\n",
    "                \"damage is repairable regardless of the residual drift.\"))\n",
    "            # we might need to have a default yield drift here\n",
    "\n",
    "        # collapse probability\n",
    "        if 'response' not in data['general'].keys():\n",
    "            data['general'].update({'response': {}})\n",
    "        if ((damage is not None) and (coll_prob is not None)):\n",
    "            data['general']['response'].update({\n",
    "                'coll_prob'   : coll_prob.get('Value',\n",
    "                                                    'estimated'),\n",
    "                'CP_est_basis': coll_prob.get('BasisOfEstimate',\n",
    "                                                    'raw EDP')})\n",
    "            if data['general']['response']['coll_prob'] != 'estimated':\n",
    "                data['general']['response']['coll_prob'] = \\\n",
    "                    float_or_None(data['general']['response']['coll_prob'])\n",
    "        else:\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Collapse probability estimation method was not defined in the \"\n",
    "                \"input file. Collapse probability is estimated using raw EDP \"\n",
    "                \"samples.\"))\n",
    "            data['general']['response'].update({\n",
    "                'coll_prob'       : 'estimated',\n",
    "                'CP_est_basis'    : 'raw EDP'})\n",
    "\n",
    "    # loss model info ----------------------------------------------------------\n",
    "    if loss is None:\n",
    "        warnings.warn(UserWarning(\n",
    "            \"Loss model characteristics were not defined in the input file\"))\n",
    "\n",
    "    # replacement cost\n",
    "    if ((loss is not None) and (repl_cost is not None)):\n",
    "        data['general'].update({\n",
    "            'replacement_cost': float_or_None(repl_cost)})\n",
    "    elif data['decision_variables']['rec_cost']:\n",
    "        if AT == 'P58':\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Building replacement cost was not defined in the \"\n",
    "                \"input file.\"))\n",
    "        elif AT.startswith('HAZUS'):\n",
    "            raise ValueError(\n",
    "                \"Building replacement cost was not defined in the input \"\n",
    "                \"file.\")\n",
    "\n",
    "    # replacement time\n",
    "    if ((loss is not None) and (repl_time is not None)):\n",
    "        data['general'].update({\n",
    "            'replacement_time': float_or_None(repl_time)})\n",
    "    elif data['decision_variables']['rec_time']:\n",
    "        if AT == 'P58':\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Building replacement cost was not defined in the \"\n",
    "                \"input file.\"))\n",
    "        elif AT.startswith('HAZUS'):\n",
    "            raise ValueError(\n",
    "                \"Building replacement cost was not defined in the input \"\n",
    "                \"file.\")\n",
    "\n",
    "    # inhabitants\n",
    "    if data['decision_variables']['injuries']:\n",
    "        if ((loss is not None) and (inhabitants is not None)):\n",
    "\n",
    "            # occupancy type\n",
    "            occupancy = inhabitants.get(\"OccupancyType\", None)\n",
    "            if occupancy is not None:\n",
    "                data['general'].update({'occupancy_type': occupancy})\n",
    "            else:\n",
    "                raise ValueError(\"Occupancy type was not defined in the input \"\n",
    "                                 \"file.\")\n",
    "\n",
    "            # peak population\n",
    "            peak_pop = inhabitants.get(\"PeakPopulation\", None)\n",
    "            if peak_pop is not None:\n",
    "                peak_pop = [float_or_None(pop) for pop in peak_pop.split(',')]\n",
    "\n",
    "                # If the number of stories is specified...\n",
    "                if 'stories' in data['general'].keys():\n",
    "                    stories = data['general']['stories']\n",
    "                    pop_in = len(peak_pop)\n",
    "\n",
    "                    # and the population list does not provide values\n",
    "                    # for every story:\n",
    "                    for s in range(pop_in, stories):\n",
    "                        # If only one value is provided, then it is assumed to\n",
    "                        # be the population on every story.\n",
    "                        if pop_in == 1:\n",
    "                            peak_pop.append(peak_pop[0])\n",
    "\n",
    "                        # Otherwise, the values are assumed to correspond to\n",
    "                        # the bottom stories and the upper ones are filled with\n",
    "                        # zeros. A warning message is displayed in this case.\n",
    "                        else:\n",
    "                            peak_pop.append(0)\n",
    "\n",
    "                    if pop_in > 1 and pop_in != stories:\n",
    "                        warnings.warn(UserWarning(\n",
    "                            \"Peak population was specified to some, but not all \"\n",
    "                            \"stories. The remaining stories are assumed to have \"\n",
    "                            \"zero population.\"\n",
    "                        ))\n",
    "\n",
    "                data['general'].update({'population': peak_pop})\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Peak population was not defined in the input file.\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Information about inhabitants was not defined in the input \"\n",
    "                \"file.\")\n",
    "\n",
    "    # dependencies -------------------------------------------------------------\n",
    "\n",
    "    # set defaults\n",
    "    # We assume 'Independent' for all unspecified fields except for the\n",
    "    # fragilities where 'per ATC recommendation' is the default setting.\n",
    "    dependency_to_acronym = {\n",
    "        'btw. Fragility Groups'  : 'FG',\n",
    "        'btw. Performance Groups': 'PG',\n",
    "        'btw. Floors'            : 'LOC',\n",
    "        'btw. Directions'        : 'DIR',\n",
    "        'btw. Component Groups'  : 'CSG',\n",
    "        'btw. Damage States'     : 'DS',\n",
    "        'Independent'            : 'IND',\n",
    "        'per ATC recommendation' : 'ATC',\n",
    "    }\n",
    "\n",
    "    if AT == 'P58':\n",
    "\n",
    "        for target_att, source_att, dv_req in [\n",
    "            ['quantities', 'Quantities', ''],\n",
    "            ['fragilities', 'Fragilities', ''],\n",
    "            ['injuries', 'Injuries', 'injuries'],\n",
    "            ['rec_costs', 'ReconstructionCosts', 'rec_cost'],\n",
    "            ['rec_times', 'ReconstructionTimes', 'rec_time'],\n",
    "            ['red_tags', 'RedTagProbabilities', 'red_tag'],]:\n",
    "\n",
    "            if ((depends is not None) and (source_att in depends.keys())):\n",
    "                data['dependencies'].update({\n",
    "                    target_att:dependency_to_acronym[depends[source_att]]})\n",
    "            elif dv_req == '' or data['decision_variables'][dv_req]:\n",
    "                if target_att != 'fragilities':\n",
    "                    data['dependencies'].update({target_att: 'IND'})\n",
    "                else:\n",
    "                    data['dependencies'].update({target_att: 'ATC'})\n",
    "\n",
    "                warnings.warn(UserWarning(\n",
    "                    \"Correlation between {} was not \".format(source_att)+\n",
    "                    \"defined in the input file. Using default values.\"))\n",
    "\n",
    "        if ((depends is not None) and ('CostAndTime' in depends.keys())):\n",
    "            data['dependencies'].update({\n",
    "                'cost_and_time': bool(depends['CostAndTime'])})\n",
    "        elif ((data['decision_variables']['rec_cost']) or\n",
    "              (data['decision_variables']['rec_time'])):\n",
    "            data['dependencies'].update({'cost_and_time': False})\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Correlation between reconstruction cost and time was not \"\n",
    "                \"defined in the input file. Using default values.\"))\n",
    "\n",
    "        if ((depends is not None) and ('InjurySeverities' in depends.keys())):\n",
    "            data['dependencies'].update({\n",
    "                'injury_lvls': bool(depends['InjurySeverities'])})\n",
    "        elif data['decision_variables']['injuries']:\n",
    "            data['dependencies'].update({'injury_lvls': False})\n",
    "            warnings.warn(UserWarning(\n",
    "                \"Correlation between injury levels was not defined in the \"\n",
    "                \"input file. Using default values.\"))\n",
    "\n",
    "    if verbose: pp.pprint(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_SimCenter_EDP_input(input_path, EDP_kinds=('PID', 'PFA'),\n",
    "                             units = dict(PID=1., PFA=1.),\n",
    "                             verbose=False):\n",
    "    \"\"\"\n",
    "    Read the EDP input information from a text file with a tabular structure.\n",
    "\n",
    "    The SimCenter in the function name refers to having specific columns\n",
    "    available in the file. Currently, the expected formatting follows the\n",
    "    output formatting of Dakota that is applied for the dakotaTab.out. When\n",
    "    using pelicun with the PBE Application, such a dakotaTab.out is\n",
    "    automatically generated. The Input section of the documentation provides\n",
    "    more information about the expected formatting of the EDP input file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path: string\n",
    "        Location of the EDP input file.\n",
    "    EDP_kinds: tuple of strings, default: ('PID', 'PFA')\n",
    "        Collection of the kinds of EDPs in the input file. The default pair of\n",
    "        'PID' and 'PFA' can be replaced or extended by any other EDPs.\n",
    "    units: dict, default: {'PID':1., 'PFA':1}\n",
    "        Defines the unit conversion that shall be applied to the EDP values.\n",
    "    verbose: boolean\n",
    "        If True, the function echoes the information read from the file. This\n",
    "        can be useful to ensure that the information in the file is properly\n",
    "        read by the method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: dict\n",
    "        A dictionary with all the EDP data.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the data container\n",
    "    data = {}\n",
    "\n",
    "    # read the collection of EDP inputs...\n",
    "    # If the file name ends with csv, we assume a standard csv file\n",
    "    if input_path[-3:] == 'csv':\n",
    "        EDP_raw = pd.read_csv(input_path, header=0, index_col=0)\n",
    "\n",
    "    # otherwise, we assume that a dakota file is provided...\n",
    "    else:\n",
    "        # the read_csv method in pandas is sufficiently versatile to handle the\n",
    "        # tabular format of dakota\n",
    "        EDP_raw = pd.read_csv(input_path, sep=r'\\s+', header=0, index_col=0)\n",
    "    # set the index to be zero-based\n",
    "    EDP_raw.index = EDP_raw.index - 1\n",
    "\n",
    "    # search the header for EDP information\n",
    "    for column in EDP_raw.columns:\n",
    "        for kind in EDP_kinds:\n",
    "            if kind in column:\n",
    "\n",
    "                if kind not in data.keys():\n",
    "                    data.update({kind: []})\n",
    "\n",
    "                # extract info about the location, direction, and scenario\n",
    "                info = column.split('-')\n",
    "\n",
    "                # get the scale factor to perform unit conversion\n",
    "                f_unit = units[kind]\n",
    "\n",
    "                # store the data\n",
    "                data[kind].append(dict(\n",
    "                    raw_data=(EDP_raw[column].values * f_unit).tolist(),\n",
    "                    location=info[2],\n",
    "                    direction=info[3],\n",
    "                    scenario_id=info[0]\n",
    "                ))\n",
    "\n",
    "    if verbose: pp.pprint(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def write_SimCenter_DL_output(output_path, output_df, index_name='#Num',\n",
    "                              collapse_columns = True, stats_only=False):\n",
    "\n",
    "    output_df = deepcopy(output_df)\n",
    "\n",
    "    # if the summary flag is set, then not all realizations are returned, but\n",
    "    # only the first two moments and the empirical CDF through 100 percentiles\n",
    "    if stats_only:\n",
    "        #output_df = output_df.describe(np.arange(1, 100)/100.)\n",
    "        output_df = output_df.describe([0.1,0.5,0.9])\n",
    "\n",
    "    # the name of the index column is replaced with the provided value\n",
    "    output_df.index.name = index_name\n",
    "\n",
    "\n",
    "    # multiple levels of indices are collapsed into a single level if needed\n",
    "    # TODO: check for the number of levels and prepare a smarter collapse method\n",
    "    if collapse_columns:\n",
    "        output_df.columns = [('{}/{}'.format(s0, s1)).replace(' ', '_')\n",
    "                     for s0, s1 in zip(output_df.columns.get_level_values(0),\n",
    "                                       output_df.columns.get_level_values(1))]\n",
    "\n",
    "    # write the results in a csv file\n",
    "    # TODO: provide other file formats\n",
    "    output_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def write_SimCenter_DM_output(DM_file_path, DMG_df):\n",
    "\n",
    "    # Start with the probability of being in a particular damage state.\n",
    "    # Here, the damage state of the building (asset) is defined as the highest\n",
    "    # damage state among the building components/component groups. This works\n",
    "    # well for a HAZUS assessment, but something more sophisticated is needed\n",
    "    # for a FEMA P58 assessment.\n",
    "\n",
    "    # Determine the probability of DS exceedance by collecting the DS from all\n",
    "    # components and assigning ones to all lower damage states.\n",
    "    DMG_agg = DMG_df.T.groupby('DS').sum().T\n",
    "    DMG_agg[DMG_agg > 0.0] = DMG_agg[DMG_agg > 0.0] / DMG_agg[DMG_agg > 0.0]\n",
    "\n",
    "    cols = DMG_agg.columns\n",
    "    for i in range(len(cols)):\n",
    "        filter = np.where(DMG_agg.iloc[:,i].values > 0.0)[0]\n",
    "        DMG_agg.iloc[filter,idx[0:i]] = 1.0\n",
    "\n",
    "    # The P(DS=ds) probability is determined by subtracting consecutive DS\n",
    "    # exceedance probabilites. This will not work well for a FEMA P58 assessment\n",
    "    # with Damage State Groups that include multiple Damage States.\n",
    "    DMG_agg_mean = DMG_agg.describe().loc['mean',:]\n",
    "    DS_0 = 1.0 - DMG_agg_mean['1-1']\n",
    "    for i in range(len(DMG_agg_mean.index)-1):\n",
    "        DMG_agg_mean.iloc[i] = DMG_agg_mean.iloc[i] - DMG_agg_mean.iloc[i+1]\n",
    "\n",
    "    # Add the probability of no damage for convenience.\n",
    "    DMG_agg_mean['0'] = DS_0\n",
    "    DMG_agg_mean = DMG_agg_mean.sort_index()\n",
    "\n",
    "    # Save the results in the output json file\n",
    "    DM = {'aggregate': {}}\n",
    "\n",
    "    for id in DMG_agg_mean.index:\n",
    "        DM['aggregate'].update({str(id): DMG_agg_mean[id]})\n",
    "\n",
    "    # Now determine the probability of being in a damage state for individual\n",
    "    # components / component assemblies...\n",
    "    DMG_mean = DMG_df.describe().loc['mean',:]\n",
    "\n",
    "    # and save the results in the output json file.\n",
    "    for FG in sorted(DMG_mean.index.get_level_values('FG').unique()):\n",
    "        DM.update({str(FG):{}})\n",
    "\n",
    "        for PG in sorted(\n",
    "            DMG_mean.loc[idx[FG],:].index.get_level_values('PG').unique()):\n",
    "            DM[str(FG)].update({str(PG):{}})\n",
    "\n",
    "            for DS in sorted(\n",
    "                DMG_mean.loc[idx[FG],:].loc[idx[:,PG],:].index.get_level_values('DS').unique()):\n",
    "                DM[str(FG)][str(PG)].update({str(DS): DMG_mean.loc[(FG,PG,DS)]})\n",
    "\n",
    "    with open(DM_file_path, 'w') as f:\n",
    "        json.dump(DM, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_SimCenter_DV_output(DV_file_path, DV_df, DV_name):\n",
    "\n",
    "    DV_name = convert_dv_name[DV_name]\n",
    "\n",
    "    try:\n",
    "        with open(DV_file_path, 'r') as f:\n",
    "            DV = json.load(f)\n",
    "    except:\n",
    "        DV = {}\n",
    "\n",
    "    DV.update({DV_name: {}})\n",
    "\n",
    "    DV_i = DV[DV_name]\n",
    "\n",
    "    try:\n",
    "        DV_tot = DV_df.sum(axis=1).describe([0.1,0.5,0.9]).drop('count')\n",
    "        DV_i.update({'total':{}})\n",
    "        for stat in DV_tot.index:\n",
    "            DV_i['total'].update({stat: DV_tot.loc[stat]})\n",
    "\n",
    "        DV_stats = DV_df.describe([0.1,0.5,0.9]).drop('count')\n",
    "        for FG in sorted(DV_stats.columns.get_level_values('FG').unique()):\n",
    "            DV_i.update({str(FG):{}})\n",
    "\n",
    "            for PG in sorted(\n",
    "                DV_stats.loc[:,idx[FG]].columns.get_level_values('PG').unique()):\n",
    "                DV_i[str(FG)].update({str(PG):{}})\n",
    "\n",
    "                for DS in sorted(\n",
    "                    DV_stats.loc[:,idx[FG, PG]].columns.get_level_values('DS').unique()):\n",
    "                    DV_i[str(FG)][str(PG)].update({str(DS): {}})\n",
    "                    DV_stats_i = DV_stats.loc[:,(FG,PG,DS)]\n",
    "                    for stat in DV_stats_i.index:\n",
    "                        DV_i[str(FG)][str(PG)][str(DS)].update({\n",
    "                            stat: DV_stats_i.loc[stat]})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with open(DV_file_path, 'w') as f:\n",
    "        json.dump(DV, f, indent = 2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
