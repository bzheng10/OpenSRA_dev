{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Copyright (c) 2018 Leland Stanford Junior University\n",
    "Copyright (c) 2018 The Regents of the University of California\n",
    "\n",
    "This file is part of pelicun.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice,\n",
    "this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "this list of conditions and the following disclaimer in the documentation\n",
    "and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors\n",
    "may be used to endorse or promote products derived from this software without\n",
    "specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n",
    "LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
    "CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
    "SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
    "INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
    "CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
    "ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
    "POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "You should have received a copy of the BSD 3-Clause License along with\n",
    "pelicun. If not, see <http://www.opensource.org/licenses/>.\n",
    "\n",
    "Contributors:\n",
    "Adam Zsarn√≥czay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module defines constants, classes and methods for uncertainty\n",
    "quantification in pelicun.\n",
    "\n",
    ".. rubric:: Contents\n",
    "\n",
    ".. autosummary::\n",
    "\n",
    "    RandomVariable\n",
    "    RandomVariableSubset\n",
    "\n",
    "    tmvn_rvs\n",
    "    mvn_orthotope_density\n",
    "    tmvn_MLE\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "from scipy.stats import norm, truncnorm, multivariate_normal, multinomial\n",
    "from scipy.stats.mvn import mvndst\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tmvn_rvs(mu, COV, lower=None, upper=None, size=1):\n",
    "    \"\"\"\n",
    "    Sample a truncated MVN distribution.\n",
    "\n",
    "    Truncation of the multivariate normal distribution is currently considered\n",
    "    through rejection sampling. The applicability of this method is limited by\n",
    "    the amount of probability density enclosed by the hyperrectangle defined by\n",
    "    the truncation limits. The lower that density is, the more samples will\n",
    "    need to be rejected which makes the method inefficient when the tails of\n",
    "    the MVN shall be sampled in high-dimensional space. Such cases can be\n",
    "    handled by a Gibbs sampler, which is a planned future feature of this\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: float scalar or ndarray\n",
    "        Mean(s) of the non-truncated distribution.\n",
    "    COV: float ndarray\n",
    "        Covariance matrix of the non-truncated distribution.\n",
    "    lower: float vector, optional, default: None\n",
    "        Lower bound(s) for the truncated distributions. A scalar value can be\n",
    "        used for a univariate case, while a list of bounds is expected in\n",
    "        multivariate cases. If the distribution is non-truncated from below\n",
    "        in a subset of the dimensions, assign an infinite value\n",
    "        (i.e. -numpy.inf) to those dimensions.\n",
    "    upper: float vector, optional, default: None\n",
    "        Upper bound(s) for the truncated distributions. A scalar value can be\n",
    "        used for a univariate case, while a list of bounds is expected in\n",
    "        multivariate cases. If the distribution is non-truncated from above\n",
    "        in a subset of the dimensions, assign an infinite value\n",
    "        (i.e. numpy.inf) to those dimensions.\n",
    "    size: int\n",
    "        Number of samples requested.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    samples: float ndarray\n",
    "        Samples generated from the truncated distribution.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mu = np.asarray(mu)\n",
    "    if mu.shape == ():\n",
    "        mu = np.asarray([mu])\n",
    "        COV = np.asarray([COV])\n",
    "\n",
    "    # if there are no bounds, simply sample an MVN distribution\n",
    "    if lower is None and upper is None:\n",
    "\n",
    "        samples = multivariate_normal.rvs(mean=mu, cov=COV, size=size)\n",
    "\n",
    "    else:\n",
    "        # first, get the rejection rate\n",
    "        alpha, eps_alpha = mvn_orthotope_density(mu, COV, lower, upper)\n",
    "\n",
    "        # initialize the data for sample collection\n",
    "        sample_count = 0\n",
    "        samples = None\n",
    "        ndim = len(mu)\n",
    "\n",
    "        if lower is None:\n",
    "            lower = np.ones(ndim) * -np.inf\n",
    "        if upper is None:\n",
    "            upper = np.ones(ndim) * np.inf\n",
    "\n",
    "        # If the error in the alpha estimate is too large, then we are\n",
    "        # beyond the applicability limits of the function used for\n",
    "        # estimating alpha. Raise an error in such a case\n",
    "        if alpha <= 100. * eps_alpha:  # i.e. max. error is limited at 1%\n",
    "            #print(alpha, eps_alpha)\n",
    "            raise ValueError(\n",
    "                \"The density of the joint probability distribution within the \"\n",
    "                \"truncation limits is too small and cannot be estimated with \"\n",
    "                \"sufficiently high accuracy. This is most probably due to \"\n",
    "                \"incorrect limits set for the distribution.\"\n",
    "            )\n",
    "\n",
    "        # If the rejection rate is sufficiently low, perform rejection sampling\n",
    "        # Note: the minimum rate is set to zero until a Gibbs sampler is\n",
    "        # implemented, but a warning message is displayed for anything below\n",
    "        # 1e-3\n",
    "        if alpha < 1e-3:\n",
    "            warnings.warn(UserWarning(\n",
    "                \"The rejection rate for sampling the prescribed truncated MVN \"\n",
    "                \"distribution is higher than 0.999. This makes sampling with \"\n",
    "                \"our current implementation very resource-intensive and \"\n",
    "                \"inefficient. If you need to sample such parts of MVN \"\n",
    "                \"distributions, please let us know and we will improve \"\n",
    "                \"this function in the future.\"\n",
    "            ))\n",
    "        if alpha > 0.:\n",
    "            while sample_count < size:\n",
    "\n",
    "                # estimate the required number of samples\n",
    "                req_samples = max(int(1.1*(size-sample_count)/alpha), 2)\n",
    "\n",
    "                # generate the raw samples\n",
    "                raw_samples = multivariate_normal.rvs(mu, COV,\n",
    "                                                      size=req_samples)\n",
    "\n",
    "                # remove the samples that are outside the truncation limits\n",
    "                good_ones = np.all([raw_samples>lower, raw_samples<upper],\n",
    "                                   axis=0)\n",
    "                if ndim > 1:\n",
    "                    good_ones = np.all(good_ones, axis=1)\n",
    "\n",
    "                new_samples = raw_samples[good_ones]\n",
    "\n",
    "                # add the new samples to the pool of samples\n",
    "                if sample_count > 0:\n",
    "                    samples = np.concatenate([samples, new_samples], axis=0)\n",
    "                else:\n",
    "                    samples = new_samples\n",
    "\n",
    "                # check the number of available samples and generate more if\n",
    "                # needed\n",
    "                sample_count = samples.shape[0]\n",
    "\n",
    "            samples = samples[:size]\n",
    "\n",
    "        #else:\n",
    "        # TODO: Gibbs sampler\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mvn_orthotope_density(mu, COV, lower=None, upper=None):\n",
    "    \"\"\"\n",
    "    Estimate the probability density within a hyperrectangle for an MVN distr.\n",
    "\n",
    "    Use the method of Alan Genz (1992) to estimate the probability density\n",
    "    of a multivariate normal distribution within an n-orthotope (i.e.,\n",
    "    hyperrectangle) defined by its lower and upper bounds. Limits can be\n",
    "    relaxed in any direction by assigning infinite bounds (i.e. numpy.inf).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: float scalar or ndarray\n",
    "        Mean(s) of the non-truncated distribution.\n",
    "    COV: float ndarray\n",
    "        Covariance matrix of the non-truncated distribution\n",
    "    lower: float vector, optional, default: None\n",
    "        Lower bound(s) for the truncated distributions. A scalar value can be\n",
    "        used for a univariate case, while a list of bounds is expected in\n",
    "        multivariate cases. If the distribution is non-truncated from below\n",
    "        in a subset of the dimensions, use either `None` or assign an infinite\n",
    "        value (i.e. -numpy.inf) to those dimensions.\n",
    "    upper: float vector, optional, default: None\n",
    "        Upper bound(s) for the truncated distributions. A scalar value can be\n",
    "        used for a univariate case, while a list of bounds is expected in\n",
    "        multivariate cases. If the distribution is non-truncated from above\n",
    "        in a subset of the dimensions, use either `None` or assign an infinite\n",
    "        value (i.e. numpy.inf) to those dimensions.\n",
    "    Returns\n",
    "    -------\n",
    "    alpha: float\n",
    "        Estimate of the probability density within the hyperrectangle\n",
    "    eps_alpha: float\n",
    "        Estimate of the error in alpha.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # process the inputs and get the number of dimensions\n",
    "    mu = np.asarray(mu)\n",
    "    if mu.shape == ():\n",
    "        mu = np.asarray([mu])\n",
    "        COV = np.asarray([COV])\n",
    "    else:\n",
    "        COV = np.asarray(COV)\n",
    "    sig = np.sqrt(np.diag(COV))\n",
    "    corr = COV / np.outer(sig,sig)\n",
    "\n",
    "    ndim = len(mu)\n",
    "\n",
    "    if lower is None:\n",
    "        lower = -np.ones(ndim) * np.inf\n",
    "    else:\n",
    "        lower = np.asarray(lower)\n",
    "\n",
    "    if upper is None:\n",
    "        upper = np.ones(ndim) * np.inf\n",
    "    else:\n",
    "        upper = np.asarray(upper)\n",
    "\n",
    "    # standardize the truncation limits\n",
    "    lower = (lower-mu)/sig\n",
    "    upper = (upper-mu)/sig\n",
    "\n",
    "    # prepare the flags for infinite bounds (these are needed for the mvndst\n",
    "    # function)\n",
    "    lowinf = np.isneginf(lower)\n",
    "    uppinf = np.isposinf(upper)\n",
    "    infin = 2.0*np.ones(ndim)\n",
    "\n",
    "    np.putmask(infin, lowinf, 0)\n",
    "    np.putmask(infin, uppinf, 1)\n",
    "    np.putmask(infin, lowinf*uppinf, -1)\n",
    "\n",
    "    # prepare the correlation coefficients\n",
    "    if ndim == 1:\n",
    "        correl = 0\n",
    "    else:\n",
    "        correl = corr[np.tril_indices(ndim, -1)]\n",
    "\n",
    "    # estimate the density\n",
    "    eps_alpha, alpha, __ = mvndst(lower, upper, infin, correl)\n",
    "\n",
    "    return alpha, eps_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmvn_MLE(samples,\n",
    "             tr_lower=None, tr_upper=None,\n",
    "             censored_count=0, det_lower=None, det_upper=None,\n",
    "             alpha_lim=None):\n",
    "    \"\"\"\n",
    "    Fit a truncated multivariate normal distribution to samples using MLE.\n",
    "\n",
    "    The number of dimensions of the distribution function are inferred from the\n",
    "    shape of the sample data. Censoring is automatically considered if the\n",
    "    number of censored samples and the corresponding detection limits are\n",
    "    provided. Infinite or unspecified truncation limits lead to fitting a\n",
    "    non-truncated normal distribution in that dimension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples: ndarray\n",
    "        Raw data that serves as the basis of estimation. The number of samples\n",
    "        equals the number of columns and each row introduces a new feature. In\n",
    "        other words: a list of sample lists is expected where each sample list\n",
    "        is a collection of samples of one variable.\n",
    "    tr_lower: float vector, optional, default: None\n",
    "        Lower bound(s) for the truncated distributions. A scalar value can be\n",
    "        used for a univariate case, while a list of bounds is expected in\n",
    "        multivariate cases. If the distribution is non-truncated from below\n",
    "        in a subset of the dimensions, use either `None` or assign an infinite\n",
    "        value (i.e. -numpy.inf) to those dimensions.\n",
    "    tr_upper: float vector, optional, default: None\n",
    "        Upper bound(s) for the truncated distributions. A scalar value can be\n",
    "        used for a univariate case, while a list of bounds is expected in\n",
    "        multivariate cases. If the distribution is non-truncated from above\n",
    "        in a subset of the dimensions, use either `None` or assign an infinite\n",
    "        value (i.e. numpy.inf) to those dimensions.\n",
    "    censored_count: int, optional, default: None\n",
    "        The number of censored samples that are beyond the detection limits.\n",
    "        All samples outside the detection limits are aggregated into one set.\n",
    "        This works the same way in one and in multiple dimensions. Prescription\n",
    "        of specific censored sample counts for sub-regions of the input space\n",
    "        outside the detection limits is not supported.\n",
    "    det_lower: float ndarray, optional, default: None\n",
    "        Lower detection limit(s) for censored data. In multivariate cases the\n",
    "        limits need to be defined as a vector; a scalar value is sufficient in\n",
    "        a univariate case. If the data is not censored from below in a\n",
    "        particular dimension, assign None to that position of the ndarray.\n",
    "    det_upper: float ndarray, optional, default: None\n",
    "        Upper detection limit(s) for censored data. In multivariate cases the\n",
    "        limits need to be defined as a vector; a scalar value is sufficient in\n",
    "        a univariate case. If the data is not censored from above in a\n",
    "        particular dimension, assign None to that position of the ndarray.\n",
    "    alpha_lim: float, optional, default:None\n",
    "        Introduces a lower limit to the probability density within the\n",
    "        n-orthotope defined by the truncation limits. Assigning a reasonable\n",
    "        minimum (such as 1e-4) can be useful when the mean of the distribution\n",
    "        is several standard deviations from the truncation limits and the\n",
    "        sample size is small. Such cases without a limit often converge to\n",
    "        distant means with inflated variances. Besides being incorrect\n",
    "        estimates, those solutions only offer negligible reduction in the\n",
    "        negative log likelihood, while making subsequent sampling of the\n",
    "        truncated normal distribution very challenging.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mu: float scalar or ndarray\n",
    "        Mean of the fitted probability distribution. A vector of means is\n",
    "        returned in a multivariate case.\n",
    "    COV: float scalar or 2D ndarray\n",
    "        Covariance matrix of the fitted probability distribution. A 2D square\n",
    "        ndarray is returned in a multi-dimensional case, while a single\n",
    "        variance (not standard deviation!) value is returned in a univariate\n",
    "        case.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    verbose = False\n",
    "    if verbose:\n",
    "        print('\\ndetection limits:')\n",
    "        print(det_lower)\n",
    "        print(det_upper)\n",
    "    # extract some basic information about the number of dimensions and the\n",
    "    # number of samples from raw data\n",
    "    samples = np.asarray(samples)\n",
    "    if samples.ndim == 1:\n",
    "        ndims = 1\n",
    "        nsamples = len(samples)\n",
    "        samplesT = samples\n",
    "    else:\n",
    "        ndims, nsamples = samples.shape\n",
    "        samplesT = np.transpose(samples)\n",
    "\n",
    "    mu_hat = np.mean(samplesT, axis=0)\n",
    "    # replace zero standard dev with negligible standard dev\n",
    "    if ndims == 1:\n",
    "        sig_hat = np.maximum(1e-6, np.std(samplesT, axis=0))\n",
    "    else:\n",
    "        sig_hat = np.std(samplesT, axis=0)\n",
    "        sig_zero_id = np.where(sig_hat == 0.0)[0]\n",
    "        sig_hat[sig_zero_id] = 1e-6 * np.abs(mu_hat[sig_zero_id])\n",
    "\n",
    "    if (det_lower is not None) and (det_upper is not None):\n",
    "        det_upper_adj = (det_upper - mu_hat) / sig_hat\n",
    "        det_lower_adj = (det_lower - mu_hat) / sig_hat\n",
    "\n",
    "    if (tr_lower is not None) and (tr_upper is not None):\n",
    "        tr_upper_adj = (tr_upper - mu_hat) / sig_hat\n",
    "        tr_lower_adj = (tr_lower - mu_hat) / sig_hat\n",
    "\n",
    "    if verbose:\n",
    "        print('\\nmethod of moments estimates:')\n",
    "        print(mu_hat)\n",
    "        print(sig_hat)\n",
    "\n",
    "    samplesT = (samplesT - mu_hat) / sig_hat\n",
    "\n",
    "    mu_hatc = np.mean(samplesT, axis=0)\n",
    "    sig_hatc = np.std(samplesT, axis=0)\n",
    "\n",
    "    if verbose:\n",
    "        print('\\nstandardized estimates:')\n",
    "        print(mu_hatc)\n",
    "        print(sig_hatc)\n",
    "\n",
    "    # define initial values of distribution parameters using simple estimates\n",
    "    if ndims == 1:\n",
    "        #mu_init = np.mean(samples)\n",
    "        mu_init = mu_hatc\n",
    "        # use biased estimate for std, because MLE will converge to that anyway\n",
    "        #sig_init = np.std(samples, ddof=0)\n",
    "        sig_init = sig_hatc\n",
    "        # replace zero variance with negligible variance\n",
    "        if sig_init == 0.0:\n",
    "            sig_init = 1e-6 * np.abs(mu_init)\n",
    "        rho_init=()\n",
    "        # prepare a vector of initial values\n",
    "        inits = np.asarray([mu_init, sig_init])\n",
    "    else:\n",
    "        #mu_init = np.mean(samples, axis=1)\n",
    "        mu_init = mu_hatc\n",
    "        # use biased estimate, see comment above\n",
    "        #sig_init = np.std(samples, axis=1, ddof=0)\n",
    "        sig_init = sig_hatc\n",
    "        # replace zero variance with negligible variance\n",
    "        sig_zero_id = np.where(sig_init == 0.0)[0]\n",
    "        sig_init[sig_zero_id] = 1e-6 * np.abs(mu_init[sig_zero_id])\n",
    "        # try to create the correlation matrix\n",
    "        rho_init = np.corrcoef(np.transpose(samplesT))\n",
    "        # If there is not enough samples, or the samples are not from a\n",
    "        # multivariate normal distribution, the rho values might be nan.\n",
    "        # Rather than trying to patch it, in this case, we assume uncorrelated\n",
    "        # samples and make sure that at least the mu and sig values will be\n",
    "        # OK.\n",
    "        if np.isnan(np.sum(rho_init.flatten())):\n",
    "            rho_init = np.zeros(rho_init.shape)\n",
    "            #TODO: show a warning message to let the user know there is an issue\n",
    "            # replace nan corrcoef with zero correlation\n",
    "            #rho_init[np.where(np.isnan(rho_init))] = 0.0\n",
    "        np.fill_diagonal(rho_init,1.0)\n",
    "        # collect the independent values (i.e. elements above the main\n",
    "        # diagonal) from the correlation matrix in a list\n",
    "        rho_init_ids = np.triu_indices(ndims, k=1)\n",
    "        rho_init_list = rho_init[rho_init_ids]\n",
    "        # save the ids of the elements below the main diagonal for future use\n",
    "        rho_init_ids2 = rho_init_ids[::-1]\n",
    "        # prepare a vector of initial values\n",
    "        # if there is too few samples, we do not fit the correlation matrix\n",
    "        fit_rho = nsamples > ndims**2.0+2.0\n",
    "        if fit_rho:\n",
    "            inits = np.concatenate([mu_init, sig_init, rho_init_list])\n",
    "        else:\n",
    "            inits = np.concatenate([mu_init, sig_init])\n",
    "\n",
    "    # If the distribution is censored or truncated, check if the number of\n",
    "    # samples is greater than the number of unknowns. If not, show a warning.\n",
    "    if (((tr_lower is not None) or (tr_upper is not None)\n",
    "        or (det_lower is not None) or (det_upper is not None))\n",
    "       and (len(inits) >= nsamples)):\n",
    "        if verbose: print('samples:',nsamples,'unknowns:',len(inits))\n",
    "        warnings.warn(UserWarning(\n",
    "            \"The number of samples is less than the number of unknowns. There \"\n",
    "            \"is no unique solution available for such a case. Expect a poor \"\n",
    "            \"estimate of the distribution (especially the covariance matrix). \"\n",
    "            \"Either provide more samples, or relax the assumed dependencies \"\n",
    "            \"between variables, or remove the truncation/detection limits to \"\n",
    "            \"improve the situation.\"\n",
    "        ))\n",
    "\n",
    "    # define the bounds for the distribution parameters\n",
    "    # mu is not bounded\n",
    "    #mu_bounds = [(-np.inf, np.inf) for t in range(ndims)]\n",
    "    mu_bounds = [(-10.0, 10.0) for t in range(ndims)]\n",
    "    # sig is bounded below at (0\n",
    "    #sig_bounds = [(np.nextafter(0,1), np.inf) for s in range(ndims)]\n",
    "    sig_bounds = [(0.05, 20.0) for s in range(ndims)]\n",
    "    # rho is bounded on both sides by (-1,1)\n",
    "    # Note that -1.0 and 1.0 are not allowed to avoid numerical problems due to\n",
    "    # a singular and/or non-positive definite covariance matrix\n",
    "    if ((ndims > 1) and (fit_rho)):\n",
    "        rho_bounds = [(-1.+1e-3, 1.-1e-3) for r in range(len(rho_init_list))]\n",
    "    else:\n",
    "        # there is no need for rho bounds in a univariate case and when we do not fit rho\n",
    "        rho_bounds = []\n",
    "    # create a lower and an upper bounds vector\n",
    "    #bounds = mu_bounds + sig_bounds + rho_bounds\n",
    "    bounds = mu_bounds + sig_bounds + rho_bounds\n",
    "    bnd_lower, bnd_upper = np.transpose(bounds)\n",
    "    if verbose:\n",
    "        print('\\nbounds:')\n",
    "        print(bounds)\n",
    "        print(sig_bounds)\n",
    "        if fit_rho: print(rho_bounds)\n",
    "\n",
    "        print('\\ninitial values:')\n",
    "        print(inits)\n",
    "\n",
    "    # create a convenience function that converts a vector of distribution\n",
    "    # parameters to the standard mu and COV arrays\n",
    "    def _get_mu_COV(params, rho, unbiased=False):\n",
    "        \"\"\"\n",
    "        The unbiased flag controls if the bias in standard deviation\n",
    "        estimates shall be corrected during conversion.\n",
    "        \"\"\"\n",
    "        if ndims == 1:\n",
    "            mu, COV = params\n",
    "        else:\n",
    "            mu = params[:ndims]\n",
    "            sig = params[ndims:2*ndims]\n",
    "            if unbiased:\n",
    "                sig = sig * nsamples / (nsamples-1)\n",
    "\n",
    "            # reconstruct the covariance matrix\n",
    "            COV = np.outer(sig, sig)\n",
    "            if len(params) > 2*ndims:\n",
    "                rho_list = params[2*ndims:]\n",
    "                # add correlation estimates above...\n",
    "                COV[rho_init_ids] = COV[rho_init_ids] * rho_list\n",
    "                # and below the main diagonal\n",
    "                COV[rho_init_ids2] = COV[rho_init_ids2] * rho_list\n",
    "            else:\n",
    "                COV = np.outer(sig, sig) * rho\n",
    "\n",
    "        return mu, COV\n",
    "\n",
    "    # create the negative log likelihood function for censored data from a\n",
    "    # truncated multivariate normal distribution\n",
    "    def _neg_log_likelihood(params, rho, enforce_bounds=False):\n",
    "\n",
    "        verbose_NLL = False\n",
    "        #if verbose_NLL: print()\n",
    "        params_to_show = params[:3]\n",
    "        #params_to_show = np.sum(params) # this is useful when there are many\n",
    "\n",
    "        # first, check if the parameters are within the pre-defined bounds\n",
    "        if enforce_bounds:\n",
    "            if ((params > bnd_lower) & (params < bnd_upper)).all(0) == False:\n",
    "                # if they are not, then return an infinite value to discourage the\n",
    "                # optimization algorithm from going in that direction\n",
    "                if verbose_NLL: print(params_to_show, 'out of bounds', 1e10)\n",
    "                return 1e10\n",
    "\n",
    "        # return inf if there is nan in params:\n",
    "        if np.isnan(np.sum(params)):\n",
    "            if verbose_NLL: print(params_to_show, 'nan in params', 1e10)\n",
    "            return 1e10\n",
    "\n",
    "        # reconstruct the mu and COV arrays from the parameters\n",
    "        mu, COV = _get_mu_COV(params, rho)\n",
    "        sig = params[ndims:2*ndims]\n",
    "\n",
    "        if ndims >= 2:\n",
    "            pos_sem_def = np.all(np.linalg.eigvals(COV) >= 0.)\n",
    "            if not pos_sem_def:\n",
    "                if verbose_NLL: print(params_to_show, 'COV not pos sem def', 1e10)\n",
    "                return 1e10\n",
    "\n",
    "        # calculate the probability density within the truncation limits\n",
    "        if (tr_lower is not None) and (tr_upper is not None):\n",
    "\n",
    "            alpha, eps_alpha = mvn_orthotope_density(mu, COV,\n",
    "                                                     tr_lower_adj,\n",
    "                                                     tr_upper_adj)\n",
    "            #if verbose: print(tr_lower_adj, tr_upper_adj, mu, COV, alpha)\n",
    "            # If the error in the alpha estimate is too large, then we are\n",
    "            # beyond the applicability limits of the function used for\n",
    "            # estimating alpha. Show a warning message and try to find another\n",
    "            # solution by discouraging the optimization algorithm from going in\n",
    "            # this direction.\n",
    "            if alpha <= 100.*eps_alpha: #i.e. max. error is limited at 1%\n",
    "                # Note: throwing an error here would be too extreme, because it\n",
    "                # would stop the analysis completely, while a solution might be\n",
    "                # reached if we let the optimization algorithm converge to it.\n",
    "                if msg[0] == False:\n",
    "                    warnings.warn(UserWarning(\n",
    "                        'The density of the joint probability distribution '\n",
    "                        'within the truncation limits is too small and '\n",
    "                        'cannot be estimated with sufficiently high '\n",
    "                        'accuracy.'\n",
    "                    ))\n",
    "                    msg[0] = True\n",
    "\n",
    "                if verbose_NLL: print(params_to_show, 'alpha estimate not applicable in truncs', 1e10)\n",
    "                return 1e10\n",
    "\n",
    "            # If a lower limit was prescribed for alpha, it should also be\n",
    "            # enforced here\n",
    "            if (alpha_lim is not None) and (alpha < alpha_lim):\n",
    "                if msg[1] == False:\n",
    "                    warnings.warn(UserWarning(\n",
    "                        'The density of the joint probability distribution '\n",
    "                        'within the truncation limits is less than the '\n",
    "                        'prescribed minimum limit.'\n",
    "                    ))\n",
    "                    msg[1] = True\n",
    "\n",
    "                if verbose_NLL: print(params_to_show, 'not enough prob mass within truncs', 1e10)\n",
    "                return 1e10\n",
    "\n",
    "        else:\n",
    "            alpha, eps_alpha = 1., 0.\n",
    "\n",
    "        # calculate the likelihood for each available sample\n",
    "        likelihoods = multivariate_normal.pdf(samplesT, mean=mu, cov=COV,\n",
    "                                              allow_singular=True)\n",
    "\n",
    "        # Zeros are a result of limited floating point precision. Replace them\n",
    "        # with the smallest possible positive floating point number to\n",
    "        # improve convergence.\n",
    "        likelihoods = np.clip(likelihoods, a_min=np.nextafter(0,1), a_max=None)\n",
    "\n",
    "        # calculate the likelihoods corresponding to censored data (if any)\n",
    "        if censored_count > 0:\n",
    "\n",
    "            # calculate the probability density within the detection limits\n",
    "            det_alpha, eps_alpha = mvn_orthotope_density(mu, COV,\n",
    "                                                         det_lower_adj,\n",
    "                                                         det_upper_adj)\n",
    "            # Similarly to alpha above, make sure that det_alpha is estimated\n",
    "            # with sufficient accuracy.\n",
    "            if det_alpha <= 100.*eps_alpha:\n",
    "                if msg[2] == False:\n",
    "                    warnings.warn(\n",
    "                        'The density of the joint probability distribution '\n",
    "                        'within the detection limits is too small and '\n",
    "                        'cannot be estimated with sufficiently high '\n",
    "                        'accuracy. '\n",
    "                        '(alpha: '+str(det_alpha)+' eps: '+str(eps_alpha)+')'\n",
    "                    )\n",
    "                    msg[2] = True\n",
    "\n",
    "                if verbose_NLL: print(params_to_show, 'alpha estimate not applicable in dets', 1e10)\n",
    "                return 1e10\n",
    "\n",
    "            # calculate the likelihood of censoring a sample\n",
    "            cen_likelihood = (alpha - det_alpha) / alpha\n",
    "\n",
    "            # make sure that the likelihood is a positive number\n",
    "            cen_likelihood = max(cen_likelihood, np.nextafter(0,1))\n",
    "\n",
    "            if verbose_NLL :\n",
    "                pass\n",
    "                #print('dets and cen_liks')\n",
    "                #print(det_lower, det_lower_adj)\n",
    "                #print(det_upper, det_upper_adj)\n",
    "                #print(det_alpha, cen_likelihood)\n",
    "\n",
    "        else:\n",
    "            # If the data is not censored, use 1.0 for cen_likelihood to get a\n",
    "            # zero log-likelihood later. Note that although this is\n",
    "            # theoretically not correct, it does not alter the solution and\n",
    "            # it is numerically much more convenient than working around the\n",
    "            # log of zero likelihood.\n",
    "            cen_likelihood = 1.\n",
    "\n",
    "        # calculate the total negative log-likelihood\n",
    "        NLL = -(\n",
    "            np.sum(np.log(likelihoods))                 # from samples\n",
    "            - nsamples*np.log(alpha)                    # truncation influence\n",
    "            + censored_count*np.log(cen_likelihood)     # censoring influence\n",
    "        )\n",
    "\n",
    "        # normalize the likelihoods with the sample count\n",
    "        NLL = NLL/nsamples\n",
    "        #print(mu[-4:], NLL)\n",
    "        #print(np.sqrt(np.diagonal(COV))[-4:],NLL)\n",
    "\n",
    "        if verbose_NLL: pass\n",
    "        #print(params_to_show, 'all good', NLL)\n",
    "        return NLL\n",
    "\n",
    "    # initialize the message flags\n",
    "    msg = [False, False, False]\n",
    "    if verbose: print(_neg_log_likelihood(inits, rho_init))\n",
    "\n",
    "    # perturbation\n",
    "    #inits[:ndims] = inits[:ndims]+np.random.uniform(low=-0.5, high=0.5, size=ndims)\n",
    "    #inits[ndims:2*ndims] += 0.5\n",
    "\n",
    "    if verbose: t_0 = time.time()\n",
    "    # minimize the negative log-likelihood function\n",
    "    #out = minimize(_neg_log_likelihood, inits, args=(rho_init, True),\n",
    "    #               bounds=bounds, method='TNC')\n",
    "\n",
    "    out_d = differential_evolution(_neg_log_likelihood, mu_bounds + sig_bounds,\n",
    "                                   args=(rho_init,),\n",
    "                                   maxiter=200,\n",
    "                                   polish=False)\n",
    "    if verbose:\n",
    "        print(out_d)\n",
    "        #print(out.fun, out.nfev, out.nit, out.message, out.x)\n",
    "        print('runtime: ', time.time() - t_0)\n",
    "\n",
    "    # minimize the negative log-likelihood function using the adaptive\n",
    "    # Adaptive Nelder-Mead algorithm (Gao and Han, 2012)\n",
    "    out_m = minimize(_neg_log_likelihood, np.concatenate([out_d.x, inits[2*ndims:]]),\n",
    "                   args=(rho_init,True), method='Nelder-Mead',\n",
    "                   options=dict(maxfev=1000*ndims,\n",
    "                                xatol = 0.001,\n",
    "                                fatol = 1e-10,\n",
    "                                adaptive=True)\n",
    "                   )\n",
    "\n",
    "    if verbose:\n",
    "        print(out_m)\n",
    "        #print(out.fun, out.nfev, out.nit, out.message, out.x)\n",
    "        print('runtime: ', time.time() - t_0)\n",
    "\n",
    "    # reconstruct the mu and COV arrays from the solutions and return them\n",
    "    mu, COV = _get_mu_COV(out_m.x, rho_init, unbiased=True)\n",
    "\n",
    "    if verbose:\n",
    "        if ndims >= 2:\n",
    "            print('mu vs mu_init')\n",
    "            show_matrix(list(zip(mu,mu_init)))\n",
    "            sig = np.sqrt(np.diagonal(COV))\n",
    "            print('sig vs sig_init')\n",
    "            show_matrix(list(zip(sig,sig_init)))\n",
    "            rho = (COV/np.outer(sig,sig))[-8:,-8:]\n",
    "            print('rho')\n",
    "            show_matrix(rho)\n",
    "            print('rho_init')\n",
    "            show_matrix(rho_init[-8:,-8:])\n",
    "        else:\n",
    "            print('mu vs mu_init')\n",
    "            print(mu, mu_init)\n",
    "            sig = np.sqrt(COV)\n",
    "            print('sig vs sig_init')\n",
    "            print(sig, sig_init)\n",
    "\n",
    "    if ndims >= 2:\n",
    "        sig = np.sqrt(np.diagonal(COV))\n",
    "        rho = (COV / np.outer(sig, sig))\n",
    "        sig = sig * sig_hat\n",
    "        COV = np.outer(sig, sig) * rho\n",
    "    else:\n",
    "        sig = np.sqrt(COV)\n",
    "        sig = sig*sig_hat\n",
    "        COV = sig**2.0\n",
    "    mu = mu_hat + mu * sig_hat\n",
    "\n",
    "    if verbose:\n",
    "        print('\\nfinal values:')\n",
    "        print(mu)\n",
    "        print(sig)\n",
    "        print(COV)\n",
    "\n",
    "    return mu, COV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVariable(object):\n",
    "    \"\"\"\n",
    "    Characterizes a Random Variable (RV) that represents a source of\n",
    "    uncertainty in the calculation.\n",
    "\n",
    "    The uncertainty can be described either through raw data or through a\n",
    "    pre-defined distribution function. When using raw data, provide potentially\n",
    "    correlated raw samples in a 2 dimensional array. If the data is left or\n",
    "    right censored in any number of its dimensions, provide the list of\n",
    "    detection limits and the number of censored samples. No other information\n",
    "    is needed to define the object from raw data. Then, either resample the raw\n",
    "    data, or fit a prescribed distribution to the samples and sample from that\n",
    "    distribution later.\n",
    "\n",
    "    Alternatively, one can choose to prescribe a distribution type and its\n",
    "    parameters and sample from that distribution later.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ID: int\n",
    "    dimension_tags: str array\n",
    "        A series of strings that identify the stochastic model parameters that\n",
    "        correspond to each dimension of the random variable. When the RV is one\n",
    "        dimensional, the dim_tag is a single string. In multi-dimensional\n",
    "        cases, the order of strings shall match the order of elements provided\n",
    "        as other inputs.\n",
    "    raw_data: float scalar or ndarray, optional, default: None\n",
    "        Samples of an uncertain variable. The samples can describe a\n",
    "        multi-dimensional random variable if they are arranged in a 2D ndarray.\n",
    "    detection_limits: float ndarray, optional, default: None\n",
    "        Defines the limits for censored data. The limits need to be defined in\n",
    "        a 2D ndarray that is structured as two vectors with N elements. The\n",
    "        vectors collect left and right limits for the N dimensions. If the data\n",
    "        is not censored in a particular direction, assign None to that position\n",
    "        of the ndarray. Replacing one of the vectors with None will assign no\n",
    "        censoring to all dimensions in that direction. The default value\n",
    "        corresponds to no censoring in either dimension.\n",
    "    censored_count: int, optional, default: None\n",
    "        The number of censored samples that are beyond the detection limits.\n",
    "        All samples outside the detection limits are aggregated into one set.\n",
    "        This works the same way in one and in multiple dimensions. Prescription\n",
    "        of censored sample counts for sub-regions of the input space outside\n",
    "        the detection limits is not yet supported. If such an approach is\n",
    "        desired, the censored raw data shall be used to fit a distribution in a\n",
    "        pre-processing step and the fitted distribution can be specified for\n",
    "        this random variable.\n",
    "    distribution_kind: {'normal', 'lognormal', 'multinomial'}, optional, default: None\n",
    "        Defines the type of probability distribution when raw data is not\n",
    "        provided, but the distribution is directly specified. When part of the\n",
    "        data is normal in log space, while the other part is normal in linear\n",
    "        space, define a list of distribution tags such as ['normal', 'normal',\n",
    "        'lognormal']. Make sure that the covariance matrix is based on log\n",
    "        transformed data for the lognormally distributed variables! Mixing\n",
    "        normal distributions with multinomials is not supported.\n",
    "    theta: float scalar or ndarray, optional, default: None\n",
    "        Median of the probability distribution. A vector of medians is expected\n",
    "        in a multi-dimensional case.\n",
    "    COV: float scalar or 2D ndarray, optional, default: None\n",
    "        Covariance matrix of the random variable. In a multi-dimensional case\n",
    "        this parameter has to be a 2D square ndarray, and the number of its\n",
    "        rows has to be equal to the number of elements in the supplied theta\n",
    "        vector. In a one-dimensional case, a single value is expected that\n",
    "        equals the variance (not the standard deviation!) of the distribution.\n",
    "        The COV for lognormal variables is assumed to be specified in\n",
    "        logarithmic space.\n",
    "    corr_ref: {'pre', 'post'}, optional, default: 'pre'\n",
    "        Determines whether the correlations prescribed by the covariance matrix\n",
    "        refer to the distribution functions before or after truncation. The\n",
    "        default 'pre' setting assumes that pre-truncation correlations are\n",
    "        prescribed and creates a multivariate normal distribution using the\n",
    "        COV matrix. That distribution is truncated according to the prescribed\n",
    "        truncation limits. The other option assumes that post-truncation\n",
    "        correlations are prescribed. The post-truncation distribution\n",
    "        is not multivariate normal in general. Currently we use a Gaussian\n",
    "        copula to describe the dependence between the truncated variables.\n",
    "        Similarly to other characteristics, the `corr_ref` can be defined as a\n",
    "        single string, or a vector of strings. The former assigns the same\n",
    "        option to all dimensions, while the latter allows for more flexible\n",
    "        assignment by setting the corr_ref for each dimension individually.\n",
    "    p_set: float vector, optional, default: None\n",
    "        Probabilities of a finite set of events described by a multinomial\n",
    "        distribution. The RV will have binomial distribution if only one\n",
    "        element is provided in this vector. The number of events equals the\n",
    "        number of vector elements if their probabilities sum up to 1.0. If the\n",
    "        sum is less than 1.0, then an additional event is assumed with the\n",
    "        remaining probability of occurrence assigned to it. The sum of\n",
    "        event probabilities shall never be more than 1.0.\n",
    "    truncation_limits: float ndarray, optional, default: None\n",
    "        Defines the limits for truncated distributions. The limits need to be\n",
    "        defined in a 2D ndarray that is structured as two vectors with N\n",
    "        elements. The vectors collect left and right limits for the N\n",
    "        dimensions. If the distribution is not truncated in a particular\n",
    "        direction, assign None to that position of the ndarray. Replacing one\n",
    "        of the vectors with None will assign no truncation to all dimensions\n",
    "        in that direction. The default value corresponds to no truncation in\n",
    "        either dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ID, dimension_tags,\n",
    "                 raw_data=None, detection_limits=None, censored_count=None,\n",
    "                 distribution_kind=None,\n",
    "                 theta=None, COV=None, corr_ref='pre', p_set=None,\n",
    "                 truncation_limits=None):\n",
    "        self._ID = ID\n",
    "\n",
    "        self._dimension_tags = np.asarray(dimension_tags)\n",
    "\n",
    "        if raw_data is not None:\n",
    "            raw_data = np.asarray(raw_data)\n",
    "            if len(raw_data.shape) > 1:\n",
    "                self._ndim, self._ncount = raw_data.shape[:2]\n",
    "            else:\n",
    "                self._ndim = 1\n",
    "                self._ncount = raw_data.shape[0]\n",
    "        self._raw_data = raw_data\n",
    "\n",
    "        if self._raw_data is not None:\n",
    "            self._detection_limits = self._convert_limits(detection_limits)\n",
    "            self._censored_count = censored_count\n",
    "        else:\n",
    "            self._detection_limits = self._convert_limits(None)\n",
    "            self._censored_count = None\n",
    "\n",
    "        if distribution_kind is not None:\n",
    "            distribution_kind = np.asarray(distribution_kind)\n",
    "        self._distribution_kind = distribution_kind\n",
    "\n",
    "        if self._distribution_kind is not None:\n",
    "            if theta is not None:\n",
    "                theta = np.asarray(theta)\n",
    "                if theta.shape == ():\n",
    "                    self._ndim = 1\n",
    "                else:\n",
    "                    self._ndim = theta.shape[0]\n",
    "            self._theta = theta\n",
    "\n",
    "            if COV is not None:\n",
    "                COV = np.asarray(COV)\n",
    "            self._COV = COV\n",
    "\n",
    "            self._corr_ref = np.asarray(corr_ref)\n",
    "\n",
    "            if p_set is not None:\n",
    "                p_set = np.asarray(p_set)\n",
    "                if np.sum(p_set) < 1.:\n",
    "                    p_set = np.append(p_set, 1.-np.sum(p_set))\n",
    "                self._ndim = 1\n",
    "            self._p_set = p_set\n",
    "\n",
    "            tr_limits = self._convert_limits(truncation_limits)\n",
    "            self._tr_limits_pre, self._tr_limits_post = \\\n",
    "                self._create_pre_post_tr_limits(tr_limits)\n",
    "        else:\n",
    "            self._theta = None\n",
    "            self._COV = None\n",
    "            self._corr_ref = corr_ref\n",
    "            self._p_set = None\n",
    "            self._tr_limits_pre = self._convert_limits(None)\n",
    "            self._tr_limits_post = deepcopy(self._tr_limits_pre)\n",
    "\n",
    "        # perform some basic checks to make sure that the provided data will be\n",
    "        # sufficient to define a random variable\n",
    "        if self._raw_data is None:\n",
    "\n",
    "            # if the RV is defined by providing distribution data...\n",
    "            if self._distribution_kind is None:\n",
    "                raise ValueError(\n",
    "                    \"Either raw samples or a distribution needs to be defined \"\n",
    "                    \"for a random variable.\"\n",
    "                )\n",
    "\n",
    "            if ((self._distribution_kind.shape!=())\n",
    "                or ((self._distribution_kind.shape==())\n",
    "                    and (self._distribution_kind in ['normal', 'lognormal']))):\n",
    "                if (self._theta is None) or (self._COV is None):\n",
    "                    raise ValueError(\n",
    "                        \"Normal and lognormal distributions require theta and \"\n",
    "                        \"COV parameters.\"\n",
    "                    )\n",
    "\n",
    "            if ((self._distribution_kind.shape==())\n",
    "                and (self._distribution_kind in ['multinomial'])\n",
    "                and (self._p_set is None)):\n",
    "                raise ValueError(\n",
    "                    \"Multinomial distributions require a set of p values as \"\n",
    "                    \"parameters.\"\n",
    "                )\n",
    "        else:\n",
    "\n",
    "            # if the RV is defined through raw samples\n",
    "            if ((self._detection_limits is None) !=\n",
    "                (self._censored_count is None)):\n",
    "                raise ValueError(\n",
    "                    \"Definition of censored data requires information about \"\n",
    "                    \"the detection limits and the number of censored samples.\"\n",
    "                )\n",
    "\n",
    "    def _convert_limits(self, limits):\n",
    "        \"\"\"\n",
    "        Convert None values to infinites in truncation and detection limits.\n",
    "\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_ndim') and (limits is not None):\n",
    "            # convert single-element limits array into a nested\n",
    "            # structure that is easier to work with\n",
    "            if not isinstance(limits[0], (list, tuple, np.ndarray)):\n",
    "                limits[0] = [limits[0], ]\n",
    "            if not isinstance(limits[1], (list, tuple, np.ndarray)):\n",
    "                limits[1] = [limits[1], ]\n",
    "\n",
    "            # assign a vector of None in place of a single None value\n",
    "            if (len(limits[0]) == 1) and (limits[0][0] is None):\n",
    "                limits[0] = [None for d in range(self._ndim)]\n",
    "            if (len(limits[1]) == 1) and (limits[1][0] is None):\n",
    "                limits[1] = [None for d in range(self._ndim)]\n",
    "\n",
    "            # replace None values with infinite limits\n",
    "            for l_i, l in enumerate(limits[0]):\n",
    "                if l is None:\n",
    "                    limits[0][l_i] = -np.inf\n",
    "            for l_i, l in enumerate(limits[1]):\n",
    "                if l is None:\n",
    "                    limits[1][l_i] = np.inf\n",
    "\n",
    "            limits = np.array(limits, dtype=np.float64)\n",
    "\n",
    "        return limits\n",
    "\n",
    "    def _create_pre_post_tr_limits(self, truncation_limits):\n",
    "        \"\"\"\n",
    "        Separates the truncation limits into two groups: (i) `pre` truncation\n",
    "        limits apply to distributions where the correlations refer to the\n",
    "        joint distribution before truncation; (ii) `post` truncation limits\n",
    "        apply to distributions after truncation. Truncation in the latter case\n",
    "        is applied differently, hence the need to separate the two types of\n",
    "        modifications.\n",
    "\n",
    "        \"\"\"\n",
    "        if (truncation_limits is not None) and (hasattr(self, '_ndim')):\n",
    "            tr_lower, tr_upper = truncation_limits\n",
    "            CR = self._corr_ref\n",
    "\n",
    "            # a single value or identical values means one setting\n",
    "            # applies to all dims\n",
    "            if (CR.size == 1) or (np.unique(CR).size == 1):\n",
    "                if CR.size > 1:\n",
    "                    CR = CR[0]\n",
    "                if CR == 'pre':\n",
    "                    trl_pre = deepcopy(truncation_limits)\n",
    "                    trl_post = None\n",
    "                elif CR == 'post':\n",
    "                    trl_pre = None\n",
    "                    trl_post = deepcopy(truncation_limits)\n",
    "            else:\n",
    "                # otherwise assign the appropriate limits to each dim\n",
    "                tr_lower_pre, tr_lower_post = -np.ones((2,self._ndim))*np.inf\n",
    "                tr_upper_pre, tr_upper_post = np.ones((2,self._ndim))*np.inf\n",
    "                tr_lower_pre[CR == 'pre'] = tr_lower[CR == 'pre']\n",
    "                tr_upper_pre[CR == 'pre'] = tr_upper[CR == 'pre']\n",
    "                tr_lower_post[CR == 'post'] = tr_lower[CR == 'post']\n",
    "                tr_upper_post[CR == 'post'] = tr_upper[CR == 'post']\n",
    "\n",
    "                trl_pre = np.asarray([tr_lower_pre, tr_upper_pre])\n",
    "                trl_post = np.asarray([tr_lower_post, tr_upper_post])\n",
    "        else:\n",
    "            trl_pre, trl_post = None, None\n",
    "\n",
    "        return trl_pre, trl_post\n",
    "\n",
    "    def _move_to_log(self, raw_data, distribution_list):\n",
    "        \"\"\"\n",
    "        Convert data to log space for the lognormal variables.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if distribution_list is None:\n",
    "            data = raw_data\n",
    "        elif distribution_list.shape == ():\n",
    "            # meaning identical distribution families\n",
    "            data = raw_data\n",
    "            if distribution_list == 'lognormal':\n",
    "                if np.min(data) > 0.:\n",
    "                    data = np.log(raw_data)\n",
    "                else:\n",
    "                    # this can gracefully handle non-positive limits for\n",
    "                    # lognormal distributions\n",
    "                    min_float = np.nextafter(0, 1)\n",
    "                    data = np.log(np.clip(raw_data, a_min=min_float,\n",
    "                                          a_max=None))\n",
    "                    if np.asarray(data).shape == ():\n",
    "                        if data == np.log(min_float):\n",
    "                            data = -np.inf\n",
    "                    # Although the following code would help with other\n",
    "                    # incorrect data, it might also hide such problems and\n",
    "                    # the current implementation does not need it, so it\n",
    "                    # is disabled for now.\n",
    "                    #else:\n",
    "                    #    data[data==np.log(min_float)] = -np.inf\n",
    "        else:\n",
    "            data = deepcopy(raw_data)\n",
    "            for dim, dk in enumerate(distribution_list):\n",
    "                if dk == 'lognormal':\n",
    "                    if np.min(data[dim]) > 0.:\n",
    "                        data[dim] = np.log(data[dim])\n",
    "                    else:\n",
    "                        # this can gracefully handle non-positive limits for\n",
    "                        # lognormal distributions\n",
    "                        min_float = np.nextafter(0, 1)\n",
    "                        data[dim] = np.log(np.clip(data[dim], a_min=min_float,\n",
    "                                                   a_max=None))\n",
    "                        if np.asarray(data[dim]).shape == ():\n",
    "                            if data[dim] == np.log(min_float):\n",
    "                                data[dim] = -np.inf\n",
    "                        #else:\n",
    "                        #    data[dim][data[dim]==np.log(min_float)] = -np.inf\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _return_from_log(self, raw_data, distribution_list):\n",
    "        \"\"\"\n",
    "        Convert data back to linear space for the lognormal variables.\n",
    "\n",
    "        \"\"\"\n",
    "        if distribution_list.shape == ():\n",
    "            # meaning identical distribution families\n",
    "            data = raw_data\n",
    "            if distribution_list == 'lognormal':\n",
    "                data = np.exp(raw_data)\n",
    "        else:\n",
    "            data = raw_data\n",
    "            for dim, dk in enumerate(distribution_list):\n",
    "                if dk == 'lognormal':\n",
    "                    data[dim] = np.exp(data[dim])\n",
    "\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def distribution_kind(self):\n",
    "        \"\"\"\n",
    "        Return the assigned probability distribution family.\n",
    "        \"\"\"\n",
    "        return self._distribution_kind\n",
    "\n",
    "    @property\n",
    "    def theta(self):\n",
    "        \"\"\"\n",
    "        Return the median value(s) of the probability distribution.\n",
    "        \"\"\"\n",
    "        if self._theta is not None:\n",
    "            return self._theta\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The median of the probability distribution of this random \"\n",
    "                \"variable is not yet specified.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def mu(self):\n",
    "        \"\"\"\n",
    "        Return the mean value(s) of the probability distribution. Note that\n",
    "        the mean value is in log space for lognormal distributions.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._move_to_log(self.theta, self._distribution_kind)\n",
    "\n",
    "    @property\n",
    "    def COV(self):\n",
    "        \"\"\"\n",
    "        Return the covariance matrix of the probability distribution. Note that\n",
    "        the covariances are in log space for lognormal distributions.\n",
    "        \"\"\"\n",
    "        if self._COV is not None:\n",
    "            return self._COV\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The covariance matrix of the probability distribution of \"\n",
    "                \"this random variable is not yet specified.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def corr(self):\n",
    "        \"\"\"\n",
    "        Return the correlation matrix of the probability distribution. Note that\n",
    "        correlation coefficient correspond to the joint distribution in log\n",
    "        space for lognormal distributions.\n",
    "        \"\"\"\n",
    "        if self._COV is not None:\n",
    "            return self._COV / np.outer(self.sig, self.sig)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The correlation matrix of the probability distribution of \"\n",
    "                \"this random variable is not yet specified.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def var(self):\n",
    "        \"\"\"\n",
    "        Return the variance vector of the probability distribution. Note that\n",
    "        the variances are in log space for lognormal distributions.\n",
    "        \"\"\"\n",
    "        if self._COV is not None:\n",
    "            return np.diagonal(self._COV)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The covariance matrix of the probability distribution of \"\n",
    "                \"this random variable is not yet specified.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def sig(self):\n",
    "        \"\"\"\n",
    "        Return the standard deviation vector of the probability distribution.\n",
    "        Note that the standard deviations are in log space for lognormal\n",
    "        distributions.\n",
    "        \"\"\"\n",
    "        if self._COV is not None:\n",
    "            return np.sqrt(self.var)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The covariance matrix of the probability distribution of \"\n",
    "                \"this random variable is not yet specified.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def dimension_tags(self):\n",
    "        \"\"\"\n",
    "        Return the tags corresponding to the dimensions of the variable.\n",
    "\n",
    "        \"\"\"\n",
    "        # this is very simple for now\n",
    "        return self._dimension_tags\n",
    "\n",
    "    @property\n",
    "    def detection_limits(self):\n",
    "        \"\"\"\n",
    "        Return the detection limits corresponding to the raw data in linear\n",
    "        space.\n",
    "\n",
    "        \"\"\"\n",
    "        # this is very simple for now\n",
    "        return self._detection_limits\n",
    "\n",
    "    @property\n",
    "    def det_lower(self):\n",
    "        \"\"\"\n",
    "        Return the lower detection limit(s) corresponding to the raw data in\n",
    "        either linear or log space according to the distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._detection_limits is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self._move_to_log(self._detection_limits[0],\n",
    "                                     self._distribution_kind)\n",
    "\n",
    "    @property\n",
    "    def det_upper(self):\n",
    "        \"\"\"\n",
    "        Return the upper detection limit(s) corresponding to the raw data in\n",
    "        either linear or log space according to the distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._detection_limits is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self._move_to_log(self._detection_limits[1],\n",
    "                                     self._distribution_kind)\n",
    "\n",
    "    @property\n",
    "    def tr_limits_pre(self):\n",
    "        \"\"\"\n",
    "        Return the `pre` truncation limits of the probability distribution in\n",
    "        linear space.\n",
    "\n",
    "        \"\"\"\n",
    "        # this is very simple for now\n",
    "        return self._tr_limits_pre\n",
    "\n",
    "    @property\n",
    "    def tr_limits_post(self):\n",
    "        \"\"\"\n",
    "        Return the `post` truncation limits of the probability distribution in\n",
    "        linear space.\n",
    "\n",
    "        \"\"\"\n",
    "        # this is very simple for now\n",
    "        return self._tr_limits_post\n",
    "\n",
    "    @property\n",
    "    def tr_lower_pre(self):\n",
    "        \"\"\"\n",
    "        Return the lower `pre` truncation limit(s) corresponding to the\n",
    "        distribution in either linear or log space according to the\n",
    "        distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._tr_limits_pre is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self._move_to_log(self._tr_limits_pre[0],\n",
    "                                     self._distribution_kind)\n",
    "\n",
    "    @property\n",
    "    def tr_upper_pre(self):\n",
    "        \"\"\"\n",
    "        Return the upper `pre` truncation limit(s) corresponding to the\n",
    "        distribution in either linear or log space according to the\n",
    "        distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._tr_limits_pre is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self._move_to_log(self._tr_limits_pre[1],\n",
    "                                     self._distribution_kind)\n",
    "\n",
    "    @property\n",
    "    def tr_lower_post(self):\n",
    "        \"\"\"\n",
    "        Return the lower `post` truncation limit(s) corresponding to the\n",
    "        distribution in either linear or log space according to the\n",
    "        distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._tr_limits_post is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self._move_to_log(self._tr_limits_post[0],\n",
    "                                     self._distribution_kind)\n",
    "\n",
    "    @property\n",
    "    def tr_upper_post(self):\n",
    "        \"\"\"\n",
    "        Return the upper `post` truncation limit(s) corresponding to the\n",
    "        distribution in either linear or log space according to the\n",
    "        distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._tr_limits_post is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self._move_to_log(self._tr_limits_post[1],\n",
    "                                     self._distribution_kind)\n",
    "\n",
    "    @property\n",
    "    def censored_count(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples beyond the detection limits.\n",
    "\n",
    "        \"\"\"\n",
    "        if self._censored_count is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self._censored_count\n",
    "\n",
    "    @property\n",
    "    def samples(self):\n",
    "        \"\"\"\n",
    "        Return the pre-generated samples from the distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_samples'):\n",
    "            return self._samples\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def raw(self):\n",
    "        \"\"\"\n",
    "        Return the pre-assigned raw data.\n",
    "\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_raw_data'):\n",
    "            return self._raw_data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def fit_distribution(self, distribution_kind, truncation_limits=None):\n",
    "        \"\"\"\n",
    "        Estimate the parameters of a probability distribution from raw data.\n",
    "\n",
    "        Parameter estimates are calculated using maximum likelihood estimation.\n",
    "        If the data spans multiple dimensions, the estimates will also describe\n",
    "        a multi-dimensional distribution automatically. Data censoring is also\n",
    "        automatically taken into consideration following the detection limits\n",
    "        specified previously for the random variable. Truncated target\n",
    "        distributions can be specified through the truncation limits. The\n",
    "        specified truncation limits are applied after the correlations are set.\n",
    "        In other words, the corr_ref proprety of the RV is set to 'pre' when\n",
    "        fitting a distribution.\n",
    "\n",
    "        Besides returning the parameter estimates, their values are also stored\n",
    "        as theta and COV attributes of the RandomVariable object for future\n",
    "        use.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        distribution_kind: {'normal', 'lognormal'} or a list of those\n",
    "            Specifies the type of the probability distribution that is fit to\n",
    "            the raw data. When part of the data is normal in log space, while\n",
    "            the other part is normal in linear space, define a list of\n",
    "            distribution tags such as ['normal', 'normal', 'lognormal'].\n",
    "        truncation_limits: float ndarray, optional, default: None\n",
    "            Defines the limits for truncated distributions. The limits need to\n",
    "            be defined in a 2D ndarray that is structured as two vectors with N\n",
    "            elements. The vectors collect left and right limits for the N\n",
    "            dimensions. If the distribution is not truncated in a particular\n",
    "            direction, assign None to that position of the ndarray. Replacing\n",
    "            one of the vectors with None will assign no truncation to all\n",
    "            dimensions in that direction. The default value corresponds to no\n",
    "            truncation in either dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        theta: float scalar or ndarray\n",
    "            Median of the probability distribution. A vector of medians is\n",
    "            returned in a multi-dimensional case.\n",
    "        COV: float scalar or 2D ndarray\n",
    "            Covariance matrix of the probability distribution. A 2D square\n",
    "            ndarray is returned in a multi-dimensional case.\n",
    "        \"\"\"\n",
    "\n",
    "        # lognormal distribution parameters are estimated by fitting a normal\n",
    "        # distribution to the data in log space\n",
    "        distribution_kind = np.asarray(distribution_kind)\n",
    "        data = self._move_to_log(self._raw_data, distribution_kind)\n",
    "\n",
    "        # prepare the information on truncation\n",
    "        if truncation_limits is not None:\n",
    "            tr_lower, tr_upper = self._convert_limits(truncation_limits)\n",
    "            tr_lower = self._move_to_log(tr_lower, distribution_kind)\n",
    "            tr_upper = self._move_to_log(tr_upper, distribution_kind)\n",
    "        else:\n",
    "            tr_lower, tr_upper = None, None\n",
    "\n",
    "        # convert the detection limits to log if needed\n",
    "        if self.detection_limits is not None:\n",
    "            det_lower, det_upper = self.detection_limits\n",
    "            det_lower = self._move_to_log(det_lower, distribution_kind)\n",
    "            det_upper = self._move_to_log(det_upper, distribution_kind)\n",
    "        else:\n",
    "            det_lower, det_upper = None, None\n",
    "\n",
    "        # perform the parameter estimation\n",
    "        mu, COV = tmvn_MLE(data,\n",
    "                           tr_lower = tr_lower, tr_upper=tr_upper,\n",
    "                           censored_count=self.censored_count,\n",
    "                           det_lower=det_lower, det_upper=det_upper)\n",
    "\n",
    "        # convert mu to theta\n",
    "        theta = self._return_from_log(mu, distribution_kind)\n",
    "\n",
    "        # store and return the parameters\n",
    "        self._theta = theta\n",
    "        self._COV = COV\n",
    "        self._corr_ref = 'pre'\n",
    "        #TODO: implement 'post' corr_ref as an option for fitting\n",
    "\n",
    "        # store the distribution properties\n",
    "        self._distribution_kind = distribution_kind\n",
    "        self._tr_limits_pre = truncation_limits\n",
    "\n",
    "        return theta, COV\n",
    "\n",
    "    def sample_distribution(self, sample_size, preserve_order=False):\n",
    "        \"\"\"\n",
    "        Sample the probability distribution assigned to the random variable.\n",
    "\n",
    "        Normal distributions (including truncated and/or multivariate normal\n",
    "        and lognormal) are sampled using the tmvn_rvs() method in this module.\n",
    "        If post-truncation correlations are set for a dimension, the\n",
    "        corresponding truncations are enforced after sampling by first applying\n",
    "        probability integral transformation to transform samples from the\n",
    "        non-truncated normal to standard uniform distribution, and then\n",
    "        applying inverse probability integral transformation to transform the\n",
    "        samples from standard uniform to the desired truncated normal\n",
    "        distribution. Multinomial distributions are sampled using the\n",
    "        multinomial method in scipy. The samples are returned and also stored\n",
    "        in the `sample` attribute of the RV.\n",
    "\n",
    "        If the random variable is defined by raw data only, we sample from the\n",
    "        raw data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample_size: int\n",
    "            Number of samples requested.\n",
    "        preserve_order: bool, default: False\n",
    "            Influences sampling from raw data. If True, the samples are copies\n",
    "            of the first n rows of the raw data where n is the sample_size. This\n",
    "            only works for sample_size <= raw data size. If False, the samples\n",
    "            are drawn from the raw data pool with replacement.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        samples: DataFrame\n",
    "            Samples generated from the distribution. Columns correspond to the\n",
    "            dimension tags that identify the variables.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._distribution_kind is not None:\n",
    "            if ((self._distribution_kind.shape == ()) and\n",
    "                (self._distribution_kind == 'multinomial')):\n",
    "\n",
    "                # sampling the multinomial distribution\n",
    "                samples = multinomial.rvs(1, self._p_set, size=sample_size)\n",
    "\n",
    "                # convert the 2D sample array into a vector of integers\n",
    "                outcomes = np.array([np.arange(len(self._p_set))])\n",
    "                samples = np.matmul(samples, outcomes.T).flatten()\n",
    "                samples = pd.DataFrame(np.transpose(samples),\n",
    "                                       columns=self._dimension_tags)\n",
    "            else:\n",
    "                # sampling the truncated multivariate normal distribution\n",
    "                raw_samples = tmvn_rvs(mu=self.mu, COV=self.COV,\n",
    "                                       lower=self.tr_lower_pre,\n",
    "                                       upper=self.tr_upper_pre,\n",
    "                                       size=sample_size)\n",
    "                raw_samples = np.transpose(raw_samples)\n",
    "\n",
    "                # enforce post-truncation correlations if needed\n",
    "                if self.tr_limits_post is not None:\n",
    "                    lower, upper = self.tr_lower_post, self.tr_upper_post\n",
    "                    for dim in range(self._ndim):\n",
    "                        if (lower[dim] > -np.inf) or (upper[dim]<np.inf):\n",
    "                            mu = self.mu[dim]\n",
    "                            sig = np.sqrt(self.COV[dim,dim])\n",
    "                            samples_U = norm.cdf(raw_samples[dim],loc=mu,scale=sig)\n",
    "                            raw_samples[dim] = truncnorm.ppf(\n",
    "                                samples_U, loc=mu, scale=sig,\n",
    "                                a = (lower[dim]-mu)/sig, b=(upper[dim]-mu)/sig)\n",
    "\n",
    "                # transform samples back from log space if needed\n",
    "                samples = self._return_from_log(raw_samples,\n",
    "                                                self._distribution_kind)\n",
    "\n",
    "                samples = pd.DataFrame(data=np.transpose(samples),\n",
    "                                       index=np.arange(sample_size),\n",
    "                                       columns=self._dimension_tags)\n",
    "\n",
    "                samples = samples.astype(np.float64)\n",
    "        else:\n",
    "            if self._raw_data is not None:\n",
    "\n",
    "                if preserve_order:\n",
    "                    if self._ncount >= sample_size:\n",
    "                        if self._ndim > 1:\n",
    "                            samples = self._raw_data[:, :sample_size]\n",
    "                        else:\n",
    "                            samples = self._raw_data[:sample_size]\n",
    "\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"The number of samples requested is larger than \"\n",
    "                            \"number of raw data points available. Either \"\n",
    "                            \"sample without preserving order or reduce the \"\n",
    "                            \"sample size.\")\n",
    "\n",
    "                else:\n",
    "                    # generate a random list of indices\n",
    "                    id_list = np.random.uniform(0, self._ncount,\n",
    "                                                size=sample_size)\n",
    "\n",
    "                    # get the raw data that corresponds to the random ids\n",
    "                    if self._ndim > 1:\n",
    "                        samples = self._raw_data[:, id_list]\n",
    "                    else:\n",
    "                        samples = self._raw_data[id_list]\n",
    "\n",
    "                # put the samples in a DataFrame\n",
    "                samples = pd.DataFrame(data=np.transpose(samples),\n",
    "                                       index=np.arange(sample_size),\n",
    "                                       columns=self._dimension_tags)\n",
    "\n",
    "                samples = samples.astype(np.float64)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Either raw samples or a distribution needs to be defined \"\n",
    "                    \"to sample a random variable.\")\n",
    "\n",
    "        self._samples = samples\n",
    "\n",
    "        return self._samples\n",
    "\n",
    "    def orthotope_density(self, lower=None, upper=None):\n",
    "        \"\"\"\n",
    "        Estimate the probability density within an orthotope for a TMVN distr.\n",
    "\n",
    "        Use the mvn_orthotope_density function in this module for the\n",
    "        calculation. Pre-defined truncation limits for the RV are automatically\n",
    "        taken into consideration. Limits for lognormal distributions shall be\n",
    "        provided in linear space - the conversion is performed by the algorithm\n",
    "        automatically. Pre- and post-truncation correlation is also considered\n",
    "        automatically.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lower: float vector, optional, default: None\n",
    "            Lower bound(s) of the orthotope. A scalar value can be used for a\n",
    "            univariate RV; a list of bounds is expected in multivariate cases.\n",
    "            If the orthotope is not bounded from below in any dimension, use\n",
    "            either 'None' or assign an infinite value (i.e. -numpy.inf) to\n",
    "            that dimension.\n",
    "        upper: float vector, optional, default: None\n",
    "            Upper bound(s) of the orthotope. A scalar value can be used for a\n",
    "            univariate RV; a list of bounds is expected in multivariate cases.\n",
    "            If the orthotope is not bounded from above in any dimension, use\n",
    "            either 'None' or assign an infinite value (i.e. numpy.inf) to\n",
    "            that dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        alpha: float\n",
    "            Estimate of the probability density within the orthotope.\n",
    "        eps_alpha: float\n",
    "            Estimate of the error in alpha.\n",
    "\n",
    "        \"\"\"\n",
    "        # get the orthotope density within the truncation limits\n",
    "        if (self.tr_lower_pre is None) and (self.tr_upper_pre is None):\n",
    "            alpha_0 = 1.\n",
    "        else:\n",
    "            alpha_0, __ = mvn_orthotope_density(self.mu, self.COV,\n",
    "                                                self.tr_lower_pre, self.tr_upper_pre)\n",
    "\n",
    "        # merge the specified limits with the pre-defined truncation limits\n",
    "        lower, upper = self._convert_limits([lower, upper])\n",
    "        lower = self._move_to_log(lower, self._distribution_kind)\n",
    "        upper = self._move_to_log(upper, self._distribution_kind)\n",
    "\n",
    "        # if there are post-truncation correlations defined, transform the\n",
    "        # prescribed limits to 'pre' type limits\n",
    "        if self.tr_limits_post is not None:\n",
    "            lower_lim_post, upper_lim_post = (self.tr_lower_post,\n",
    "                                              self.tr_upper_post)\n",
    "            for dim in range(self._ndim):\n",
    "                if ((lower_lim_post[dim] < lower[dim])\n",
    "                     or (upper_lim_post[dim] > upper[dim])):\n",
    "                    mu =self.mu[dim]\n",
    "                    sig = np.sqrt(self.COV[dim, dim])\n",
    "                    lim_U = truncnorm.cdf([lower[dim], upper[dim]],\n",
    "                                          loc=mu, scale=sig,\n",
    "                                          a=(lower_lim_post[dim]-mu)/sig,\n",
    "                                          b=(upper_lim_post[dim]-mu)/sig)\n",
    "                    lim_pre = norm.ppf(lim_U, loc=mu, scale=sig)\n",
    "                    lower[dim], upper[dim] = lim_pre\n",
    "\n",
    "        if self.tr_limits_pre is not None:\n",
    "            lower_lim_pre, upper_lim_pre = (self.tr_lower_pre,\n",
    "                                            self.tr_upper_pre)\n",
    "            lower_lim_pre = np.maximum(lower_lim_pre, lower)\n",
    "            upper_lim_pre = np.minimum(upper_lim_pre, upper)\n",
    "        else:\n",
    "            lower_lim_pre = lower\n",
    "            upper_lim_pre = upper\n",
    "\n",
    "        # get the orthotope density within the prescribed limits\n",
    "        alpha, eps_alpha = mvn_orthotope_density(self.mu, self.COV,\n",
    "                                                 lower_lim_pre, upper_lim_pre)\n",
    "\n",
    "        # note that here we assume that the error in alpha_0 is negligible\n",
    "        return min(alpha / alpha_0, 1.), eps_alpha / alpha_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVariableSubset(object):\n",
    "    \"\"\"\n",
    "    Provides convenient access to a subset of components of a RandomVariable.\n",
    "\n",
    "    This object is useful when working with multivariate RVs, but it is used in\n",
    "    all cases to provide a general approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    RV: RandomVariable\n",
    "        The potentially multivariate random variable that is accessed through\n",
    "        this object.\n",
    "    tags: str or list of str\n",
    "        A string or list of strings that identify the subset of component we\n",
    "        are interested in. These strings shall be among the `dimension_tags` of\n",
    "        the RV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, RV, tags):\n",
    "\n",
    "        self._RV = RV\n",
    "        self._tags = tags\n",
    "\n",
    "    @property\n",
    "    def tags(self):\n",
    "        \"\"\"\n",
    "        Return the tags corresponding to the components in the RV subset.\n",
    "\n",
    "        \"\"\"\n",
    "        # this is very simple for now\n",
    "        return self._tags\n",
    "\n",
    "    @property\n",
    "    def samples(self):\n",
    "        \"\"\"\n",
    "        Return the pre-generated samples of the selected component from the\n",
    "        RV distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        samples = self._RV.samples\n",
    "\n",
    "        if samples is not None:\n",
    "            return samples[self._tags]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def sample_distribution(self, sample_size, preserve_order=False):\n",
    "        \"\"\"\n",
    "        Sample the probability distribution assigned to the connected RV.\n",
    "\n",
    "        Note that this function will sample the full, potentially multivariate,\n",
    "        distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample_size: int\n",
    "            Number of samples requested.\n",
    "        preserve_order: bool, default: False\n",
    "            Influences sampling from raw data. If True, the samples are copies\n",
    "            of the first n rows of the raw data where n is the sample_size. This\n",
    "            only works for sample_size <= raw data size. If False, the samples\n",
    "            are drawn from the raw data pool with replacement.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        samples: DataFrame\n",
    "            Samples of the selected component generated from the distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        samples = self._RV.sample_distribution(sample_size, preserve_order)\n",
    "\n",
    "        return samples[self._tags]\n",
    "\n",
    "    def orthotope_density(self, lower=None, upper=None):\n",
    "        \"\"\"\n",
    "        Return the density within the orthotope in the marginal pdf of the RVS.\n",
    "\n",
    "        The function considers the influence of every dependent variable in the\n",
    "        RV on the marginal pdf of the RVS. Note that such influence only occurs\n",
    "        when the RV is a truncated distribution and at least two variables are\n",
    "        dependent. Pre- and post-truncation correlation is considered\n",
    "        automatically.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lower: float vector, optional, default: None\n",
    "            Lower bound(s) of the orthotope. A scalar value can be used for a\n",
    "            univariate RVS; a list of bounds is expected in multivariate cases.\n",
    "            If the orthotope is not bounded from below in any dimension, use\n",
    "            either 'None' or assign an infinite value (i.e. -numpy.inf) to\n",
    "            that dimension.\n",
    "        upper: float vector, optional, default: None\n",
    "            Upper bound(s) of the orthotope. A scalar value can be used for a\n",
    "            univariate RVS; a list of bounds is expected in multivariate cases.\n",
    "            If the orthotope is not bounded from above in any dimension, use\n",
    "            either 'None' or assign an infinite value (i.e. numpy.inf) to\n",
    "            that dimension.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        alpha: float\n",
    "            Estimate of the probability density within the orthotope.\n",
    "        eps_alpha: float\n",
    "            Estimate of the error in alpha.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the dimension tags from the parent RV and find the ones that\n",
    "        # define this RVS\n",
    "        dtags = self._RV.dimension_tags\n",
    "        if dtags.size == 1:\n",
    "            dtags = [dtags]\n",
    "        sorter = np.argsort(dtags)\n",
    "        tag_ids = sorter[np.searchsorted(dtags, self._tags, sorter=sorter)]\n",
    "\n",
    "        # prepare the limit vectors and assign the limits to the appropriate\n",
    "        # dimensions\n",
    "        lower_full = [None for i in range(len(dtags))]\n",
    "        upper_full = deepcopy(lower_full)\n",
    "\n",
    "        if lower is not None:\n",
    "            lower_full = np.asarray(lower_full)\n",
    "            lower_full[tag_ids] = np.asarray(lower)\n",
    "            lower_full = lower_full.tolist()\n",
    "\n",
    "        if upper is not None:\n",
    "            upper_full = np.asarray(upper_full)\n",
    "            upper_full[tag_ids] = np.asarray(upper)\n",
    "            upper_full = upper_full.tolist()\n",
    "\n",
    "        # get the alpha value from the parent RV\n",
    "        return self._RV.orthotope_density(lower_full, upper_full)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
