# -----------------------------------------------------------
# Open-Source Seismic Risk Assessment, OpenSRA(TM)
#
# Copyright(c) 2020-2022 The Regents of the University of California and
# Slate Geotechnical Consultants. All Rights Reserved.
#
# Base classes used in OpenSRA
#
# Created: April 1, 2022
# @author: Barry Zheng (Slate Geotechnical Consultants)
# -----------------------------------------------------------


# -----------------------------------------------------------
# Python base modules
import os
import logging
import importlib
# import itertools

# data manipulation modules
import numpy as np
from numpy.testing import assert_array_equal, assert_allclose
import pandas as pd
from pandas import DataFrame
from scipy.interpolate import interp1d

# geospatial processing modules


# efficient processing modules
from numba import njit
from numba.core import types
from numba.typed import Dict

# plotting modules
# if importlib.util.find_spec('matplotlib') is not None:
    # import matplotlib.pyplot as plt
    # from matplotlib.collections import LineCollection
# if importlib.util.find_spec('contextily') is not None:
    # import contextily as ctx

# OpenSRA modules


# -----------------------------------------------------------
class GMPE(object):
    """
    Base class for GMPEs
    
    Parameters
    ----------
    
    Returns
    -------
        
    References
    ----------
    .. [1] Author, Year, Title, Publication, Issue, Pages.
    
    """
    
    # class definitions
    _NAME = None   # Name of the model
    _ABBREV = None                 # Abbreviated name of the model
    _REF = "".join([                 # Reference for the model
        'Authors, Year, ',
        'Title, ',
        'Publication, ',
        'Issue, Pages.'
    ])
    _MODEL_PBEE_CAT = 'IM'           # Return category in PBEE framework, e.g., IM, EDP, DM
    _MODEL_RETURN_RV = ['PGA', 'PGV', 'Sa_T'] # Return variable for PBEE category, e.g., pgdef, pipe_strain
    _MODEL_TYPE = 'seismic_intensity'               # Type of model (e.g., liquefaction, landslide)
    _MODEL_DIST = {                          # Distribution information for model
        'type': 'lognormal',
        'mean': None,
        'aleatory': {
            'tau': None,
            'phi': None,
            'sigma': None
        },
        'epistemic': None,
    }
    _REQ_PBEE_CAT = None     # Upstream PBEE variable required by model, e.g, IM, EDP, DM
    _REQ_PBEE_RV = None     # Randdom variable from upstream PBEE category required by model, e.g, pga, pgdef, pipe_strain
    
    # Model inputs
    _MODEL_INPUT = {}
    
    # Fault/source variables
    _MODEL_INPUT_SOURCE = {}
    
    # Distance/path variables
    _MODEL_INPUT_PATH = {}
    
    # Site variables
    _MODEL_INPUT_SITE = {}
    
    # TEMP
    _FIXED = {}
    _OUTPUT = []
    
    # Other backend parameters
    _GMPE_DIR = r'C:\Users\barry\OneDrive - SlateGeotech\CEC\OpenSRA\lib\NGAW2_Supplement_Data'
    _PGA_PERIOD = 0
    _PGV_PERIOD = -1
    
    
    # instantiation
    def __init__(self):
        """Create an instance of the class"""
        
        # initialize instance variables
        self._set_instance_var()
        
        # read coefficients
        self.coeffs = self._read_coeffs().copy()
        
        # initialize params
        self._inputs = {}
        self._calc_params = {}
        self._missing_req_inputs = []
        self._missing_opt_inputs = []
    
    
    def _set_instance_var(self):
        """Store class variables to instance"""
        class_var_to_set = [
            attr for attr in dir(self) \
                if attr.startswith("_") and \
                not attr.startswith("__") and \
                attr[1].isupper()
        ]
        for var in class_var_to_set:
            if isinstance(getattr(self, var),dict):
                setattr(self, var.lower()[1:], getattr(self, var).copy())
            else:
                setattr(self, var.lower()[1:], getattr(self, var))
    
    
    @classmethod
    def _read_coeffs(cls):
        """Read table of coefficients"""
        coeffs_path = os.path.join(cls._GMPE_DIR, f"{cls.__name__.lower()}.csv") # file
        coeffs = pd.read_csv(coeffs_path) # read from table
        coeffs = cls._get_constant_coeffs(coeffs) # get additional constant coefficients
        return coeffs
    
    
    @staticmethod
    def _get_constant_coeffs(coeffs):
        """Get constant coefficients"""
        return coeffs
        
    
    def print_inputs(self):
        """Print required inputs for the different parts to the GMPE"""
        print(self.model_input['desc'])
        req_params = self.model_input['required']
        opt_params = self.model_input['optional']
        print(f'\nRequired:')
        for i, param in enumerate(req_params):
            print(f'{i+1}) {param}: {req_params[param]["desc"]}')
            # print(f'\t- Default: {req_params[param]["default"]}')
            if req_params[param]['note'] is not None:
                print(f'\t-> Note: {req_params[param]["note"]}')
        print(f'\nOptional:')
        for i, param in enumerate(opt_params):
            print(f'{i+1}) {param}: {opt_params[param]["desc"]}')
            print(f'\t-> Default: {opt_params[param]["default"]}')
            if opt_params[param]['note'] is not None:
                print(f'\t-> Note: {opt_params[param]["note"]}')
        
    
    def set_inputs(self, kwargs):
        """Checks kwargs for fault parameters and stores in class"""
        # initalize lists for tracking missing variables
        missing_req_inputs = []
        missing_opt_inputs = []
        # required inputs
        req_params = self.model_input['required']
        opt_params = self.model_input['optional']
        # required inputs:
        for i, param in enumerate(req_params):
            self._inputs[param] = kwargs.get(param, None)
            if not param in kwargs:
                missing_req_inputs.append(param) # add to list of missing inputs to report
        # optional inputs:
        for i, param in enumerate(opt_params):
            self._inputs[param] = kwargs.get(param, opt_params[param]['default'])
            if not param in kwargs:
                missing_opt_inputs.append(param) # add to list of missing inputs to report
        # print missing inputs
        if len(missing_req_inputs) > 0:
            print(f'missing "{", ".join(missing_req_inputs)}" for required inputs')
        # if len(missing_opt_inputs) > 0:
        #     print(f'missing "{", ".join(missing_opt_inputs)}" for optional inputs; will use default values')
        # store missing inputs
        self._missing_req_inputs = missing_req_inputs
        self._missing_opt_inputs = missing_opt_inputs
        
        
    def _calc_dist(self):
        """Calculate distances: r_rup, r_jb, r_x"""
        # convert to UTM if input coordinates
        if self._inputs('crs') == 'EPSG:4326':
            utm_x, utm_y, zone, _ = wgs84_to_utm(lon, lat, include_north_south=False, force_zone_num=-99)
        pass
        
    
    @staticmethod
    def interp(x_source, y_source, x_out, scale='linear', kind='linear', fill_value=np.nan, bounds_error=False):
        """
        Interpolate source data at target x values
        Options: linear, semilogx, semilogy, loglog
        """
        if scale == 'linear':
            interp_func = interp1d(
                x_source, y_source,
                axis=0, kind=kind,
                fill_value=fill_value, bounds_error=bounds_error)
            return interp_func(x_out)
        elif scale == 'semilogx':
            interp_func = interp1d(
                np.log(x_source), y_source,
                axis=0, kind=kind,
                fill_value=fill_value, bounds_error=bounds_error)
            return interp_func(np.log(x_out))
        elif scale == 'semilogy':
            interp_func = interp1d(
                x_source, np.log(y_source),
                axis=0, kind=kind,
                fill_value=fill_value, bounds_error=bounds_error)
            return np.exp(interp_func(x_out))
        elif scale == 'loglog':
            interp_func = interp1d(
                np.log(x_source), np.log(y_source),
                axis=0, kind=kind,
                fill_value=fill_value, bounds_error=bounds_error)
            return np.exp(interp_func(np.log(x_out)))
        
        
    def get_coeff_for_period(self, period):
        """Get coeffients for a specific period"""
        # pull inputs locally
        coeffs = self.coeffs.copy()
        pga_period = self.pga_period
        pgv_period = self.pgv_period
        
        # action
        if isinstance(period,str):
            if period.lower() == 'pga':
                return coeffs.loc[coeffs.Period==pga_period,:].copy()
            elif period.lower() == 'pgv':
                return coeffs.loc[coeffs.Period==pgv_period,:].copy()
        else:
            if isinstance(period,list):
                ind = [np.where(coeffs.Period==val)[0][0] for val in period if len(np.where(coeffs.Period==val)[0])>0]
                return coeffs.loc[ind].copy()
            else:
                return coeffs.loc[coeffs.Period==period,:].copy()
        
    
    def get_im(self, period):
        """Get IMs for a specific period(s)"""
        pass
    
    
    @classmethod
    def _convert_periods_to_numerics(cls, periods):
        """Convert periods to an array of numerics"""
        period_num = []
        # if string, then either PGA or PGV
        if isinstance(periods,str):
            if periods.lower() == 'pga':
                period_num.append(cls._PGA_PERIOD) # replace with period proxy
            elif periods.lower() == 'pgv':
                period_num.append(cls._PGV_PERIOD) # replace with period proxy
            else:
                raise ValueError(f'Acceptable string inputs: "PGA", "PGV"')
        # if single value
        elif isinstance(periods,float) or isinstance(periods,int):
            period_num.append(periods)
        else:
            # lists or arrays
            for val in periods:
                if isinstance(val,str):
                    if val.lower() == 'pga':
                        period_num.append(cls._PGA_PERIOD) # replace with period proxy
                    elif val.lower() == 'pgv':
                        period_num.append(cls._PGV_PERIOD) # replace with period proxy
                else:
                    period_num.append(val)
        return np.asarray(period_num)
    
    
    @classmethod
    def _reduce_coeffs_to_required_periods(cls, coeffs, period_out):
        """Convert period_out to array of pure numerics and reduce coeff matrix to only periods needed"""
        # convert period_out to array of pure numerics
        period_out_num = cls._convert_periods_to_numerics(period_out).astype(float)
        
        # see if periods in period_out all coincide with periods in coeffs matrix
        has_all_period_out = True
        coeff_period_array = coeffs.Period.to_numpy()
        ind_to_get = []
        for per in period_out_num:
            if per in coeff_period_array:
                ind_to_get.append(np.where(coeff_period_array==per)[0][0])
            else:
                has_all_period_out = False
                break
        
        # if coeffs contain all periods in period_out, then just get those rows
        if has_all_period_out:
            coeffs = coeffs.loc[ind_to_get].reset_index(drop=True)

        else:
            # return periods not pga, pgv
            not_pga_pgv_ind = np.where(np.logical_and(period_out_num!=cls._PGA_PERIOD,period_out_num!=cls._PGV_PERIOD))
            period_out_num_no_pga_pgv = period_out_num[not_pga_pgv_ind]
            
            # check if target periods are outside the range for coefficients
            coeff_period_not_pga_pgv = coeffs.Period[np.where(np.logical_and(coeffs.Period!=cls._PGA_PERIOD,coeffs.Period!=cls._PGV_PERIOD))[0]]
            coeff_period_min = min(coeff_period_not_pga_pgv)
            coeff_period_max = max(coeff_period_not_pga_pgv)
            if min(period_out_num_no_pga_pgv) < coeff_period_min or max(period_out_num_no_pga_pgv) > coeff_period_max:
                raise ValueError(
                    f"Target periods cannot be outside the range of {coeff_period_min} and {coeff_period_max} for {cls.__name__}!" + \
                    f"; except for T_PGA={cls._PGA_PERIOD} and T_PGV={cls._PGV_PERIOD}")
            
            # remove coeffs outside limits of period_out_num
            if len(period_out_num_no_pga_pgv) > 0:
                ind_under_period_out_min = np.where(coeffs.Period<min(period_out_num_no_pga_pgv))[0]
                ind_above_period_out_max = np.where(coeffs.Period>max(period_out_num_no_pga_pgv))[0]
                # lower_bound
                if len(ind_under_period_out_min) > 0:
                    one_ind_up_from_min = min(ind_under_period_out_min[-1]+1,len(coeffs.Period)-1)
                    if coeffs.Period[one_ind_up_from_min] == min(period_out_num_no_pga_pgv):
                        ind_lower_cutoff = one_ind_up_from_min
                    else:
                        ind_lower_cutoff = one_ind_up_from_min-1
                else:
                    ind_lower_cutoff = 0
                # upper_bound
                if len(ind_above_period_out_max)>0:
                    one_ind_down_from_max = max(ind_above_period_out_max[0]-1,0)
                    if coeffs.Period[one_ind_down_from_max] == max(period_out_num_no_pga_pgv):
                        ind_upper_cutoff = one_ind_down_from_max
                    else:
                        ind_upper_cutoff = one_ind_down_from_max+1
                else:
                    ind_upper_cutoff = coeffs.shape[0]-1
                # get periods
                coeffs = pd.concat([
                    coeffs.loc[np.where(np.logical_or(coeffs.Period==cls._PGA_PERIOD,coeffs.Period==cls._PGV_PERIOD))[0]],
                    coeffs.loc[ind_lower_cutoff:ind_upper_cutoff],
                ],axis=0).reset_index(drop=True)
            else:
                coeffs = coeffs.loc[np.where(np.logical_or(coeffs.Period==cls._PGA_PERIOD,coeffs.Period==cls._PGV_PERIOD))[0]].reset_index(drop=True)
            # check to keep PGA and PGV coeffs
            if not cls._PGA_PERIOD in period_out_num:
                coeffs = coeffs.drop(np.where(coeffs.Period==cls._PGA_PERIOD)[0]).reset_index(drop=True)
            if not cls._PGV_PERIOD in period_out_num:
                coeffs = coeffs.drop(np.where(coeffs.Period==cls._PGV_PERIOD)[0]).reset_index(drop=True)
        #
        return coeffs, period_out_num
    
    
    @classmethod
    def _interp_output_at_period_out(cls, output, coeffs_period):
        """interpolate spectra at target periods"""
        period_out = output['period']
        # see if periods = 0 (PGA) and -1 (PGV) are in source and target periods
        periods_pga_ind = np.where(coeffs_period==cls._PGA_PERIOD)[0]
        periods_pgv_ind = np.where(coeffs_period==cls._PGV_PERIOD)[0]
        period_out_pga_ind = np.where(period_out==cls._PGA_PERIOD)[0]
        period_out_pgv_ind = np.where(period_out==cls._PGV_PERIOD)[0]
        # get indices of periods that are not PGA, PGV
        periods_not_pga_pgv = np.where(period_out>0)[0]
        # go through output items
        for item in output:
            if item != 'dist' and item != 'period' and item != 'dims' and item != 'dims_desc':
                # initialize interp array (n_period_out x n_site)
                interp_vals = np.zeros(shape=(period_out.shape[0],output['dims'][1]))
                # first get values <= 0 (PGA, PGV):
                if len(period_out_pga_ind) > 0:
                    interp_vals[period_out_pga_ind[0]] = output[item][periods_pga_ind[0]]
                if len(period_out_pgv_ind) > 0:
                    interp_vals[period_out_pgv_ind[0]] = output[item][periods_pgv_ind[0]]
                # perform interpolation on rest of target periods
                if len(periods_not_pga_pgv) > 0:
                    if item == 'mean':
                        # for mean, just interpolate with semilogx
                        interp_vals[periods_not_pga_pgv] = \
                            cls.interp(
                                x_source=coeffs_period[coeffs_period>0],
                                y_source=output['mean'][coeffs_period>0],
                                x_out=period_out[periods_not_pga_pgv],
                                scale='semilogx',
                            )
                    else:
                        # for sigmas, square, interpolate, then sqrt
                        interp_vals[periods_not_pga_pgv] = np.sqrt(cls.interp(
                            x_source=coeffs_period[coeffs_period>0],
                            y_source=(output[item][coeffs_period>0])**2,
                            x_out=period_out[periods_not_pga_pgv],
                            scale='semilogx',
                        ))
                # update model_output dictionary
                output[item] = interp_vals
        return output
    

    @staticmethod
    def _convert_to_ndarray(arr, length=1):
        """Convert to array; if float/int, then add dimension to convert to array"""
        if isinstance(arr, np.ndarray):
            return arr
        elif isinstance(arr, list):
            return np.asarray(arr)
        else:
            return np.asarray([arr]*length)
        # return arr if isinstance(arr, np.ndarray) else np.asarray([arr]*length)
    
    
    @staticmethod
    def _get_coeff(coeffs,name,n=1):
        """Return coeff values for ID=name and repeat by n"""
        if n == 1:
            # return coeffs[name]
            return np.expand_dims(coeffs[name],axis=1)
        else:
            return np.repeat(np.expand_dims(coeffs[name],axis=1),repeats=n,axis=1)
    
    
    @classmethod
    def run_check(cls, rtol=1e-7, atol=0):
        """Check result from function against known results"""
        #-------------------------------------------
        # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        test_file_dir = r'C:\Users\barry\OneDrive - SlateGeotech\CEC\Structure\test\gmpe' # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        #-------------------------------------------
        test_file = os.path.join(test_file_dir, f"{cls.__name__.lower()}_check.csv")
        # read test file
        test_data = pd.read_csv(test_file)
        # get inputs
        # note - NGAW2 spreadsheet gives ground motions for 23 periods (including PGA, PGV)
        n_period = 23
        # last 23 columns are sigmas
        test_sigma = test_data.iloc[:,-n_period:].copy()
        # next 23 columns from last are median values
        test_median = test_data.iloc[:,-2*n_period:-n_period].copy()
        # rest are input values
        test_inputs = test_data.iloc[:,:-2*n_period].copy() #
        # set sigma columns to median columns (pandas appends numbers to repeating column names)
        test_sigma.columns = test_median.columns
        # get periods from column names
        test_periods = np.asarray(test_median.columns)
        test_periods[test_periods=='pga'] = cls._PGA_PERIOD # replace PGA text with period value
        test_periods[test_periods=='pgv'] = cls._PGV_PERIOD # replace PGA text with period value
        test_periods = pd.to_numeric(test_periods) # convert to numeric

        # read coeffcients
        coeffs = cls._read_coeffs().copy()
        # run each case and assert allclose
        for i in range(test_inputs.shape[0]):
            kwargs = {} # put inputs into dictionary
            for col in test_inputs.columns:
                kwargs[col] = test_inputs[col][i]
            kwargs['coeffs'] = coeffs
            kwargs['period_out'] = test_periods
            result = cls.model(**kwargs)
            # check relative error on mean
            assert_allclose(
                result['mean'][:,0], # calculation
                np.log(test_median.loc[i].values), # true
                rtol=rtol, atol=atol, equal_nan=True,
            )
        # print message if able to check through all cases
        print(f"Tests pass for {cls.__name__}")
        
       
# ----------------------------------------------------------- 
class CY14(GMPE):
    """
    Chiou and Youngs (2014)
    
    Parameters
    ----------
    
    Returns
    -------
        
    References
    ----------
    .. [1] Chiou, B.S.-J., and Youngs, R.R., 2014, Update of the Chiou and Youngs NGA model
    for the average horizontal component of peak ground motion and response spectra, Earthquake Spectra, vol. 30, no. 3, pp. 1117-1153.
    
    """
    
    # class definitions
    _NAME = 'Chiou and Youngs (2022)'   # Name of the model
    _ABBREV = 'CY14'                 # Abbreviated name of the model
    _REF = "".join([                 # Reference for the model
        'Chiou, B.S.-J., and Youngs, R.R., 2014, ',
        'Update of the Chiou and Youngs NGA model for the average horizontal component of peak ground motion and response spectra, ',
        'Earthquake Spectra, ',
        'vol. 30, no. 3, pp. 1117-1153.'
    ])
    _MODEL_PBEE_CAT = 'IM'           # Return category in PBEE framework, e.g., IM, EDP, DM
    _MODEL_RETURN_RV = ['PGA', 'PGV', 'Sa_T'] # Return variable for PBEE category, e.g., pgdef, pipe_strain
    _MODEL_TYPE = 'seismic_intensity'               # Type of model (e.g., liquefaction, landslide)
    _MODEL_DIST = {                          # Distribution information for model
        'type': 'lognormal',
        'aleatory': {
            'phi': None,
            'tau': None,
            'sigma': None
        },
        'epistemic': None,
        'x_val': None,
        'dims': None,
        'dims_desc': None
    }
    
    # Model inputs
    _MODEL_INPUT = {
        "desc": f'Inputs for model:\n'+
                f'Note: for fault toggles, provide both "f_nm" and "f_rv" directly or either "fault_type", or "rake"',
        'required': {
            # 'lon': {
            #     'desc': 'Longitudes (deg)',
            #     'default': None, 'unit': 'degree', 'note': None},
            # 'lat': {
            #     'desc': 'Latitudes (deg)',
            #     'default': None, 'unit': 'degree', 'note': None},
            'vs30': {
                'desc': 'Vs30 (m/s)',
                'default': None, 'unit': 'm/s', 'note': None},
            'vs30_source': {
                'desc': 'Source for Vs30 (inferred or measured)',
                'default': None, 'unit': None, 'note': None},
            'z1p0': {
                'desc': 'Depth to Vs = 1 km/s (km)',
                'default': None, 'unit': 'km', 'note': None},
            'mag': {
                'desc': 'Moment magnitude',
                'default': None, 'unit': None, 'note': None},
            'dip': {
                'desc': 'Dip angle (deg)',
                'default': None, 'unit': 'degree', 'note': None},
            'z_tor': {
                'desc': 'Depth to top of rupture (km)',
                'default': None, 'unit': 'km', 'note': None},
            'r_rup': {
                'desc': 'Closest distance to coseismic rupture (km)',
                'default': None, 'unit': 'km', 'note': None},
            'r_jb': {
                'desc': 'Closest distance to surface projection of coseismic rupture (km)',
                'default': None, 'unit': 'km', 'note': None},
            'r_x': {
                'desc': 'Horizontal distance from top of rupture measured perpendicular to fault strike (km)',
                'default': None, 'unit': 'km', 'note': None},
            'f_hw': {
                'desc': 'Hanging wall toggle (1: True, 0: False)',
                'default': None, 'unit': 'km', 'note': None},
        },
        "optional": {
            'f_region': {
                'desc': 'Region to analyze\n'+
                        '\t0: Global\n'+
                        '\t1: Japan/Italy\n'+
                        '\t2: Wenchuan (only applicable for M7.9 event)',
                'default': 0, 'unit': None, 'note': None},
            'fault_type': {
                'desc': 'Fault type\n'+
                        '\t-1:      Normal\n'+
                        '\t 0,-0.5: Strike-Slip and Nml\Obl\n'+
                        '\t 1, 0.5: Reverse and Rev/Obl',
                'default': 0, 'unit': None,
                # 'note': 'provide "fault_type" directly or use "rake" to infer "fault_type"'},
                'note': None},
            'rake': {
                'desc': 'Rake angle (deg)\n'+
                        '\t-120 < rake < -60: Normal\n'+
                        '\t  30 < rake < 150: Strike-Slip and Nml\Obl\n'+
                        '\t    otherwise:     Reverse and Rev/Obl',
                'default': None, 'unit': 'degree',
                # 'note': 'not needed if "fault_type" is provided'},
                'note': None},
            'f_nm': {
                'desc': 'Normal fault toggle (1: True, 0: False)',
                'default': None, 'unit': None, 'note': None},
            'f_rv': {
                'desc': 'Reverse fault toggle (1: True, 0: False)',
                'default': None, 'unit': None, 'note': None},
            'period_out': {
                'desc': 'Periods to return (list or single str, int, float), note:\n'+
                        '\tPGA: provide "PGA", "pga", or "0"\n'+
                        '\tPGV: provide "PGV", "pgv", or "-1"',
                'default': [
                    0, -1, 0.01, 0.02, 0.03, 0.05, 0.075, 0.1, 0.15, 0.2,
                    0.25, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5, 7.5, 10
                ], 'unit': None, 'note': None},
            'delta_dpp': {
                'desc': 'Directivity term, direct point parameter (use 0 for median predictions)',
                'default': 0, 'unit': None, 'note': None},
            'crs': {
                'desc': 'Input coordinate reference system, crs (WGS84=EPSG:4326)',
                'default': 'EPSG:4326', 'unit': None, 'note': None},
            'utm_zone_num': {
                'desc': 'Zone number to use for UTM conversion (CA=10)',
                'default': 10, 'unit': None, 'note': None},
        }, 
        "other_notes": "For fault type, either provide:"
    }
    
    # Constant parameters/toggles
    # _MODEL_INPUT_CONST = {}  # Constant parameters/toggles
    
    
    # instantiation
    def __init__(self):
        """Create an instance of the class"""
        super().__init__()
        self.site_inputs = {}
        
        
    @staticmethod
    def _get_constant_coeffs(coeffs):
        """Get constant coefficients"""
        return coeffs
    
    
    def perform_calc(self):
        """Perform calculation"""
        
        # pull inputs locally
        coeffs = self.coeffs.copy()
        # from inputs
        # source
        mag = self._inputs['mag']
        z1p0 = self._inputs['z1p0']
        dip = self._inputs['dip']
        z_tor = self._inputs['z_tor']
        fault_type = self._inputs['fault_type']
        rake = self._inputs['rake']
        f_nm = self._inputs['f_nm']
        f_rv = self._inputs['f_rv']
        f_region = self._inputs['f_region']
        # path
        delta_dpp = self._inputs['delta_dpp']
        r_rup = self._inputs['r_rup']
        r_jb = self._inputs['r_jb']
        r_x = self._inputs['r_x']
        f_hw = self._inputs['f_hw']
        # site
        vs30 = self._inputs['vs30']
        vs30_source = self._inputs['vs30_source']
        # others
        period_out = self._inputs['period_out']
        
        #-------------------------------------------
        # determine fault toggles
        if f_nm is None or f_rv is None:
            # first see if fault_type is given and determine from rake if not provided
            if fault_type is None:
                if rake is None:
                    raise ValueError('Not enough inputs to determine fault toggles:\n'+
                                     '\tNeed "fault_type", "rake", or both "f_nm" and "f_rv"\n'+
                                     '\tDefault to fault_type = 0 (strike-slip))')
                    fault_type = self.model_input_source['optional']['fault_type']['default'] # default = strike-slip
                else:
                    if rake >= 30 and rake <= 150:
                        fault_type = 1
                    elif rake >= -120 and rake <= -60:
                        fault_type = -1
                    else:
                        fault_type = 0
                # store into class instance
                self._inputs['fault_type'] = fault_type
            
            #-------------------------------------------
            # get fault type toggles
            if fault_type == -1: # normal
                f_rv = 0
                f_nm = 1
            elif fault_type >= 0.5: # reverse and reverse/oblique
                f_rv = 1
                f_nm = 0
            else: # strike-slip and normal/oblique
                f_rv = 0
                f_nm = 0
            # store into class instance
            self._calc_params['f_rv'] = f_rv
            self._calc_params['f_nm'] = f_nm
        
        #-------------------------------------------
        # set z1p0 to mean from model if z1p0 is not given
        if z1p0 is None:
            # Japan only, eq. 2
            if f_region == 1:
                numer = vs30**2 + 412**2
                denom = 1360**2 + 412**2
                z1p0 = np.exp(-5.23/2 * np.log(numer/denom)) # m
            # California and non-Japan regions - eq. 1
            else:
                numer = vs30**4 + 571**4
                denom = 1360**2 + 571**4
                z1p0 = np.exp(-7.15/4 * np.log(numer/denom)) # m
            # store into class instance
            self._inputs['z1p0'] = z1p0/1000
        
        #-------------------------------------------
        # set z_tor to mean from model if z_tor is not given
        if z_tor is None:
            if f_rv == 1: # reverse and reverse/oblique
                z_tor = np.maximum(2.704-1.226*np.maximum(mag-5.849,0),0)**2 # eq. 4
            else: # normal and strike-slip
                z_tor = np.maximum(2.673-1.136*np.maximum(mag-4.970,0),0)**2 # eq. 5
            # store into class instance
            self._inputs['z_tor'] = z_tor
        
        #-------------------------------------------
        # run calculation
        output = \
            self.model(
                # model coefficients
                coeffs,
                # source/fault params
                mag, dip, z_tor, f_rv, f_nm, f_region, 
                # path/dist params
                r_rup, r_jb, r_x, delta_dpp, 
                # site params
                vs30, vs30_source, z1p0, f_hw, 
                # others
                period_out
            )
        
        #-------------------------------------------
        # store into class instance
        self.model_dist['dims'] = output['dims']
        self.model_dist['dims_desc'] = output['dims_desc']
        self.model_dist['x_val'] = output['period']
        self.model_dist['mean'] = output['mean']
        self.model_dist['aleatory']['tau'] = output['tau']
        self.model_dist['aleatory']['phi'] = output['phi']
        self.model_dist['aleatory']['sigma'] = output['sigma']
    
    
    @classmethod
    def model(cls, 
        # model coefficients
        coeffs,
        # source/fault params
        mag, dip, z_tor, f_rv, f_nm, f_region, 
        # path/dist params
        r_rup, r_jb, r_x, delta_dpp, 
        # site params
        vs30, vs30_source, z1p0, f_hw, 
        # others
        period_out=[
            0, -1, 0.01, 0.02, 0.03, 0.05, 0.075, 0.1, 0.15, 0.2,
            0.25, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5, 7.5, 10],
        # r_max=200 #km
        ):
        """Model"""
        
        #-------------------------------------------
        # convert period_out to array of pure numerics and reduce coeff matrix to only periods needed
        coeffs, period_out_num = cls._reduce_coeffs_to_required_periods(coeffs, period_out)
            
        #-------------------------------------------
        # convert params to arrays
        # if float/int, add dimension and convert to array
        # site
        vs30 = cls._convert_to_ndarray(vs30).astype(float)
        vs30_source = cls._convert_to_ndarray(vs30_source).astype(float)
        z1p0 = cls._convert_to_ndarray(z1p0).astype(float)
        f_hw = cls._convert_to_ndarray(f_hw).astype(float)
        # # path
        r_rup = cls._convert_to_ndarray(r_rup).astype(float)
        r_jb = cls._convert_to_ndarray(r_jb).astype(float)
        r_x = cls._convert_to_ndarray(r_x).astype(float)
        
        #-------------------------------------------
        # expand dim site_params to match output dimensions
        # site
        vs30 = np.expand_dims(vs30,axis=0)
        vs30_source = np.expand_dims(vs30_source,axis=0)
        z1p0 = np.expand_dims(z1p0,axis=0)
        f_hw = np.expand_dims(f_hw,axis=0)
        # path
        r_rup = np.expand_dims(r_rup,axis=0)
        r_jb = np.expand_dims(r_jb,axis=0)
        r_x = np.expand_dims(r_x,axis=0)
        
        #-------------------------------------------
        # dimension for outputs
        # n_period x n_site
        n_period = coeffs.shape[0]
        n_site = r_rup.shape[1]
        
        #-------------------------------------------
        # prepare for numba operation
        # convert coeffs to numba type dictionary
        coeffs_nb = Dict.empty(
            key_type=types.unicode_type,
            value_type=types.float64[:,:],
        )
        non_jp_coeffs = ['phi1','phi5','phi6','sigma2']
        jp_coeffs = [c+'Jp' for c in non_jp_coeffs]
        # add dictionary entires
        for col in coeffs.columns:
            if col in non_jp_coeffs or col in jp_coeffs:
                pass
            else:
                coeffs_nb[col] = np.asarray(cls._get_coeff(coeffs,col,n_site),dtype=float)
        # Japan only
        if f_region == 1:
            coeffs_nb['phi1'] = np.asarray(cls._get_coeff(coeffs,'phi1Jp',n_site),dtype=float)
            coeffs_nb['phi5'] = np.asarray(cls._get_coeff(coeffs,'phi5Jp',n_site),dtype=float)
            coeffs_nb['phi6'] = np.asarray(cls._get_coeff(coeffs,'phi6Jp',n_site),dtype=float)
            coeffs_nb['sigma2'] = np.asarray(cls._get_coeff(coeffs,'sigma2Jp',n_site),dtype=float)
        # Other sites
        else:
            coeffs_nb['phi1'] = np.asarray(cls._get_coeff(coeffs,'phi1',n_site),dtype=float)
            coeffs_nb['phi5'] = np.asarray(cls._get_coeff(coeffs,'phi5',n_site),dtype=float)
            coeffs_nb['phi6'] = np.asarray(cls._get_coeff(coeffs,'phi6',n_site),dtype=float)
            coeffs_nb['sigma2'] = np.asarray(cls._get_coeff(coeffs,'sigma2',n_site),dtype=float)        
        
        #-------------------------------------------
        # run model
        mean, phi, tau, sigma = \
            cls._model(
                # model coefficients
                coeffs_nb,
                # source/fault params
                mag, dip, z_tor, f_rv, f_nm, f_region, 
                # path/dist params
                r_rup, r_jb, r_x, delta_dpp, 
                # site params
                vs30, vs30_source, z1p0, f_hw, 
            )
                            
        #-------------------------------------------
        # setup output dictionary
        # period_out = np.asarray(period_out) # convert to NumPy array
        output = {
            'mean': mean,
            'phi': phi,
            'tau': tau,
            'sigma': sigma,
            'dist': cls._MODEL_DIST['type'],
            # 'period': period_out,
            'period': period_out_num,
            'dims': mean.shape,
            'dims_desc': 'n_period x n_site'
        }
        
        #-------------------------------------------
        # interpolate spectra at target periods
        output = cls._interp_output_at_period_out(output, coeffs.Period.copy())
        
        #-------------------------------------------
        # return
        return output
    
    
    @staticmethod
    @njit
    def _model(
        # model coefficients
        coeffs,
        # source/fault params
        mag, dip, z_tor, f_rv, f_nm, f_region, 
        # path/dist params
        r_rup, r_jb, r_x, delta_dpp, 
        # site params
        vs30, vs30_source, z1p0, f_hw, 
        # others
        ):
        
        #-------------------------------------------
        # precompute some terms
        cos_dip = np.cos(np.radians(dip))
        
        #-------------------------------------------
        # dimensions
        n_period = len(coeffs)
        n_site = r_rup.shape[1]
        
        #-------------------------------------------
        # preset output matrices
        ln_y = np.zeros(shape=(n_period,n_site))
        tau = np.zeros(shape=(n_period,n_site))
        phi = np.zeros(shape=(n_period,n_site))
        
        #-------------------------------------------
        # calculate expected value for z1.0
        # Japan only, eq. 2
        if f_region == 1:
            numer = vs30**2 + 412**2
            denom = 1360**2 + 412**2
            z1p0_mean = np.exp(-5.23/2 * np.log(numer/denom)) # m
        # California and non-Japan regions - eq. 1
        else:
            numer = vs30**4 + 571**4
            denom = 1360**2 + 571**4
            z1p0_mean = np.exp(-7.15/4 * np.log(numer/denom)) # m
            
        #-------------------------------------------
        # term: fault type - eq. 11
        # reverse
        term_fault_rv = coeffs['c1a'] + coeffs['c1c']/np.cosh(2*np.maximum(mag-4.5,0))
        term_fault_rv = term_fault_rv*f_rv
        # normal
        term_fault_nm = coeffs['c1b'] + coeffs['c1d']/np.cosh(2*np.maximum(mag-4.5,0))
        term_fault_nm = term_fault_nm*f_nm

        #-------------------------------------------
        # term: top of rupture - eq. 11
        # mean z_tor from model
        if f_rv == 1: # reverse and reverse/oblique
            z_tor_mean = np.maximum(2.704-1.226*np.maximum(mag-5.849,0),0)**2 # eq. 4
        else: # normal and strike-slip
            z_tor_mean = np.maximum(2.673-1.136*np.maximum(mag-4.970,0),0)**2 # eq. 5
        # delta z_tor
        delta_z_tor = z_tor - z_tor_mean
        # term for model
        term_z_tor = coeffs['c7'] + coeffs['c7b']/np.cosh(2*np.maximum(mag-4.5,0))
        term_z_tor = term_z_tor*delta_z_tor
        
        #-------------------------------------------
        # term: dip - eq. 11
        term_dip = coeffs['c11'] + coeffs['c11b']/np.cosh(2*np.maximum(mag-4.5,0))
        term_dip = term_dip * cos_dip**2
        
        #-------------------------------------------
        # term: magnitude scaling - eq. 11
        term_mag_1 = coeffs['c1']
        term_mag_2 = coeffs['c2']*(mag-6)
        term_mag_3 = (coeffs['c2']-coeffs['c3'])/coeffs['cn']*np.log(1 + np.exp(coeffs['cn']*(coeffs['cM']-mag)))
        term_mag = term_mag_1 + term_mag_2 + term_mag_3
        
        #-------------------------------------------
        # term: near-field magnitude and distance scaling - eq. 11
        cc = coeffs['c5'] * np.cosh(coeffs['c6']*np.maximum(mag-coeffs['cHM'],0))
        term_near_field = coeffs['c4'] * np.log(r_rup + cc)
        
        #-------------------------------------------
        # term: large distance scaling - eq. 11
        gamma = coeffs['gamma1'] + coeffs['gamma2']/np.cosh(np.maximum(mag-coeffs['gm'],0))
        # correct for Japan and Wenchuan
        if f_region == 1: # Japan
            if mag > 6 and mag < 6.9: # correct for this range of magnitudes only
                gamma = gamma * coeffs['gscaleJp']
        elif f_region == 2: # Wenchuan, single event
            gamma = gamma * coeffs['gscaleWen']
        term_large_dist_1 = (coeffs['c4a'] - coeffs['c4']) * np.log(np.sqrt(r_rup**2 + coeffs['cRB']**2))
        term_large_dist_2 = gamma*r_rup
        term_large_dist = term_large_dist_1 + term_large_dist_2
        
        #-------------------------------------------
        # term: directivity - eq. 11
        term_direct_1 = coeffs['c8']*np.maximum(1-np.maximum(r_rup-40,0)/30,0)
        term_direct_2 = \
            np.minimum(np.maximum(mag-5.5,0)/0.8,1) * \
            np.exp(-coeffs['c8a']*(mag-coeffs['c8b'])**2)*delta_dpp
        term_direct = term_direct_1 * term_direct_2
        
        #-------------------------------------------
        # term: hanging wall - eq. 11
        term_hw = \
            coeffs['c9']*f_hw*cos_dip * \
            (coeffs['c9a']+(1-coeffs['c9a'])*np.tanh(r_x/coeffs['c9b'])) * \
            (1-np.sqrt(r_jb**2+z_tor**2)/(r_rup+1))
        
        #-------------------------------------------
        # population mean for Vs30 = 1130 m/s (Class B: rock) - eq. 11
        ln_y_ref = \
            term_fault_rv + term_fault_nm + term_z_tor + term_dip + \
            term_mag + term_near_field + term_large_dist + term_direct + term_hw
        
        #-------------------------------------------
        # term: site amplification, linear - eq. 12
        term_amp_lin = coeffs['phi1'] * np.minimum(np.log(vs30/1130) ,0)
        
        #-------------------------------------------
        # term: site amplification, nonlinear - eq. 12
        term_amp_nonlin = \
            coeffs['phi2'] * \
            (np.exp(coeffs['phi3']*(np.minimum(vs30,1130)-360)) - np.exp(coeffs['phi3']*(1130-360))) * \
            np.log((np.exp(ln_y_ref) + coeffs['phi4'])/coeffs['phi4']) # NOTE: scaling with between-event residual

        #-------------------------------------------
        # term: bedrock depth - eq. 12
        delta_z1p0 = z1p0*1000 - z1p0_mean # m
        term_z1p0 = coeffs['phi5']*(1-np.exp(-delta_z1p0/coeffs['phi6']))
        
        #-------------------------------------------
        # site-specific mean (site Vs30 dependent) - eq. 12
        ln_y = ln_y_ref + term_amp_lin + term_amp_nonlin + term_z1p0
        
        #-------------------------------------------
        # aleatory variability - eq. 13
        # inferred vs measured vs30
        f_inferred = np.zeros(vs30_source.shape)
        f_measured = np.zeros(vs30_source.shape)
        f_inferred[np.where(vs30_source==0)[0]] = 1
        f_measured[np.where(vs30_source==1)[0]] = 1
        # nonlinear adjustment
        nl0 = \
            coeffs['phi2'] * \
            (np.exp(coeffs['phi3']*(np.minimum(vs30,1130)-360)) - np.exp(coeffs['phi3']*(1130-360))) * \
            np.exp(ln_y_ref)/(np.exp(ln_y_ref) + coeffs['phi4'])
        # within-event phi
        phi = \
            (coeffs['sigma1'] + (coeffs['sigma2']-coeffs['sigma1'])/1.5*(np.minimum(np.maximum(mag,5),6.5)-5)) * \
            np.sqrt(coeffs['sigma3']*f_inferred + 0.7*f_measured + (1+nl0)**2)
        # between-event tau
        tau = \
            (coeffs['tau1'] + (coeffs['tau2']-coeffs['tau1'])/1.5*(np.minimum(np.maximum(mag,5),6.5)-5))
        tau = tau*(1+nl0) # correct tau with nonlinear term
        # total aleatory
        sigma = np.sqrt(phi**2 + tau**2)

        # return
        return ln_y, phi, tau, sigma