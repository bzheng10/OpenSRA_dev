{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required packages\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os, time\n",
    "import importlib\n",
    "import im.fcn_im as fcn_im\n",
    "import pandas as pd\n",
    "# from numpy.random import standard_normal\n",
    "# from scipy.linalg import cholesky\n",
    "import model, fcn_gen, os, time, warnings\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1977"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base inputs\n",
    "site_loc_file = r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\CA_pipe_midpt_allsites.txt'\n",
    "rate_min = 1/10000\n",
    "im_tool = 'sha'\n",
    "\n",
    "im_dir = r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\im_output_10000yr_allsites'\n",
    "dm_dir = r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\rr_out'\n",
    "rup_meta_file = r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\rup_meta_10000yr_allsites.hdf5'\n",
    "groups2run = np.loadtxt(r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\groups2run_allsites.txt',dtype=str)\n",
    "num_rups = 2\n",
    "im_list = ['pga','pgv']\n",
    "multi_max = int(np.ceil(len(groups2run)/100))\n",
    "\n",
    "## define number of realizations and generate samples\n",
    "nsamp=10\n",
    "flag_corr_d = False\n",
    "flag_corr_T = False\n",
    "\n",
    "len(groups2run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload model.py if it has been modified\n",
    "importlib.reload(model)\n",
    "## create class object\n",
    "A = model.assessment()\n",
    "##\n",
    "inc1 = 5\n",
    "inc2 = 100\n",
    "# out = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read inputs for Zhu et al. (2017)\n",
    "zhu_inputs = pd.read_csv(r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\CA_Zhu_pipe_midpt_allsites_withWB_v2_remap.csv')\n",
    "wtd = zhu_inputs['WTD_30m (m)'].values\n",
    "dr = zhu_inputs['DistRivers (km)'].values\n",
    "dc = zhu_inputs['DistCoast (km)'].values\n",
    "dw = zhu_inputs['DistAnyWaterNoWB (km)'].values\n",
    "precip = zhu_inputs['CA_Precip (mm)'].values\n",
    "vs30 = zhu_inputs['vs30_opensha (m/s)'].values\n",
    "dwWB = zhu_inputs['DistAnyWaterWB (km)'].values\n",
    "ky = zhu_inputs['ky_inf_bray'].values\n",
    "\n",
    "## change all -9999 entries to NaN\n",
    "find_val = -9999\n",
    "set_val = np.nan\n",
    "wtd = fcn_gen.find_set_nan(wtd,find_val,set_val)\n",
    "dr = fcn_gen.find_set_nan(dr,find_val,set_val)\n",
    "dc = fcn_gen.find_set_nan(dc,find_val,set_val)\n",
    "dw = fcn_gen.find_set_nan(dw,find_val,set_val)\n",
    "precip = fcn_gen.find_set_nan(precip,find_val,set_val)\n",
    "vs30 = fcn_gen.find_set_nan(vs30,find_val,set_val)\n",
    "dwWB = fcn_gen.find_set_nan(dwWB,find_val,set_val)\n",
    "ky = fcn_gen.find_set_nan(ky,find_val,set_val)\n",
    "\n",
    "## elevation from DEM maps\n",
    "z = np.ones(vs30.shape)*10 ## set to 10 for now, get data from DEM map later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lateral spreading\n",
    "time_start = time.time()\n",
    "\n",
    "multi = 0\n",
    "range_start = 0+100*multi\n",
    "range_end = range_start+1\n",
    "flag_sample_exist = True\n",
    "count = range_start\n",
    "\n",
    "##\n",
    "for im_group in groups2run[range_start:range_end]:\n",
    "    #####################################################################\n",
    "    count += 1\n",
    "    path_sample = im_dir+'/'+im_group+'/'\n",
    "    \n",
    "    #####################################################################\n",
    "    ## load GM predictions and create random variable\n",
    "    A.create_RV(im_tool, site_loc_file, im_dir, im_list, rup_meta_file, flag_clear_dict=True, \n",
    "                flag_sample_exist=flag_sample_exist, im_group=im_group, num_rups=num_rups)\n",
    "\n",
    "    #####################################################################\n",
    "    ## generate/import samples\n",
    "    A.get_IM(nsamp, im_list, flag_corr_d, flag_corr_T, flag_clear_dict=True, \n",
    "            flag_sample_exist=flag_sample_exist,path_sample=path_sample)\n",
    "\n",
    "    if flag_sample_exist is False:\n",
    "        for i in range(nsamp):\n",
    "            out1 = sparse.csc_matrix(np.round(A._IM_dict['pgv'][i].log1p(),decimals=3))\n",
    "            out2 = sparse.csc_matrix(np.round(A._IM_dict['pga'][i].log1p(),decimals=3))\n",
    "            sparse.save_npz(im_dir+'/'+im_group+'/'+'pgv_samp_'+str(i)+'.npz',out1)\n",
    "            sparse.save_npz(im_dir+'/'+im_group+'/'+'pga_samp_'+str(i)+'.npz',out2)\n",
    "\n",
    "    #####################################################################\n",
    "    ## get liq_susc from Zhu et al. (2017)\n",
    "#     category = 'liq'\n",
    "#     method = 'zhu_etal_2017'\n",
    "#     return_param = ['liq_susc']\n",
    "#     dc_cutoff = 20\n",
    "#     A.get_EDP(category=category, method=method, return_param=return_param, \n",
    "#               vs30=vs30, precip=precip, dc=dc, dr=dr, dw=dw, \n",
    "#               wtd=wtd, dc_cutoff=dc_cutoff)\n",
    "    \n",
    "    ## get p_liq and liq_susc from Zhu et al. (2017)\n",
    "    category = 'liq'\n",
    "    method = 'zhu_etal_2017'\n",
    "    return_param = ['liq_susc','p_liq']\n",
    "    flag_pgv = True\n",
    "    flag_M = True\n",
    "    dc_cutoff = 20\n",
    "    A.get_EDP(category=category, method=method, return_param=return_param,\n",
    "              flag_pgv=flag_pgv, flag_M=flag_M, vs30=vs30, precip=precip, dc=dc, dr=dr, dw=dw, \n",
    "              wtd=wtd, dc_cutoff=dc_cutoff, nsamp=nsamp)\n",
    "    \n",
    "    ## get pgd_ls from Grant et al. (2016), same as HAZUS (FEMA, 2014)\n",
    "    category = 'ls'\n",
    "    method = 'hazus_2014_ls'\n",
    "    return_param = ['pgd_ls']\n",
    "    source_dict = ['_EDP_dict']\n",
    "    source_param = ['liq_susc']\n",
    "    source_method = ['zhu_etal_2017']\n",
    "    flag_pga = True\n",
    "    flag_M = True\n",
    "    dw_cutoff = 50\n",
    "    store_name = ['pgd_ls_dw'+str(dw_cutoff)]\n",
    "    A.get_EDP(category=category, method=method, return_param=return_param,\n",
    "              flag_pga=flag_pga, flag_M=flag_M, source_dict=source_dict, \n",
    "              source_param=source_param, source_method=source_method, dw=dwWB*1000, z=z, nsamp=nsamp,\n",
    "              flag_extrap_Epgd=True, dw_cutoff=dw_cutoff)\n",
    "\n",
    "    #####################################################################\n",
    "    dm_method = []\n",
    "    ## get rr from HAZUS (FEMA, 2014)\n",
    "    category = 'rr'\n",
    "    method = 'hazus_2014_rr'\n",
    "    return_param = ['rr_pgd']\n",
    "    source_dict = ['_EDP_dict','_EDP_dict']\n",
    "    source_param = ['p_liq','pgd_ls']\n",
    "    source_method = ['zhu_etal_2017','hazus_2014_ls']\n",
    "    pgd_label = 'pgd_ls'\n",
    "    flag_rup_depend = True\n",
    "    dm_method.append(method[0:method.find('_')])\n",
    "    A.get_DM(category=category, method=method, return_param=return_param, flag_rup_depend=True\n",
    "              source_dict=source_dict, source_param=source_param, source_method=source_method, \n",
    "              nsamp=nsamp, pgd_label=pgd_label)\n",
    "    \n",
    "    ## get rr from ALA (2001)\n",
    "    category = 'rr'\n",
    "    method = 'orourke_2020_rr'\n",
    "    return_param = ['rr_pgd']\n",
    "    source_dict = ['_EDP_dict']\n",
    "    source_param = ['pgd_ls']\n",
    "    source_method = ['hazus_2014_ls']\n",
    "    pgd_label = 'pgd_ls'\n",
    "    flag_rup_depend = True\n",
    "    pgd_cutoff = 1\n",
    "    dm_method.append(method[0:method.find('_')])\n",
    "    A.get_DM(category=category, method=method, return_param=return_param, flag_rup_depend=True\n",
    "                source_dict=source_dict, source_param=source_param, source_method=source_method, nsamp=nsamp,\n",
    "                pgd_cutoff=pgd_cutoff, pgd_label=pgd_label)\n",
    "\n",
    "    #####################################################################\n",
    "#     if count % inc1 == 0:\n",
    "    print('folder '+str(count)+': '+im_group+'--- %10.6f seconds ---' % (time.time() - time_start))\n",
    "    time_start = time.time()\n",
    "\n",
    "    ####################################################################\n",
    "#     if count % inc2 == 0 or count == len(groups2run)-1:\n",
    "#         print('saved files: '+str(range_start)+'_'+str(range_end))\n",
    "#         for param in return_param:\n",
    "#             np.savetxt(dm_dir+'/'+param+'_'+str(range_start)+'_'+str(range_end)+'.txt',np.transpose(A._DM_dict[param]),fmt='%6.4E')\n",
    "    if count % inc2 == 0 or count == len(groups2run):\n",
    "        print('saved files: '+str(range_start)+'_'+str(range_end))\n",
    "        for i in range(len(return_param)):\n",
    "            count_dm = 0\n",
    "            str_src = '_ls'\n",
    "            if 'pgd' in return_param[i]:\n",
    "                str_dw = '_dw'+str(dw_cutoff)\n",
    "            for j in A._DM_dict[return_param[i]]:\n",
    "#                 save_name = dm_dir+'/'+return_param[i]+'_'+dm_method[count_dm]+str_src+str_dw+'_'+str(range_start)+'_'+str(range_end)+'.txt'\n",
    "#                 np.savetxt(save_name,np.transpose(A._DM_dict[return_param[i]][j]['output']),fmt='%6.4E')\n",
    "                save_name = dm_dir+'/'+return_param[i]+'_'+dm_method[count_dm]+str_src+str_dw+'_'+str(range_start)+'_'+str(range_end)+'.npz'\n",
    "                sparse.save_npz(save_name,np.transpose(A._DM_dict[return_param[i]][j]['output']))\n",
    "                count_dm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload model.py if it has been modified\n",
    "importlib.reload(model)\n",
    "## create class object\n",
    "A = model.assessment()\n",
    "##\n",
    "inc1 = 5\n",
    "inc2 = 100\n",
    "# out = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved file\n"
     ]
    }
   ],
   "source": [
    "## Ground settlement\n",
    "time_start = time.time()\n",
    "\n",
    "flag_sample_exist = True\n",
    "im_group = groups2run[0]\n",
    "\n",
    "#####################################################################\n",
    "## load GM predictions and create random variable\n",
    "A.create_RV(im_tool, site_loc_file, im_dir, im_list, rup_meta_file, flag_clear_dict=True, \n",
    "            flag_sample_exist=flag_sample_exist, im_group=im_group, num_rups=num_rups)\n",
    "\n",
    "#####################################################################\n",
    "## get liq_susc from Zhu et al. (2017)\n",
    "category = 'liq'\n",
    "method = 'zhu_etal_2017'\n",
    "return_param = ['liq_susc']\n",
    "dc_cutoff = 20\n",
    "A.get_EDP(category=category, method=method, return_param=return_param, \n",
    "          vs30=vs30, precip=precip, dc=dc, dr=dr, dw=dw, \n",
    "          wtd=wtd, dc_cutoff=dc_cutoff)\n",
    "\n",
    "## get pgd_gs from HAZUS (FEMA, 2014)\n",
    "category = 'gs'\n",
    "method = 'hazus_2014_gs'\n",
    "return_param = ['pgd_gs']\n",
    "source_dict = ['_EDP_dict']\n",
    "source_param = ['liq_susc']\n",
    "source_method = ['zhu_etal_2017']\n",
    "A.get_EDP(category=category, method=method, return_param=return_param, \n",
    "          source_dict=source_dict, source_param=source_param, source_method=source_method)\n",
    "\n",
    "#####################################################################\n",
    "dm_method = []\n",
    "## get rr from ALA (2001)\n",
    "category = 'rr'\n",
    "method = 'orourke_2020_rr'\n",
    "return_param = ['rr_pgd']\n",
    "source_dict = ['_EDP_dict']\n",
    "source_param = ['pgd_gs']\n",
    "source_method = ['hazus_2014_gs']\n",
    "pgd_label = 'pgd_gs'\n",
    "pgd_cutoff = 0\n",
    "dm_method.append(method[0:method.find('_')])\n",
    "A.get_DM(category=category, method=method, return_param=return_param,\n",
    "         source_dict=source_dict, source_param=source_param, source_method=source_method, nsamp=1,\n",
    "         pgd_cutoff=pgd_cutoff, pgd_label=pgd_label)\n",
    "\n",
    "####################################################################\n",
    "print('saved file')\n",
    "for i in range(len(return_param)):\n",
    "    count_dm = 0\n",
    "    str_src = '_gs'\n",
    "    str_dw = None\n",
    "    for j in A._DM_dict[return_param[i]]:\n",
    "# \t\tsave_name = dm_dir+'/'+return_param[i]+'_'+dm_method[count_dm]+str_src+'_all.txt'\n",
    "# \t\tnp.savetxt(save_name,np.transpose(A._DM_dict[return_param[i]][j]['output']),fmt='%6.4E')\n",
    "        save_name = dm_dir+'/'+return_param[i]+'_'+dm_method[count_dm]+str_src+'_all.npz'\n",
    "        out = np.transpose(A._DM_dict[return_param[i]][j]['output']).toarray()\n",
    "        out = sparse.csr_matrix(out[:,0])\n",
    "        sparse.save_npz(save_name,out)\n",
    "        count_dm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = A._DM_dict[return_param[i]][j]['output'].tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55443402, 0.55443402, 0.55443402, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x123316 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 72035 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = sparse.coo_matrix(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55443402, 0.55443402, 0.55443402, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sparse.load_npz(r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\rr_out\\rr_pgd_hazus_ls_dw50_all.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 123316)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload model.py if it has been modified\n",
    "importlib.reload(model)\n",
    "## create class object\n",
    "A = model.assessment()\n",
    "##\n",
    "inc1 = 5\n",
    "inc2 = 100\n",
    "# out = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder 1: 0_99---   1.014670 seconds ---\n",
      "folder 2: 100000_100099---   0.300197 seconds ---\n",
      "folder 3: 10000_10099---   0.292257 seconds ---\n",
      "folder 4: 1000_1099---   0.301910 seconds ---\n",
      "folder 5: 100100_100199---   0.296752 seconds ---\n",
      "folder 6: 100200_100299---   0.306608 seconds ---\n",
      "folder 7: 100300_100399---   0.295713 seconds ---\n",
      "folder 8: 100400_100499---   0.293950 seconds ---\n",
      "folder 9: 100500_100599---   0.293969 seconds ---\n",
      "folder 10: 100600_100699---   0.300456 seconds ---\n",
      "folder 11: 100700_100799---   0.301335 seconds ---\n",
      "folder 12: 100800_100899---   0.292185 seconds ---\n",
      "folder 13: 100900_100999---   0.301197 seconds ---\n",
      "folder 14: 100_199---   0.298884 seconds ---\n",
      "folder 15: 101000_101099---   0.293192 seconds ---\n",
      "folder 16: 101100_101199---   0.305742 seconds ---\n",
      "folder 17: 101200_101299---   0.296093 seconds ---\n",
      "folder 18: 101300_101399---   0.296903 seconds ---\n",
      "folder 19: 101400_101499---   0.292996 seconds ---\n",
      "folder 20: 101500_101599---   0.294053 seconds ---\n",
      "folder 21: 101600_101699---   0.293993 seconds ---\n",
      "folder 22: 101700_101799---   0.301424 seconds ---\n",
      "folder 23: 101800_101899---   0.297581 seconds ---\n",
      "folder 24: 101900_101999---   0.294025 seconds ---\n",
      "folder 25: 102000_102099---   0.294963 seconds ---\n",
      "folder 26: 102100_102199---   0.314981 seconds ---\n",
      "folder 27: 102200_102299---   0.299238 seconds ---\n",
      "folder 28: 102300_102399---   0.295209 seconds ---\n",
      "folder 29: 102400_102499---   0.297259 seconds ---\n",
      "folder 30: 102500_102599---   0.295363 seconds ---\n",
      "folder 31: 102600_102699---   0.293987 seconds ---\n",
      "folder 32: 102700_102799---   0.300270 seconds ---\n",
      "folder 33: 102800_102899---   0.299960 seconds ---\n",
      "folder 34: 102900_102999---   0.312161 seconds ---\n",
      "folder 35: 103000_103099---   0.299204 seconds ---\n",
      "folder 36: 103100_103199---   0.294174 seconds ---\n",
      "folder 37: 103200_103299---   0.297224 seconds ---\n",
      "folder 38: 103300_103399---   0.298052 seconds ---\n",
      "folder 39: 103400_103499---   0.292917 seconds ---\n",
      "folder 40: 103500_103599---   0.293020 seconds ---\n",
      "folder 41: 103600_103699---   0.293010 seconds ---\n",
      "folder 42: 103700_103799---   0.294967 seconds ---\n",
      "folder 43: 103800_103899---   0.299052 seconds ---\n",
      "folder 44: 103900_103999---   0.295038 seconds ---\n",
      "folder 45: 104000_104099---   0.303940 seconds ---\n",
      "folder 46: 104100_104199---   0.292362 seconds ---\n",
      "folder 47: 104200_104299---   0.296432 seconds ---\n",
      "folder 48: 104300_104399---   0.294187 seconds ---\n",
      "folder 49: 104400_104499---   0.301011 seconds ---\n",
      "folder 50: 104500_104599---   0.297027 seconds ---\n",
      "folder 51: 104600_104699---   0.292267 seconds ---\n",
      "folder 52: 104700_104799---   0.293253 seconds ---\n",
      "folder 53: 104800_104899---   0.332678 seconds ---\n",
      "folder 54: 104900_104999---   0.294817 seconds ---\n",
      "folder 55: 105000_105099---   0.292985 seconds ---\n",
      "folder 56: 105100_105199---   0.297153 seconds ---\n",
      "folder 57: 105200_105299---   0.293798 seconds ---\n",
      "folder 58: 105300_105399---   0.301823 seconds ---\n",
      "folder 59: 105400_105499---   0.300157 seconds ---\n",
      "folder 60: 105500_105599---   0.297078 seconds ---\n",
      "folder 61: 105600_105699---   0.304424 seconds ---\n",
      "folder 62: 105700_105799---   0.293775 seconds ---\n",
      "folder 63: 105800_105899---   0.293731 seconds ---\n",
      "folder 64: 105900_105999---   0.301794 seconds ---\n",
      "folder 65: 106000_106099---   0.296294 seconds ---\n",
      "folder 66: 106100_106199---   0.302940 seconds ---\n",
      "folder 67: 106200_106299---   0.295285 seconds ---\n",
      "folder 68: 106300_106399---   0.295754 seconds ---\n",
      "folder 69: 106400_106499---   0.296871 seconds ---\n",
      "folder 70: 106500_106599---   0.300037 seconds ---\n",
      "folder 71: 106600_106699---   0.294045 seconds ---\n",
      "folder 72: 106700_106799---   0.294054 seconds ---\n",
      "folder 73: 106800_106899---   0.293923 seconds ---\n",
      "folder 74: 106900_106999---   0.296030 seconds ---\n",
      "folder 75: 107000_107099---   0.298088 seconds ---\n",
      "folder 76: 107100_107199---   0.297825 seconds ---\n",
      "folder 77: 107200_107299---   0.295078 seconds ---\n",
      "folder 78: 107300_107399---   0.297845 seconds ---\n",
      "folder 79: 107400_107499---   0.298213 seconds ---\n",
      "folder 80: 107500_107599---   0.306984 seconds ---\n",
      "folder 81: 107600_107699---   0.297239 seconds ---\n",
      "folder 82: 107700_107799---   0.296824 seconds ---\n",
      "folder 83: 107800_107899---   0.294981 seconds ---\n",
      "folder 84: 107900_107999---   0.295951 seconds ---\n",
      "folder 85: 108000_108099---   0.300873 seconds ---\n",
      "folder 86: 108100_108199---   0.301346 seconds ---\n",
      "folder 87: 108200_108299---   0.295761 seconds ---\n",
      "folder 88: 108300_108399---   0.296003 seconds ---\n",
      "folder 89: 108400_108499---   0.297022 seconds ---\n",
      "folder 90: 108500_108599---   0.298025 seconds ---\n",
      "folder 91: 108600_108699---   0.302983 seconds ---\n",
      "folder 92: 108700_108799---   0.305025 seconds ---\n",
      "folder 93: 108800_108899---   0.307808 seconds ---\n",
      "folder 94: 108900_108999---   0.302815 seconds ---\n",
      "folder 95: 109000_109099---   0.306628 seconds ---\n",
      "folder 96: 109100_109199---   0.322916 seconds ---\n",
      "folder 97: 109200_109299---   0.314968 seconds ---\n",
      "folder 98: 109300_109399---   0.314893 seconds ---\n",
      "folder 99: 109400_109499---   0.309075 seconds ---\n",
      "folder 100: 109500_109599---   0.320836 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## Surface fault rupture\n",
    "time_start = time.time()\n",
    "\n",
    "dir_intersect = r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\src_intersect'\n",
    "multi = 0\n",
    "range_start = 0+100*multi\n",
    "range_end = range_start+100\n",
    "flag_sample_exist = True\n",
    "count = range_start\n",
    "\n",
    "##\n",
    "for im_group in groups2run[range_start:range_end]:\n",
    "    #####################################################################\n",
    "    count += 1\n",
    "    path_sample = im_dir+'/'+im_group+'/'\n",
    "    \n",
    "    #####################################################################\n",
    "    ## load GM predictions and create random variable\n",
    "    A.create_RV(im_tool, site_loc_file, im_dir, im_list, rup_meta_file, flag_clear_dict=True, \n",
    "                flag_sample_exist=flag_sample_exist, im_group=im_group, num_rups=num_rups)\n",
    "\n",
    "    #####################################################################\n",
    "    ## set up for surface fault rupture\n",
    "    rows = []\n",
    "    cols = []\n",
    "    dim_rup = len(A._RV_dict['rup']['src'])\n",
    "    dim_d = len(A._rup_site_dict['site_lon'])\n",
    "    count_seg = 0\n",
    "    for src in A._RV_dict['rup']['src']:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            seg_list = np.loadtxt(dir_intersect+'/src_'+str(src)+'.txt',dtype=int,ndmin=1)\n",
    "        for seg in seg_list:\n",
    "            rows.append(count_seg)\n",
    "            cols.append(seg)\n",
    "        count_seg += 1\n",
    "    rows = np.asarray(rows)\n",
    "    cols = np.asarray(cols)\n",
    "    mat = sparse.coo_matrix((np.ones(len(rows)),(rows,cols)),shape=(dim_rup,dim_d))\n",
    "    \n",
    "    #####################################################################\n",
    "    ## get pgd_gs from HAZUS (FEMA, 2014)\n",
    "    category = 'surf'\n",
    "    method = 'wells_coppersmith_1994'\n",
    "    return_param = ['pgd_surf']\n",
    "    flag_M = True\n",
    "    A.get_EDP(category=category, method=method, return_param=return_param, flag_M=flag_M, mat_seg2calc=mat)\n",
    "\n",
    "    #####################################################################\n",
    "    dm_method = []\n",
    "    ## get rr from ALA (2001)\n",
    "    category = 'rr'\n",
    "    method = 'orourke_2020_rr'\n",
    "    return_param = ['rr_pgd']\n",
    "    source_dict = ['_EDP_dict']\n",
    "    source_param = ['pgd_surf']\n",
    "    source_method = ['wells_coppersmith_1994']\n",
    "    pgd_label = 'pgd_surf'\n",
    "    pgd_cutoff = 0\n",
    "    flag_rup_depend = True\n",
    "    dm_method.append(method[0:method.find('_')])\n",
    "    A.get_DM(category=category, method=method, return_param=return_param, flag_rup_depend=flag_rup_depend,\n",
    "             source_dict=source_dict, source_param=source_param, source_method=source_method, nsamp=1,\n",
    "             pgd_cutoff=pgd_cutoff, pgd_label=pgd_label)\n",
    "    \n",
    "    #####################################################################\n",
    "#     if count % inc1 == 0:\n",
    "    print('folder '+str(count)+': '+im_group+'--- %10.6f seconds ---' % (time.time() - time_start))\n",
    "    time_start = time.time()\n",
    "\n",
    "#     ####################################################################\n",
    "#     if count % inc2 == 0 or count == len(groups2run):\n",
    "#         print('saved files: '+str(range_start)+'_'+str(range_end))\n",
    "#         for i in range(len(return_param)):\n",
    "#             count_dm = 0\n",
    "#             str_src = '_surf'\n",
    "#             for j in A._DM_dict[return_param[i]]:\n",
    "#     #             save_name = dm_dir+'/'+return_param[i]+'_'+dm_method[count_dm]+str_src+'_all.txt'\n",
    "#     #             np.savetxt(save_name,np.transpose(A._DM_dict[return_param[i]][j]['output']),fmt='%6.4E')\n",
    "#                 save_name = dm_dir+'/'+return_param[i]+'_'+dm_method[count_dm]+str_src+'_'+str(range_start)+'_'+str(range_end)+'.npz'\n",
    "#                 sparse.save_npz(save_name,np.transpose(A._DM_dict[return_param[i]][j]['output']))\n",
    "#                 count_dm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x123316 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 337 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A._DM_dict['rr_pgd']['method1']['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123316, 100)\n",
      "(123316,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sparse.load_npz(r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\rr_out\\rr_pgd_orourke_surf_0_100.npz').toarray()\n",
    "b = np.sum(a,axis=1)\n",
    "sparse.save_npz(r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\rr_out\\rr_pgd_orourke_surf_0_100.npz',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123316, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sparse.load_npz(r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\rr_out\\rr_pgv_hazus_800_900.npz')\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = r'C:\\Users\\barry\\Desktop\\OpenSHAInterface\\rr_out'\n",
    "dirList = os.listdir(baseDir)\n",
    "for file in dirList:\n",
    "#     mat = np.loadtxt(baseDir+'/'+file)\n",
    "#     sparse.save_npz(baseDir+'/'+file[:file.find('.txt')]+'.npz',sparse.csc_matrix(mat))\n",
    "    \n",
    "    if '_surf' in file:\n",
    "        mat_sparse = sparse.load_npz(baseDir+'/'+file).toarray()\n",
    "        mat_sparse_sum = sparse.csr_matrix(np.sum(mat_sparse,axis=1))\n",
    "        sparse.save_npz(baseDir+'/'+file,mat_sparse_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file)\n",
    "print(file[:file.find('.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.loadtxt(baseDir+'/'+dirList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_sparse = sparse.csc_matrix(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.csc_matrix(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [[2,2],[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = sparse.csc_matrix([[2,2],[0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sparse.csc_matrix([[1,0],[2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
